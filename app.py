import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import json
import time
from collections import Counter
import nltk
from nltk.tokenize import word_tokenize, regexp_tokenize
from nltk.stem import SnowballStemmer
import spacy
import razdel
import pymorphy3
import re
import string

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
nltk.download('punkt_tab')

class TextProcessor:
    def __init__(self):
        self._nlp_spacy = None
        self._morph = None
        self._snowball_stemmer = None
    
    @property
    def nlp_spacy(self):
        if self._nlp_spacy is None:
            try:
                self._nlp_spacy = spacy.load("ru_core_news_sm")
            except:
                # –£–±—Ä–∞–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                self._nlp_spacy = None
        return self._nlp_spacy
    
    @property
    def morph(self):
        if self._morph is None:
            self._morph = pymorphy3.MorphAnalyzer()
        return self._morph
    
    @property
    def snowball_stemmer(self):
        if self._snowball_stemmer is None:
            self._snowball_stemmer = SnowballStemmer("russian")
        return self._snowball_stemmer
    
    def tokenize_naive(self, text):
        tokens = text.split()
        return self._filter_tokens(tokens)
    
    def tokenize_regex(self, text):
        # –¢–æ–ª—å–∫–æ —Å–ª–æ–≤–∞ –∏ —á–∏—Å–ª–∞, –±–µ–∑ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        tokens = regexp_tokenize(text, r'\b\w+\b|\d+')
        return self._filter_tokens(tokens)
    
    def tokenize_nltk(self, text):
        tokens = word_tokenize(text, language='russian')
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        tokens = [token for token in tokens if token not in string.punctuation and not all(c in string.punctuation for c in token)]
        return self._filter_tokens(tokens)
    
    def tokenize_spacy(self, text):
        if self.nlp_spacy:
            doc = self.nlp_spacy(text)
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
            tokens = [token.text for token in doc if not token.is_punct]
            return self._filter_tokens(tokens)
        return []
    
    def tokenize_razdel(self, text):
        tokens = [token.text for token in razdel.tokenize(text)]
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        tokens = [token for token in tokens if token not in string.punctuation and not all(c in string.punctuation for c in token)]
        return self._filter_tokens(tokens)
    
    def _filter_tokens(self, tokens):
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–æ–≤ - —É–±–∏—Ä–∞–µ—Ç –ø—É—Å—Ç—ã–µ, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ"""
        filtered = []
        for token in tokens:
            clean_token = token.strip()
            if clean_token and len(clean_token) > 0:
                # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–∫—Ä–æ–º–µ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä)
                if (clean_token not in string.punctuation and 
                    not all(c in string.punctuation for c in clean_token) and
                    (clean_token.isalnum() or len(clean_token) > 1)):
                    filtered.append(clean_token)
        return filtered
    
    def _should_normalize(self, token):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–æ–∫–µ–Ω"""
        # –ù–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º: —á–∏—Å–ª–∞, –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏/—Å–æ—é–∑—ã
        if (token.isdigit() or
            token in ['a', '–∏', '–≤', '—Å', '—É', '–æ', '–∫', '–Ω–∞', '–ø–æ', '–∑–∞', '–∏–∑']):
            return False
        return True
    
    def stem_snowball(self, tokens):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å—Ç–µ–º–º–∏–Ω–≥ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        stemmed = []
        for token in tokens:
            if not self._should_normalize(token):
                stemmed.append(token)
            else:
                try:
                    stemmed_token = self.snowball_stemmer.stem(token)
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç–µ–º–º–∏–Ω–≥ –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏–ª —Å–ª–æ–≤–æ
                    if len(stemmed_token) >= 2:
                        stemmed.append(stemmed_token)
                    else:
                        stemmed.append(token)
                except:
                    stemmed.append(token)
        return stemmed
    
    def lemmatize_pymorphy(self, tokens):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        lemmatized = []
        for token in tokens:
            if not self._should_normalize(token):
                lemmatized.append(token)
            else:
                try:
                    parsed = self.morph.parse(token)
                    if parsed:
                        # –í—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—É—é –ª–µ–º–º—É —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                        best_parse = parsed[0]
                        lemma = best_parse.normal_form
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ª–µ–º–º–∞ –Ω–µ –∫–æ—Ä–æ—á–µ 2 —Å–∏–º–≤–æ–ª–æ–≤
                        if len(lemma) >= 2:
                            lemmatized.append(lemma)
                        else:
                            lemmatized.append(token)
                    else:
                        lemmatized.append(token)
                except:
                    lemmatized.append(token)
        return lemmatized
    
    def lemmatize_spacy(self, tokens):
        if self.nlp_spacy:
            text = ' '.join(tokens)
            doc = self.nlp_spacy(text)
            lemmatized = []
            for token in doc:
                if not self._should_normalize(token.text):
                    lemmatized.append(token.text)
                else:
                    lemma = token.lemma_
                    if len(lemma) >= 2:
                        lemmatized.append(lemma)
                    else:
                        lemmatized.append(token.text)
            return lemmatized
        return tokens

def validate_texts(texts):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
    if not texts:
        raise ValueError("–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    valid_texts = [text for text in texts if text and isinstance(text, str) and len(text.strip()) > 0]
    
    if len(valid_texts) == 0:
        raise ValueError("–í—Å–µ —Ç–µ–∫—Å—Ç—ã –ø—É—Å—Ç—ã–µ –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞")
    
    return valid_texts

def load_texts_from_jsonl(uploaded_file):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ JSONL —Ñ–∞–π–ª–∞"""
    texts = []
    success_count = 0
    error_count = 0
    
    uploaded_file.seek(0)
    
    for i, line in enumerate(uploaded_file):
        try:
            line_str = line.decode('utf-8').strip()
            
            if not line_str:
                continue
                
            article = json.loads(line_str)
            
            text_content = article.get('text') or article.get('content') or article.get('body') or article.get('title')
            if text_content and isinstance(text_content, str) and text_content.strip():
                texts.append(text_content.strip())
                success_count += 1
            else:
                error_count += 1
                
        except (json.JSONDecodeError, KeyError, AttributeError, UnicodeDecodeError) as e:
            error_count += 1
            continue
    
    # –£–±—Ä–∞–Ω–æ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–∞—Ö
    return texts

def create_token_length_distribution(tokens_list):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤"""
    token_lengths = [len(token) for tokens in tokens_list for token in tokens if token.strip()]
    
    if not token_lengths:
        return create_empty_plot("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    fig = px.histogram(
        x=token_lengths,
        nbins=30,
        title='–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤',
        labels={'x': '–î–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞', 'y': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'},
        color_discrete_sequence=['#1f77b4']
    )
    fig.update_layout(showlegend=False)
    return fig

def create_token_frequency_chart(tokens_list, top_n=20):
    """–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤"""
    all_tokens = [token.lower() for tokens in tokens_list for token in tokens if token.strip()]
    
    if not all_tokens:
        return create_empty_plot("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    token_counter = Counter(all_tokens)
    top_tokens = token_counter.most_common(top_n)
    
    tokens, freqs = zip(*top_tokens)
    
    fig = px.bar(
        x=tokens,
        y=freqs,
        title=f'–¢–æ–ø-{top_n} —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤',
        labels={'x': '–¢–æ–∫–µ–Ω', 'y': '–ß–∞—Å—Ç–æ—Ç–∞'},
        color=freqs,
        color_continuous_scale='viridis'
    )
    fig.update_layout(
        xaxis_tickangle=-45, 
        showlegend=False,
        height=400
    )
    return fig

def create_empty_plot(message):
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Å—Ç–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ —Å —Å–æ–æ–±—â–µ–Ω–∏–µ–º"""
    fig = go.Figure()
    fig.add_annotation(
        text=message,
        xref="paper", yref="paper",
        x=0.5, y=0.5,
        showarrow=False,
        font=dict(size=16)
    )
    fig.update_layout(
        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
        height=400
    )
    return fig

def calculate_oov_rate(processed_texts, reference_vocab):
    """–†–∞—Å—á–µ—Ç –¥–æ–ª–∏ OOV —Å–ª–æ–≤"""
    all_tokens = [token for tokens in processed_texts for token in tokens if token.strip()]
    
    if not reference_vocab or not all_tokens:
        return 0
    
    oov_count = sum(1 for token in all_tokens if token not in reference_vocab)
    return (oov_count / len(all_tokens)) * 100

def create_oov_gauge(oov_rate):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ OOV"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number",
        value = oov_rate,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "–î–æ–ª—è OOV (%)"},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': "darkblue"},
            'steps': [
                {'range': [0, 10], 'color': "lightgreen"},
                {'range': [10, 20], 'color': "yellow"},
                {'range': [20, 100], 'color': "red"}],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 90}}))
    
    fig.update_layout(
        margin=dict(t=50, b=10, l=10, r=10)
    )
    return fig

def generate_report(processed_texts, method_name, processing_time):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
    all_tokens = [token for tokens in processed_texts for token in tokens if token.strip()]
    
    total_tokens = len(all_tokens)
    unique_tokens = len(set(all_tokens)) if all_tokens else 0
    avg_token_length = sum(len(token) for token in all_tokens) / len(all_tokens) if all_tokens else 0
    
    metrics_df = pd.DataFrame({
        '–ú–µ—Ç—Ä–∏–∫–∞': [
            '–ú–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏',
            '–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤', 
            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤',
            '–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞',
            '–í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å–µ–∫)'
        ],
        '–ó–Ω–∞—á–µ–Ω–∏–µ': [
            method_name,
            f"{total_tokens:,}",
            f"{unique_tokens:,}",
            f"{avg_token_length:.2f}",
            f"{processing_time:.2f}"
        ]
    })
    
    return metrics_df

def preprocess_text(text):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏"""
    # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def process_with_progress(processor, texts, tokenize_func, normalize_func):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
    processed_texts = []
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    for i, text in enumerate(texts):
        progress_text.text(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ {i+1}/{len(texts)}")
        
        try:
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
            cleaned_text = preprocess_text(text)
            tokens = tokenize_func(cleaned_text)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            tokens = [token for token in tokens if token.strip()]
            
            # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            filtered_tokens = []
            for j, token in enumerate(tokens):
                if (j == 0 or 
                    token != tokens[j-1] or 
                    len(token) > 1 or 
                    token.isalnum()):
                    filtered_tokens.append(token)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é
            normalized_tokens = normalize_func(filtered_tokens)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
            final_tokens = [token for token in normalized_tokens if token.strip()]
            
            processed_texts.append(final_tokens)
        except Exception:
            # –£–±—Ä–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            processed_texts.append([])
        
        progress_bar.progress((i + 1) / len(texts))
    
    progress_text.empty()
    progress_bar.empty()
    
    return processed_texts

def main():
    st.set_page_config(
        page_title="NLP Text Analyzer",
        page_icon="üìä",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üìä NLP –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–µ–∫—Å—Ç–∞")
    st.markdown("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    st.markdown("---")
    
    if 'processor' not in st.session_state:
        st.session_state.processor = TextProcessor()
    
    if 'loaded_texts' not in st.session_state:
        st.session_state.loaded_texts = []
    
    # –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–µ–∫
    with st.sidebar:
        st.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        st.subheader("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        uploaded_file = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ JSONL —Ñ–∞–π–ª", 
            type=['jsonl'],
            help="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ç–µ–∫—Å—Ç—ã. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ - –æ—Ç–¥–µ–ª—å–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç."
        )
        
        if uploaded_file is not None:
            if st.button("üîÑ –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã", use_container_width=True):
                with st.spinner("–ó–∞–≥—Ä—É–∂–∞—é —Ñ–∞–π–ª..."):
                    texts = load_texts_from_jsonl(uploaded_file)
                    if texts:
                        st.session_state.loaded_texts = texts
                        st.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
                    else:
                        st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã –∏–∑ —Ñ–∞–π–ª–∞")
        
        st.subheader("üî§ –ú–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏")
        tokenization_method = st.selectbox(
            "–ö–∞–∫ —Ä–∞–∑–¥–µ–ª–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã:",
            [
                "–ù–∞–∏–≤–Ω–∞—è (–ø–æ –ø—Ä–æ–±–µ–ª–∞–º)",
                "–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞)", 
                "NLTK (–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è)",
                "spaCy (—Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)",
                "razdel (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)"
            ],
            index=4
        )
        
        st.subheader("üîÑ –ú–µ—Ç–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏")
        normalization_method = st.selectbox(
            "–ö–∞–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞:",
            [
                "–ë–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–∏—Å—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞)",
                "Snowball —Å—Ç–µ–º–º–∏–Ω–≥ (–æ—Å–Ω–æ–≤–∞ —Å–ª–æ–≤–∞)",
                "pymorphy3 –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (—Å–ª–æ–≤–∞—Ä–Ω–∞—è —Ñ–æ—Ä–º–∞)", 
                "spaCy –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è)"
            ],
            index=2
        )
        
        st.subheader("üìà –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∞–Ω–∞–ª–∏–∑–∞")
        top_n_tokens = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:",
            min_value=10,
            max_value=50,
            value=20
        )
        
        sample_size = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:",
            min_value=1,
            max_value=500,
            value=50
        )
        
        process_button = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", type="primary", use_container_width=True)
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –æ–±–ª–∞—Å—Ç—å
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.header("üìã –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        texts_to_process = []
        if st.session_state.loaded_texts:
            texts_to_process = st.session_state.loaded_texts
            
            if sample_size > 0 and len(texts_to_process) > sample_size:
                texts_to_process = texts_to_process[:sample_size]
            
            st.subheader("üìù –ü—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤:")
            for i, text in enumerate(texts_to_process[:3]):
                with st.expander(f"–¢–µ–∫—Å—Ç {i+1} ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)"):
                    st.text(text[:500] + "..." if len(text) > 500 else text)
        else:
            st.info("üëÜ –ó–∞–≥—Ä—É–∑–∏—Ç–µ JSONL —Ñ–∞–π–ª –∏ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É '–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã' –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
    
    with col2:
        st.header("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        if texts_to_process:
            total_chars = sum(len(text) for text in texts_to_process)
            total_words = sum(len(text.split()) for text in texts_to_process)
            avg_words = total_words / len(texts_to_process) if texts_to_process else 0
            avg_chars = total_chars / len(texts_to_process) if texts_to_process else 0
            
            st.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤", len(texts_to_process))
            st.metric("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤", f"{total_chars:,}")
            st.metric("–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤", f"{total_words:,}")
            st.metric("–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)", f"{avg_words:.1f}")
            st.metric("–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª–æ–≤)", f"{avg_chars:.1f}")
        else:
            st.info("–î–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    if process_button and texts_to_process:
        try:
            validated_texts = validate_texts(texts_to_process)
        except ValueError as e:
            # –£–±—Ä–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            return
            
        st.markdown("---")
        st.header("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        
        with st.spinner("üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–µ–∫—Å—Ç—ã..."):
            token_funcs = {
                "–ù–∞–∏–≤–Ω–∞—è (–ø–æ –ø—Ä–æ–±–µ–ª–∞–º)": st.session_state.processor.tokenize_naive,
                "–†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è (—Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞)": st.session_state.processor.tokenize_regex,
                "NLTK (–∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è)": st.session_state.processor.tokenize_nltk,
                "spaCy (—Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)": st.session_state.processor.tokenize_spacy,
                "razdel (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)": st.session_state.processor.tokenize_razdel
            }
            
            norm_funcs = {
                "–ë–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–∏—Å—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞)": lambda x: x,
                "Snowball —Å—Ç–µ–º–º–∏–Ω–≥ (–æ—Å–Ω–æ–≤–∞ —Å–ª–æ–≤–∞)": st.session_state.processor.stem_snowball,
                "pymorphy3 –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (—Å–ª–æ–≤–∞—Ä–Ω–∞—è —Ñ–æ—Ä–º–∞)": st.session_state.processor.lemmatize_pymorphy,
                "spaCy –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è)": st.session_state.processor.lemmatize_spacy
            }
            
            tokenize_func = token_funcs[tokenization_method]
            normalize_func = norm_funcs[normalization_method]
            
            start_time = time.time()
            processed_texts = process_with_progress(
                st.session_state.processor, 
                validated_texts, 
                tokenize_func, 
                normalize_func
            )
            processing_time = time.time() - start_time
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è –¥–ª—è OOV
            split_index = int(len(processed_texts) * 0.8)
            reference_vocab = set()
            for tokens in processed_texts[:split_index]:
                reference_vocab.update(tokens)
            
            test_texts = processed_texts[split_index:] if split_index < len(processed_texts) else processed_texts
            oov_rate = calculate_oov_rate(test_texts, reference_vocab)
            
            method_name = f"{tokenization_method} + {normalization_method}"
            report_df = generate_report(processed_texts, method_name, processing_time)
        
        st.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.subheader("üìã –û—Ç—á–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            st.dataframe(report_df, use_container_width=True, hide_index=True)
        
        with col2:
            st.subheader("‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ OOV")
            st.plotly_chart(create_oov_gauge(oov_rate), use_container_width=True)
            st.caption("OOV –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç —Ç–æ–∫–µ–Ω–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –æ–±—É—á–∞—é—â–µ–º —Å–ª–æ–≤–∞—Ä–µ")
        
        with col3:
            st.subheader("üîç –ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            if processed_texts and processed_texts[0]:
                sample_tokens = processed_texts[0][:15]
                st.write("–ü–µ—Ä–≤—ã–µ 15 —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:")
                for i, token in enumerate(sample_tokens, 1):
                    st.write(f"{i}. {token}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        st.markdown("---")
        st.header("üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üìè –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω —Ç–æ–∫–µ–Ω–æ–≤")
            fig_length = create_token_length_distribution(processed_texts)
            st.plotly_chart(fig_length, use_container_width=True)
        
        with col2:
            st.subheader("üìä –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤")
            fig_freq = create_token_frequency_chart(processed_texts, top_n_tokens)
            st.plotly_chart(fig_freq, use_container_width=True)
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        st.subheader("üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤")
        
        all_tokens = [token for tokens in processed_texts for token in tokens if token.strip()]
        if all_tokens:
            token_counter = Counter(all_tokens)
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("–í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤", f"{len(all_tokens):,}")
            with col2:
                st.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤", f"{len(token_counter):,}")
            with col3:
                unique_once = sum(1 for count in token_counter.values() if count == 1)
                st.metric("–¢–æ–∫–µ–Ω—ã —Å —á–∞—Å—Ç–æ—Ç–æ–π 1", f"{unique_once:,}")
            with col4:
                if token_counter:
                    most_common_token, most_common_freq = token_counter.most_common(1)[0]
                    st.metric("–°–∞–º—ã–π —á–∞—Å—Ç—ã–π —Ç–æ–∫–µ–Ω", f"'{most_common_token}': {most_common_freq:,}")
            
            # –¢–∞–±–ª–∏—Ü–∞ —Å —Ç–æ–ø —Ç–æ–∫–µ–Ω–∞–º–∏
            st.subheader("üèÜ –¢–æ–ø —Ç–æ–∫–µ–Ω—ã")
            top_tokens_df = pd.DataFrame(
                token_counter.most_common(20),
                columns=['–¢–æ–∫–µ–Ω', '–ß–∞—Å—Ç–æ—Ç–∞']
            )
            st.dataframe(top_tokens_df, use_container_width=True, hide_index=True)

if __name__ == "__main__":
    main()