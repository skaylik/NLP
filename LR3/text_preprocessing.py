"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
–≠—Ç–∞–ø 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º train/validation/test —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
"""

import re
import string
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Union, Optional, Any
from collections import Counter
import warnings
import pickle
import json
import os
warnings.filterwarnings('ignore')

# –î–ª—è NLP –æ–±—Ä–∞–±–æ—Ç–∫–∏
try:
    import spacy
    from spacy.lang.ru.stop_words import STOP_WORDS
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    print("‚ö†Ô∏è spaCy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install spacy")

try:
    import nltk
    from nltk.corpus import stopwords
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("‚ö†Ô∏è NLTK –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install nltk")

# –î–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
try:
    from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
    from sklearn.preprocessing import StandardScaler
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    print("‚ö†Ô∏è scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install scikit-learn")

# –î–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
try:
    import gensim
    from gensim.models import Word2Vec, FastText
    GENSIM_AVAILABLE = True
except ImportError:
    GENSIM_AVAILABLE = False
    print("‚ö†Ô∏è Gensim –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install gensim")

# –î–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤
try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("‚ö†Ô∏è Transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install transformers")


class TextPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    
    def __init__(self, language: str = 'russian', 
                 remove_stopwords: bool = True,
                 use_spacy: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            language: —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ ('russian' –∏–ª–∏ 'english')
            remove_stopwords: —É–¥–∞–ª—è—Ç—å –ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            use_spacy: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å spaCy (True) –∏–ª–∏ NLTK (False)
        """
        self.language = language
        self.remove_stopwords = remove_stopwords
        self.use_spacy = use_spacy
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP –º–æ–¥–µ–ª–µ–π
        self.nlp = None
        self.stop_words = set()
        
        if use_spacy and SPACY_AVAILABLE:
            try:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å spaCy –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
                self.nlp = spacy.load("ru_core_news_sm" if language == 'russian' else "en_core_web_sm")
                self.stop_words = STOP_WORDS
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ spaCy –º–æ–¥–µ–ª—å –¥–ª—è —è–∑—ã–∫–∞: {language}")
            except OSError:
                print(f"‚ö†Ô∏è spaCy –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É—é –ø—Ä–æ—Å—Ç—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É.")
                self.nlp = None
        elif NLTK_AVAILABLE:
            try:
                nltk.download('stopwords', quiet=True)
                nltk.download('punkt', quiet=True)
                if language == 'russian':
                    # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –≤ NLTK
                    self.stop_words = set(stopwords.words('russian'))
                else:
                    self.stop_words = set(stopwords.words('english'))
                print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º NLTK –¥–ª—è —è–∑—ã–∫–∞: {language}")
            except:
                print("‚ö†Ô∏è NLTK –Ω–µ –º–æ–∂–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ—á–∏—Å—Ç–∫–∏
        self.url_pattern = re.compile(r'https?://\S+|www\.\S+')
        self.html_pattern = re.compile(r'<.*?>')
        self.email_pattern = re.compile(r'\S+@\S+')
        self.phone_pattern = re.compile(r'[\+]?[78]\s?[\(]?\d{3}[\)]?\s?\d{3}[\-]?\d{2}[\-]?\d{2}')
        
        # –°–ª–æ–≤–∞—Ä—å –¥–ª—è —ç–º–æ–¥–∑–∏
        self.emoji_dict = {
            'üòÄ': '—Å–º–∞–π–ª–∏–∫_—Ä–∞–¥–æ—Å—Ç—å', 'üòÇ': '—Å–º–µ—Ö', 'üòä': '—É–ª—ã–±–∫–∞', 'üòç': '–ª—é–±–æ–≤—å',
            'üò≠': '–ø–ª–∞—á', 'üò°': '–∑–ª–æ—Å—Ç—å', 'üò±': '—É–∂–∞—Å', 'üëç': '–ª–∞–π–∫',
            'üëé': '–¥–∏–∑–ª–∞–π–∫', '‚ù§Ô∏è': '—Å–µ—Ä–¥—Ü–µ', 'üôè': '—Å–ø–∞—Å–∏–±–æ', 'üòî': '–≥—Ä—É—Å—Ç—å',
            'ü§î': '–∑–∞–¥—É–º—á–∏–≤–æ—Å—Ç—å', 'üòé': '–∫—Ä—É—Ç–æ', 'ü§ó': '–æ–±—ä—è—Ç–∏—è', 'üò¥': '—Å–æ–Ω',
            'ü§Æ': '—Ç–æ—à–Ω–æ—Ç–∞', 'ü§Ø': '–≤–∑—Ä—ã–≤_–º–æ–∑–≥–∞', 'ü•∞': '–≤–ª—é–±–ª–µ–Ω–Ω–æ—Å—Ç—å',
            'üò§': '—Ä–∞–∑–æ—á–∞—Ä–æ–≤–∞–Ω–∏–µ', 'üò®': '—Å—Ç—Ä–∞—Ö', 'üò©': '—É—Å—Ç–∞–ª–æ—Å—Ç—å'
        }
    
    def clean_text(self, text: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç HTML, URL, —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        
        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not isinstance(text, str):
            return ""
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ URL
        text = self.url_pattern.sub('', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ HTML —Ç–µ–≥–æ–≤
        text = self.html_pattern.sub('', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ email
        text = self.email_pattern.sub('', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
        text = self.phone_pattern.sub('', text)
        
        # –ó–∞–º–µ–Ω–∞ —ç–º–æ–¥–∑–∏ –Ω–∞ —Ç–µ–∫—Å—Ç
        for emoji, desc in self.emoji_dict.items():
            text = text.replace(emoji, f' {desc} ')
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        text = re.sub(r'[^–∞-—è—ëa-z0-9\s.,!?;:()\-"\']', ' ', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize_with_spacy(self, text: str) -> List[str]:
        """
        –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º spaCy
        
        Args:
            text: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            —Å–ø–∏—Å–æ–∫ –ª–µ–º–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        """
        if not self.nlp:
            return self.tokenize_simple(text)
        
        doc = self.nlp(text)
        tokens = []
        
        for token in doc:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–æ–±–µ–ª—ã
            if token.is_punct or token.is_space:
                continue
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.remove_stopwords and token.text in self.stop_words:
                continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–µ–º–º—É –∏–ª–∏ —Ç–µ–∫—Å—Ç
            lemma = token.lemma_ if token.lemma_ != '-PRON-' else token.text
            
            # –î–æ–±–∞–≤–ª—è–µ–º –µ—Å–ª–∏ –ª–µ–º–º–∞ –Ω–µ –ø—É—Å—Ç–∞—è
            if lemma.strip():
                tokens.append(lemma)
        
        return tokens
    
    def tokenize_simple(self, text: str) -> List[str]:
        """
        –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –±–µ–∑ spaCy
        
        Args:
            text: –æ—á–∏—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            
        Returns:
            —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –ø–æ –ø—Ä–æ–±–µ–ª–∞–º –∏ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        tokens = re.findall(r'\b[–∞-—è—ëa-z]+\b', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.remove_stopwords and self.stop_words:
            tokens = [token for token in tokens if token not in self.stop_words]
        
        return tokens
    
    def preprocess(self, text: str, return_string: bool = False) -> Union[str, List[str]]:
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        Args:
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            return_string: –≤–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä–æ–∫—É (True) –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ (False)
            
        Returns:
            –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –û—á–∏—Å—Ç–∫–∞
        cleaned_text = self.clean_text(text)
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        if self.use_spacy and self.nlp:
            tokens = self.tokenize_with_spacy(cleaned_text)
        else:
            tokens = self.tokenize_simple(cleaned_text)
        
        if return_string:
            return ' '.join(tokens)
        else:
            return tokens
    
    def preprocess_batch(self, texts: List[str], return_string: bool = False) -> List[Union[str, List[str]]]:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            return_string: –≤–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä–æ–∫—É (True) –∏–ª–∏ —Å–ø–∏—Å–æ–∫ —Ç–æ–∫–µ–Ω–æ–≤ (False)
            
        Returns:
            —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        """
        return [self.preprocess(text, return_string) for text in texts]


class FeatureExtractor:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        self.scaler = StandardScaler() if SKLEARN_AVAILABLE else None
    
    def extract_statistical_features(self, text: str) -> Dict:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            text: —Ç–µ–∫—Å—Ç
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        """
        if not text:
            return {}
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = text.split()
        chars = text.replace(' ', '')
        
        # –ü–æ–¥—Å—á–µ—Ç—ã
        num_words = len(words)
        num_chars = len(chars)
        num_sentences = len(re.split(r'[.!?]+', text))
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–∏—Ö
        avg_word_length = np.mean([len(w) for w in words]) if words else 0
        avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0
        
        # –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
        unique_words = len(set(words))
        lexical_diversity = unique_words / num_words if num_words > 0 else 0
        
        # –î–ª–∏–Ω–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        char_count = len(text)
        char_count_no_spaces = len(text.replace(' ', ''))
        
        # –ü–æ–¥—Å—á–µ—Ç —Ü–∏—Ñ—Ä
        digit_count = sum(c.isdigit() for c in text)
        digit_ratio = digit_count / len(text) if len(text) > 0 else 0
        
        # –ü–æ–¥—Å—á–µ—Ç –∑–∞–≥–ª–∞–≤–Ω—ã—Ö –±—É–∫–≤
        uppercase_count = sum(1 for c in text if c.isupper())
        uppercase_ratio = uppercase_count / len(text) if len(text) > 0 else 0
        
        # –ü–æ–¥—Å—á–µ—Ç –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞—Ü–∏–∏
        punctuation_chars = set(string.punctuation + '¬´¬ª‚Äî‚Äì')
        punctuation_count = sum(1 for c in text if c in punctuation_chars)
        punctuation_ratio = punctuation_count / len(text) if len(text) > 0 else 0
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞)
        syllables_count = self._count_syllables_russian(text)
        flesch_kincaid = 206.835 - 1.3 * (num_words / num_sentences) - 60.1 * (syllables_count / num_words) if num_words > 0 and num_sentences > 0 else 0
        
        features = {
            'word_count': num_words,
            'char_count': char_count,
            'char_count_no_spaces': char_count_no_spaces,
            'sentence_count': num_sentences,
            'avg_word_length': avg_word_length,
            'avg_sentence_length': avg_sentence_length,
            'unique_word_count': unique_words,
            'lexical_diversity': lexical_diversity,
            'digit_count': digit_count,
            'digit_ratio': digit_ratio,
            'uppercase_count': uppercase_count,
            'uppercase_ratio': uppercase_ratio,
            'punctuation_count': punctuation_count,
            'punctuation_ratio': punctuation_ratio,
            'syllable_count': syllables_count,
            'flesch_kincaid_score': flesch_kincaid,
            'is_short_text': 1 if num_words < 10 else 0,
            'is_long_text': 1 if num_words > 100 else 0
        }
        
        return features
    
    def _count_syllables_russian(self, text: str) -> int:
        """
        –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤ –≤ —Ä—É—Å—Å–∫–æ–º —Ç–µ–∫—Å—Ç–µ
        
        Args:
            text: —Ç–µ–∫—Å—Ç
            
        Returns:
            –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≥–æ–≤
        """
        vowels = '–∞–µ—ë–∏–æ—É—ã—ç—é—è'
        text = text.lower()
        
        syllables = 0
        for char in text:
            if char in vowels:
                syllables += 1
        
        return syllables if syllables > 0 else 1
    
    def extract_batch_features(self, texts: List[str]) -> pd.DataFrame:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        features_list = []
        for text in texts:
            features = self.extract_statistical_features(text)
            features_list.append(features)
        
        return pd.DataFrame(features_list)
    
    def normalize_features(self, features_df: pd.DataFrame, fit: bool = True) -> np.ndarray:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        
        Args:
            features_df: DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
            fit: –æ–±—É—á–∏—Ç—å —Å–∫–µ–π–ª–µ—Ä (True) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–π (False)
            
        Returns:
            –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        """
        if self.scaler is None:
            return features_df.values
        
        if fit:
            return self.scaler.fit_transform(features_df.values)
        else:
            return self.scaler.transform(features_df.values)


class SplitAwareVectorizer:
    """
    –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä, –∫–æ—Ç–æ—Ä—ã–π —É—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test
    –û–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö!
    """
    
    def __init__(self, method: str = 'tfidf', **kwargs):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            method: –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ('bow', 'tfidf', 'word2vec', 'fasttext', 'bert')
            **kwargs: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞
        """
        self.method = method
        self.vectorizer = None
        self.embedding_model = None
        self.tokenizer = None
        self.model = None
        self.is_fitted = False
        self.vector_size = kwargs.get('vector_size', 100)  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –î–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤
        if method in ['bow', 'tfidf']:
            if not SKLEARN_AVAILABLE:
                raise ImportError("scikit-learn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            
            if method == 'bow':
                self.vectorizer = CountVectorizer(
                    max_features=kwargs.get('max_features', 5000),
                    ngram_range=kwargs.get('ngram_range', (1, 2)),
                    min_df=kwargs.get('min_df', 2),
                    max_df=kwargs.get('max_df', 0.95)
                )
            else:  # tfidf
                self.vectorizer = TfidfVectorizer(
                    max_features=kwargs.get('max_features', 5000),
                    ngram_range=kwargs.get('ngram_range', (1, 2)),
                    min_df=kwargs.get('min_df', 2),
                    max_df=kwargs.get('max_df', 0.95)
                )
        
        # –î–ª—è Word2Vec - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –º–µ—Å—Ç–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
        elif method == 'word2vec':
            if not GENSIM_AVAILABLE:
                raise ImportError("gensim –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            
            model_path = kwargs.get('model_path')
            if model_path and os.path.exists(model_path):
                try:
                    self.embedding_model = Word2Vec.load(model_path)
                    self.vector_size = self.embedding_model.vector_size
                    self.is_fitted = True
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å Word2Vec –∏–∑ {model_path}")
                except:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Word2Vec –∏–∑ {model_path}")
                    self.embedding_model = None
            else:
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö
                print("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å Word2Vec –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö")
                self.embedding_model = None
        
        # –î–ª—è FastText - –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ
        elif method == 'fasttext':
            if not GENSIM_AVAILABLE:
                raise ImportError("gensim –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            
            model_path = kwargs.get('model_path')
            if model_path and os.path.exists(model_path):
                try:
                    self.embedding_model = FastText.load(model_path)
                    self.vector_size = self.embedding_model.vector_size
                    self.is_fitted = True
                    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å FastText –∏–∑ {model_path}")
                except:
                    print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å FastText –∏–∑ {model_path}")
                    self.embedding_model = None
            else:
                print("‚ÑπÔ∏è –ú–æ–¥–µ–ª—å FastText –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –±—É–¥–µ—Ç –æ–±—É—á–µ–Ω–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö")
                self.embedding_model = None
        
        # –î–ª—è BERT
        elif method in ['bert', 'rubert']:
            if not TRANSFORMERS_AVAILABLE:
                raise ImportError("transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            
            model_name = 'cointegrated/rubert-tiny' if method == 'rubert' else 'bert-base-multilingual-cased'
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                self.model = AutoModel.from_pretrained(model_name)
                self.model.eval()
                self.vector_size = self.model.config.hidden_size
                self.is_fitted = True
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")
            except Exception as e:
                print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}: {e}")
                self.model = None
    
    def fit(self, texts: List[str]):
        """
        –û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ train!)
        """
        if self.vectorizer and not self.is_fitted:
            self.vectorizer.fit(texts)
            self.is_fitted = True
            print(f"‚úÖ –û–±—É—á–µ–Ω –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä {self.method}")
        
        # –û–±—É—á–µ–Ω–∏–µ Word2Vec –∏–ª–∏ FastText –Ω–∞ –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –±—ã–ª–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∞
        elif self.method == 'word2vec' and not self.is_fitted:
            print("üîÑ –û–±—É—á–µ–Ω–∏–µ Word2Vec –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            try:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
                tokenized_texts = [text.split() for text in texts]
                
                self.embedding_model = Word2Vec(
                    sentences=tokenized_texts,
                    vector_size=self.vector_size,
                    window=5,
                    min_count=2,
                    workers=4,
                    epochs=10,
                    seed=42
                )
                self.is_fitted = True
                print(f"‚úÖ –û–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å Word2Vec –Ω–∞ {len(texts)} —Ç–µ–∫—Å—Ç–∞—Ö")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ Word2Vec: {e}")
                self.embedding_model = None
        
        elif self.method == 'fasttext' and not self.is_fitted:
            print("üîÑ –û–±—É—á–µ–Ω–∏–µ FastText –Ω–∞ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
            try:
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤
                tokenized_texts = [text.split() for text in texts]
                
                self.embedding_model = FastText(
                    sentences=tokenized_texts,
                    vector_size=self.vector_size,
                    window=5,
                    min_count=2,
                    workers=4,
                    epochs=10,
                    seed=42
                )
                self.is_fitted = True
                print(f"‚úÖ –û–±—É—á–µ–Ω–∞ –º–æ–¥–µ–ª—å FastText –Ω–∞ {len(texts)} —Ç–µ–∫—Å—Ç–∞—Ö")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ FastText: {e}")
                self.embedding_model = None
    
    def transform(self, texts: List[str]) -> np.ndarray:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        if self.method in ['bow', 'tfidf']:
            if not self.is_fitted:
                raise ValueError("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –æ–±—É—á–µ–Ω! –°–Ω–∞—á–∞–ª–∞ –≤—ã–∑–æ–≤–∏—Ç–µ fit() –Ω–∞ train –¥–∞–Ω–Ω—ã—Ö.")
            return self.vectorizer.transform(texts)
        
        elif self.method == 'word2vec':
            return self._get_word2vec_vectors(texts)
        
        elif self.method == 'fasttext':
            return self._get_fasttext_vectors(texts)
        
        elif self.method in ['bert', 'rubert']:
            return self._get_bert_vectors(texts)
        
        else:
            raise ValueError(f"–ú–µ—Ç–æ–¥ {self.method} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
    
    def _get_word2vec_vectors(self, texts: List[str]) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ Word2Vec"""
        if not self.embedding_model:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å Word2Vec –Ω–µ –æ–±—É—á–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞—é –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã")
            return np.zeros((len(texts), self.vector_size))
        
        vectors = []
        for text in texts:
            words = text.split()
            word_vectors = []
            
            for word in words:
                try:
                    vec = self.embedding_model.wv[word]
                    word_vectors.append(vec)
                except KeyError:
                    continue
            
            if word_vectors:
                doc_vector = np.mean(word_vectors, axis=0)
            else:
                doc_vector = np.zeros(self.vector_size)
            
            vectors.append(doc_vector)
        
        return np.array(vectors)
    
    def _get_fasttext_vectors(self, texts: List[str]) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ FastText"""
        if not self.embedding_model:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å FastText –Ω–µ –æ–±—É—á–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞—é –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã")
            return np.zeros((len(texts), self.vector_size))
        
        vectors = []
        for text in texts:
            words = text.split()
            word_vectors = []
            
            for word in words:
                try:
                    vec = self.embedding_model.wv[word]
                    word_vectors.append(vec)
                except KeyError:
                    try:
                        vec = self.embedding_model.wv.get_vector(word)
                        word_vectors.append(vec)
                    except:
                        continue
            
            if word_vectors:
                doc_vector = np.mean(word_vectors, axis=0)
            else:
                doc_vector = np.zeros(self.vector_size)
            
            vectors.append(doc_vector)
        
        return np.array(vectors)
    
    def _get_bert_vectors(self, texts: List[str]) -> np.ndarray:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ BERT"""
        if not self.model or not self.tokenizer:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å BERT –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞—é –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã")
            return np.zeros((len(texts), self.vector_size))
        
        vectors = []
        
        for text in texts:
            inputs = self.tokenizer(text, return_tensors='pt', 
                                   truncation=True, max_length=512,
                                   padding='max_length')
            
            with torch.no_grad():
                outputs = self.model(**inputs)
                cls_embedding = outputs.last_hidden_state[0, 0, :].numpy()
                vectors.append(cls_embedding)
        
        return np.array(vectors)
    
    def fit_transform(self, texts: List[str]) -> np.ndarray:
        """
        –û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –¥–ª—è train!)
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            
        Returns:
            –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        if self.vectorizer:
            self.fit(texts)
        return self.transform(texts)
    
    def save(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        with open(filepath, 'wb') as f:
            pickle.dump(self, f)
    
    @classmethod
    def load(cls, filepath: str) -> 'SplitAwareVectorizer':
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞"""
        with open(filepath, 'rb') as f:
            return pickle.load(f)


class TextDataProcessor:
    """
    –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ train/validation/test
    """
    
    def __init__(self, 
                 preprocessor_params: Dict = None,
                 vectorizer_params: Dict = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
        
        Args:
            preprocessor_params: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
            vectorizer_params: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
        """
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if preprocessor_params is None:
            preprocessor_params = {
                'language': 'russian',
                'remove_stopwords': True,
                'use_spacy': True
            }
        
        if vectorizer_params is None:
            vectorizer_params = {
                'method': 'tfidf',
                'max_features': 5000
            }
        
        self.preprocessor = TextPreprocessor(**preprocessor_params)
        self.feature_extractor = FeatureExtractor()
        self.vectorizer = None
        self.vectorizer_params = vectorizer_params
        
        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.processed_data = {
            'train': {},
            'validation': {},
            'test': {}
        }
        self.vectorization_successful = True
        self.fallback_to_tfidf = False
    
    def process_splits(self, 
                      splits: Dict[str, List[Dict]],
                      extract_meta: bool = True,
                      create_vectors: bool = True,
                      text_field: str = 'text') -> Dict[str, Dict]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö (train/validation/test)
        
        Args:
            splits: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–∞–∑–±–∏—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ {'train': [...], 'validation': [...], 'test': [...]}
            extract_meta: –∏–∑–≤–ª–µ–∫–∞—Ç—å –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
            create_vectors: —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            text_field: –ø–æ–ª–µ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
        """
        print("üîß –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        
        results = {}
        
        try:
            # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ TRAIN –¥–∞–Ω–Ω—ã—Ö (–æ–±—É—á–∞–µ–º –Ω–∞ –Ω–∏—Ö)
            print("\n1Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ TRAIN –¥–∞–Ω–Ω—ã—Ö (–æ–±—É—á–µ–Ω–∏–µ)...")
            train_texts = self._extract_texts(splits['train'], text_field)
            train_processed = self._process_split(
                'train', train_texts, 
                extract_meta=extract_meta, 
                create_vectors=create_vectors,
                fit_vectorizer=True
            )
            results['train'] = train_processed
            
            # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ VALIDATION –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä)
            print("\n2Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ VALIDATION –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ)...")
            val_texts = self._extract_texts(splits['validation'], text_field)
            val_processed = self._process_split(
                'validation', val_texts,
                extract_meta=extract_meta,
                create_vectors=create_vectors,
                fit_vectorizer=False
            )
            results['validation'] = val_processed
            
            # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ TEST –¥–∞–Ω–Ω—ã—Ö (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä)
            print("\n3Ô∏è‚É£ –û–±—Ä–∞–±–æ—Ç–∫–∞ TEST –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ)...")
            test_texts = self._extract_texts(splits['test'], text_field)
            test_processed = self._process_split(
                'test', test_texts,
                extract_meta=extract_meta,
                create_vectors=create_vectors,
                fit_vectorizer=False
            )
            results['test'] = test_processed
            
            print("\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–º {self.vectorizer_params.get('method')}: {e}")
            
            # –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TF-IDF –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
            if self.vectorizer_params.get('method') != 'tfidf':
                print("üîÑ –ü—Ä–æ–±—É–µ–º –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ TF-IDF –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥...")
                self.fallback_to_tfidf = True
                self.vectorizer_params['method'] = 'tfidf'
                self.vectorizer = None
                
                # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø–æ–ø—ã—Ç–∫—É —Å TF-IDF
                return self.process_splits(splits, extract_meta, create_vectors, text_field)
            else:
                # –ï—Å–ª–∏ —É–∂–µ TF-IDF –∏ –≤—Å–µ —Ä–∞–≤–Ω–æ –æ—à–∏–±–∫–∞
                print("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö")
                raise
        
        return results
    
    def process_splits_with_fallback(self, 
                                   splits: Dict[str, List[Dict]],
                                   extract_meta: bool = True,
                                   create_vectors: bool = True,
                                   text_field: str = 'text') -> Dict[str, Dict]:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º
        
        –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, 
        –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è –Ω–∞ TF-IDF
        """
        print("üîß –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–µ–∑–µ—Ä–≤–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º...")
        
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
            return self.process_splits(splits, extract_meta, create_vectors, text_field)
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –º–µ—Ç–æ–¥–æ–º {self.vectorizer_params.get('method')}: {e}")
            print("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –º–µ—Ç–æ–¥ TF-IDF...")
            
            # –ú–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ –Ω–∞ TF-IDF
            self.vectorizer_params['method'] = 'tfidf'
            self.vectorizer = None  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
            
            # –ü—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
            return self.process_splits(splits, extract_meta, create_vectors, text_field)
    
    def _extract_texts(self, data: List[Dict], text_field: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        texts = []
        for item in data:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è —Å —Ç–µ–∫—Å—Ç–æ–º
            text = (item.get(text_field) or 
                   item.get('text') or 
                   item.get('–æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç') or 
                   item.get('content') or 
                   '')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
            title = (item.get('title') or 
                    item.get('–∑–∞–≥–æ–ª–æ–≤–æ–∫') or 
                    item.get('headline') or 
                    '')
            
            if title and text:
                combined = f"{title}. {text}"
            elif title:
                combined = title
            else:
                combined = text
            
            texts.append(combined)
        
        return texts
    
    def _process_split(self, 
                      split_name: str,
                      texts: List[str],
                      extract_meta: bool = True,
                      create_vectors: bool = True,
                      fit_vectorizer: bool = True) -> Dict:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            split_name: –∏–º—è —Ä–∞–∑–¥–µ–ª–∞ ('train', 'validation', 'test')
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            extract_meta: –∏–∑–≤–ª–µ–∫–∞—Ç—å –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
            create_vectors: —Å–æ–∑–¥–∞–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            fit_vectorizer: –æ–±—É—á–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä (True) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—ã–π (False)
            
        Returns:
            —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        print(f"  üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ {split_name}: {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        
        result = {}
        
        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        print(f"    1Ô∏è‚É£ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞...")
        processed_texts = self.preprocessor.preprocess_batch(texts, return_string=True)
        result['processed_texts'] = processed_texts
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if extract_meta:
            print(f"    2Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            meta_features = self.feature_extractor.extract_batch_features(processed_texts)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            if split_name == 'train' or not hasattr(self.feature_extractor.scaler, 'mean_'):
                meta_array = self.feature_extractor.normalize_features(meta_features, fit=(split_name == 'train'))
            else:
                meta_array = self.feature_extractor.normalize_features(meta_features, fit=False)
            
            result['meta_features'] = meta_features
            result['meta_features_array'] = meta_array
            print(f"       –ò–∑–≤–ª–µ—á–µ–Ω–æ {meta_features.shape[1]} –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # 3. –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π
        if create_vectors:
            print(f"    3Ô∏è‚É£ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π...")
            try:
                # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
                if self.vectorizer is None:
                    self.vectorizer = SplitAwareVectorizer(**self.vectorizer_params)
                
                if fit_vectorizer:
                    # –û–±—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è train)
                    text_vectors = self.vectorizer.fit_transform(processed_texts)
                    print(f"      –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –æ–±—É—á–µ–Ω –Ω–∞ {split_name} –¥–∞–Ω–Ω—ã—Ö")
                else:
                    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
                    text_vectors = self.vectorizer.transform(processed_texts)
                    print(f"      –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ–±—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä")
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –≤ –ø–ª–æ—Ç–Ω—ã–µ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                if hasattr(text_vectors, 'toarray'):
                    text_vectors = text_vectors.toarray()
                
                result['text_vectors'] = text_vectors
                print(f"       –°–æ–∑–¥–∞–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {text_vectors.shape}")
                
            except Exception as e:
                print(f"      ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")
                result['text_vectors'] = None
        
        # 4. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–µ—Å–ª–∏ –æ–±–∞ —Ç–∏–ø–∞ –∏–∑–≤–ª–µ—á–µ–Ω—ã)
        if extract_meta and create_vectors and result.get('meta_features_array') is not None and result.get('text_vectors') is not None:
            print(f"    4Ô∏è‚É£ –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            combined = np.hstack([result['meta_features_array'], result['text_vectors']])
            result['combined_features'] = combined
            print(f"       –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {combined.shape}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ–±—â–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.processed_data[split_name] = result
        
        return result
    
    def get_dense_features(self, splits_results=None):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –ø–ª–æ—Ç–Ω—ã–µ
        
        Args:
            splits_results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–¥–µ–ª–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–ª–æ—Ç–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        dense_results = {}
        
        if splits_results is None and hasattr(self, 'results'):
            splits_results = self.results
        
        if not splits_results:
            return dense_results
        
        for split_name, split_data in splits_results.items():
            dense_results[split_name] = {}
            
            # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            for key, value in split_data.items():
                if key != 'text_vectors':
                    dense_results[split_name][key] = value
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã
            if 'text_vectors' in split_data and split_data['text_vectors'] is not None:
                vectors = split_data['text_vectors']
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã –≤ –ø–ª–æ—Ç–Ω—ã–µ
                if hasattr(vectors, 'toarray'):
                    dense_vectors = vectors.toarray()
                else:
                    dense_vectors = vectors
                
                dense_results[split_name]['text_vectors'] = dense_vectors
                dense_results[split_name]['text_vectors_dense'] = True
            
            # –¢–∞–∫–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º combined_features
            if 'combined_features' in split_data and split_data['combined_features'] is not None:
                combined = split_data['combined_features']
                
                if hasattr(combined, 'toarray'):
                    dense_combined = combined.toarray()
                else:
                    dense_combined = combined
                
                dense_results[split_name]['combined_features'] = dense_combined
        
        return dense_results
    
    def get_processed_texts(self, split_name: str) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞"""
        return self.processed_data.get(split_name, {}).get('processed_texts', [])
    
    def get_features(self, split_name: str, feature_type: str = 'combined') -> Optional[np.ndarray]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–∞
        
        Args:
            split_name: –∏–º—è —Ä–∞–∑–¥–µ–ª–∞
            feature_type: —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ('meta', 'text', 'combined')
            
        Returns:
            –º–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–ª–∏ None
        """
        data = self.processed_data.get(split_name, {})
        
        if feature_type == 'meta':
            return data.get('meta_features_array')
        elif feature_type == 'text':
            return data.get('text_vectors')
        elif feature_type == 'combined':
            return data.get('combined_features')
        else:
            return None
    
    def save_processed_data(self, output_dir: str = "processed_data"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –¥–∏—Å–∫"""
        os.makedirs(output_dir, exist_ok=True)
        
        for split_name, data in self.processed_data.items():
            if data:
                split_dir = os.path.join(output_dir, split_name)
                os.makedirs(split_dir, exist_ok=True)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                if 'processed_texts' in data:
                    with open(os.path.join(split_dir, 'texts.json'), 'w', encoding='utf-8') as f:
                        json.dump(data['processed_texts'], f, ensure_ascii=False, indent=2)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
                if 'meta_features' in data:
                    data['meta_features'].to_csv(os.path.join(split_dir, 'meta_features.csv'), index=False)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä—ã
                if 'text_vectors' in data and data['text_vectors'] is not None:
                    np.save(os.path.join(split_dir, 'text_vectors.npy'), data['text_vectors'])
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                if 'combined_features' in data and data['combined_features'] is not None:
                    np.save(os.path.join(split_dir, 'combined_features.npy'), data['combined_features'])
                
                print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {split_name} –≤ {split_dir}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        if self.vectorizer:
            self.vectorizer.save(os.path.join(output_dir, 'vectorizer.pkl'))
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä")
    
    def load_processed_data(self, input_dir: str = "processed_data"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –¥–∏—Å–∫–∞"""
        for split_name in ['train', 'validation', 'test']:
            split_dir = os.path.join(input_dir, split_name)
            if os.path.exists(split_dir):
                data = {}
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—Å—Ç—ã
                texts_file = os.path.join(split_dir, 'texts.json')
                if os.path.exists(texts_file):
                    with open(texts_file, 'r', encoding='utf-8') as f:
                        data['processed_texts'] = json.load(f)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–∏
                meta_file = os.path.join(split_dir, 'meta_features.csv')
                if os.path.exists(meta_file):
                    data['meta_features'] = pd.read_csv(meta_file)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã
                vectors_file = os.path.join(split_dir, 'text_vectors.npy')
                if os.path.exists(vectors_file):
                    data['text_vectors'] = np.load(vectors_file)
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                combined_file = os.path.join(split_dir, 'combined_features.npy')
                if os.path.exists(combined_file):
                    data['combined_features'] = np.load(combined_file)
                
                self.processed_data[split_name] = data
                print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {split_name}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
        vectorizer_file = os.path.join(input_dir, 'vectorizer.pkl')
        if os.path.exists(vectorizer_file):
            self.vectorizer = SplitAwareVectorizer.load(vectorizer_file)
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä")


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def example_usage_with_splits():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    splits = {
        'train': [
            {'text': '–≠—Ç–æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–æ–º–µ—Ä –æ–¥–∏–Ω. –û–Ω –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.', 'category': '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'},
            {'text': '–ï—â–µ –æ–¥–∏–Ω —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.', 'category': '–Ω–∞—É–∫–∞'},
        ],
        'validation': [
            {'text': '–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏.', 'category': '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏'},
        ],
        'test': [
            {'text': '–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.', 'category': '–Ω–∞—É–∫–∞'},
        ]
    }
    
    print("üß™ –ü—Ä–∏–º–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 60)
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = TextDataProcessor(
        preprocessor_params={
            'language': 'russian',
            'remove_stopwords': True,
            'use_spacy': False
        },
        vectorizer_params={
            'method': 'tfidf',
            'max_features': 100,
            'ngram_range': (1, 1)
        }
    )
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã
    results = processor.process_splits(
        splits,
        extract_meta=True,
        create_vectors=True
    )
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for split_name, result in results.items():
        print(f"\nüìä {split_name.upper()}:")
        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(result.get('processed_texts', []))}")
        
        if 'meta_features' in result:
            print(f"  –ú–µ—Ç–∞-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {result['meta_features'].shape[1]}")
        
        if 'text_vectors' in result and result['text_vectors'] is not None:
            print(f"  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {result['text_vectors'].shape[1]}")
        
        if 'combined_features' in result and result['combined_features'] is not None:
            print(f"  –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {result['combined_features'].shape[1]}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    processor.save_processed_data("example_processed")


if __name__ == "__main__":
    example_usage_with_splits()