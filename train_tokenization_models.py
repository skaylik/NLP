import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tokenizers import Tokenizer
from tokenizers.models import BPE, WordPiece, Unigram
from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer
from tokenizers.pre_tokenizers import Whitespace
import json
from typing import List
import re
import numpy as np

class SubwordModelExperiment:
    def __init__(self, texts: List[str]):
        self.texts = texts
        self.results = []
    
    def _reconstruct_wordpiece_text(self, tokens):
        """–ö–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è WordPiece"""
        text = ""
        for token in tokens:
            if token.startswith('##'):
                # –°–∫–ª–µ–∏–≤–∞–µ–º —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Å–ª–æ–≤–æ–º
                text += token[2:]
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ –Ω–æ–≤—ã–º —Å–ª–æ–≤–æ–º
                if text:
                    text += " "
                text += token
        return text
    
    def debug_reconstruction(self, tokenizer, sample_texts, model_name):
        """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        print(f"\n--- –û–¢–õ–ê–î–ö–ê –†–ï–ö–û–ù–°–¢–†–£–ö–¶–ò–ò –î–õ–Ø {model_name} ---")
        
        for i, text in enumerate(sample_texts[:3]):
            print(f"\n–ü—Ä–∏–º–µ—Ä {i+1}:")
            print(f"–ò—Å—Ö–æ–¥–Ω—ã–π: {text[:100]}...")
            
            try:
                # –ö–æ–¥–∏—Ä—É–µ–º
                encoding = tokenizer.encode(text)
                tokens = encoding.tokens
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏
                model_type = str(type(tokenizer.model)).lower()
                if 'wordpiece' in model_type:
                    reconstructed = self._reconstruct_wordpiece_text(tokens)
                else:
                    reconstructed = tokenizer.decode(encoding.ids, skip_special_tokens=False)
                
                print(f"–¢–æ–∫–µ–Ω—ã: {tokens[:20]}...")
                print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π: {reconstructed[:100]}...")
                
                # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
                if self._texts_semantically_equal(text, reconstructed):
                    print("‚úÖ –°–û–í–ü–ê–î–ê–ï–¢ (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏)")
                else:
                    print("‚ùå –ù–ï –°–û–í–ü–ê–î–ê–ï–¢")
                    print(f"–†–∞–∑–Ω–∏—Ü–∞: '{text[:60]}' vs '{reconstructed[:60]}'")
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    
    def _texts_semantically_equal(self, text1, text2):
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é, –∞ –Ω–µ –ø–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—é"""
        def normalize(t):
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            t = re.sub(r'\s+', '', t)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
            t = re.sub(r'<\s*NUM\s*>', '<NUM>', t)
            t = re.sub(r'<\s*URL\s*>', '<URL>', t)
            t = re.sub(r'<\s*EMAIL\s*>', '<EMAIL>', t)
            return t
        
        text1_norm = normalize(text1)
        text2_norm = normalize(text2)
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ (—á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏)
        compare_length = min(100, len(text1_norm), len(text2_norm))
        
        return text1_norm[:compare_length] == text2_norm[:compare_length]

    def train_bpe(self, vocab_size: int, min_frequency: int = 2) -> Tokenizer:
        """–û–±—É—á–µ–Ω–∏–µ BPE –º–æ–¥–µ–ª–∏"""
        try:
            tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
            tokenizer.pre_tokenizer = Whitespace()
            
            trainer = BpeTrainer(
                vocab_size=vocab_size,
                min_frequency=min_frequency,
                special_tokens=["[UNK]", "<NUM>", "<URL>", "<EMAIL>"]
            )
            
            tokenizer.train_from_iterator(self.texts, trainer)
            return tokenizer
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ train_bpe: {e}")
            return None
    
    def train_wordpiece(self, vocab_size: int, min_frequency: int = 2) -> Tokenizer:
        """–û–±—É—á–µ–Ω–∏–µ WordPiece –º–æ–¥–µ–ª–∏"""
        try:
            tokenizer = Tokenizer(WordPiece(unk_token="[UNK]"))
            tokenizer.pre_tokenizer = Whitespace()
            
            trainer = WordPieceTrainer(
                vocab_size=vocab_size,
                min_frequency=min_frequency,
                special_tokens=["[UNK]", "<NUM>", "<URL>", "<EMAIL>"]
            )
            
            tokenizer.train_from_iterator(self.texts, trainer)
            return tokenizer
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ train_wordpiece: {e}")
            return None
    
    def train_unigram(self, vocab_size: int, min_frequency: int = 2):
        """–û–±—É—á–µ–Ω–∏–µ Unigram –º–æ–¥–µ–ª–∏"""
        try:
            tokenizer = Tokenizer(Unigram())
            tokenizer.pre_tokenizer = Whitespace()
            
            trainer = UnigramTrainer(
                vocab_size=vocab_size,
                unk_token="[UNK]",
                special_tokens=["[UNK]", "<NUM>", "<URL>", "<EMAIL>"],
                shrinking_factor=0.75
            )
            
            tokenizer.train_from_iterator(self.texts, trainer)
            return tokenizer
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ train_unigram: {e}")
            return None

    def calculate_fragmentation_rate(self, tokenizer, sample_texts: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–ª–æ–≤"""
        try:
            fragmented_count = 0
            total_words = 0
            
            for text in sample_texts:
                words = text.split()
                total_words += len(words)
                
                for word in words:
                    if word in ['<NUM>', '<URL>', '<EMAIL>']:
                        continue
                        
                    tokens = tokenizer.encode(word).tokens
                    if len(tokens) >= 2:
                        fragmented_count += 1
            
            return (fragmented_count / total_words) * 100 if total_words > 0 else 0
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ calculate_fragmentation_rate: {e}")
            return 0
    
    def calculate_compression_ratio(self, tokenizer, sample_texts: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ —Å–∂–∞—Ç–∏—è"""
        try:
            total_original_words = 0
            total_tokens = 0
            
            for text in sample_texts:
                words = text.split()
                total_original_words += len(words)
                
                tokens = tokenizer.encode(text).tokens
                total_tokens += len(tokens)
            
            return total_original_words / total_tokens if total_tokens > 0 else 0
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ calculate_compression_ratio: {e}")
            return 0
    
    def calculate_reconstruction_efficiency(self, tokenizer, sample_texts: List[str]) -> float:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        try:
            correct_reconstructions = 0
            total_texts = len(sample_texts)
            
            for i, text in enumerate(sample_texts):
                try:
                    encoding = tokenizer.encode(text)
                    tokens = encoding.tokens
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–µ—Ç–æ–¥–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
                    model_type = str(type(tokenizer.model)).lower()
                    
                    # –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è WordPiece
                    if 'wordpiece' in model_type:
                        reconstructed = self._reconstruct_wordpiece_text(tokens)
                    else:
                        reconstructed = tokenizer.decode(encoding.ids, skip_special_tokens=False)
                    
                    if self._texts_semantically_equal(text, reconstructed):
                        correct_reconstructions += 1
                    elif i < 2:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏—è
                        print(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ {i+1}:")
                        print(f"  –û—Ä–∏–≥–∏–Ω–∞–ª: {text[:80]}")
                        print(f"  –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {reconstructed[:80]}")
                        print(f"  –¢–æ–∫–µ–Ω—ã: {tokens[:15]}")
                            
                except Exception as e:
                    if i < 2:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ {i+1}: {e}")
                    continue
            
            efficiency = (correct_reconstructions / total_texts) * 100 if total_texts > 0 else 0
            print(f"–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {efficiency}% ({correct_reconstructions}/{total_texts})")
            return efficiency
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≤ calculate_reconstruction_efficiency: {e}")
            return 0

    def analyze_vocabulary_coverage(self, tokenizer, sample_texts: List[str]):
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è —Å–ª–æ–≤–∞—Ä—è"""
        vocab = tokenizer.get_vocab()
        print(f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(vocab)}")
        
        # –ê–Ω–∞–ª–∏–∑ OOV (out-of-vocabulary) —Ç–æ–∫–µ–Ω–æ–≤
        oov_count = 0
        total_tokens = 0
        
        for text in sample_texts[:10]:  # –ù–∞ –≤—ã–±–æ—Ä–∫–µ –∏–∑ 10 —Ç–µ–∫—Å—Ç–æ–≤
            encoding = tokenizer.encode(text)
            tokens = encoding.tokens
            total_tokens += len(tokens)
            oov_count += tokens.count('[UNK]')
        
        oov_rate = (oov_count / total_tokens) * 100 if total_tokens > 0 else 0
        print(f"OOV rate: {oov_rate:.2f}%")
        return oov_rate

    def run_experiment(self):
        """–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞"""
        vocab_sizes = [8000, 16000, 32000]
        min_frequencies = [2]
        
        test_texts = self.texts[:min(50, len(self.texts))]
        debug_texts = self.texts[:3]
        
        print(f"–ù–∞—á–∞–ª–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ —Å {len(test_texts)} —Ç–µ—Å—Ç–æ–≤—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏")
        
        for vocab_size in vocab_sizes:
            for min_freq in min_frequencies:
                print(f"\n=== –û–±—É—á–µ–Ω–∏–µ: vocab_size={vocab_size}, min_frequency={min_freq} ===")
                
                # BPE
                print("–û–±—É—á–µ–Ω–∏–µ BPE...")
                bpe_tokenizer = self.train_bpe(vocab_size, min_freq)
                if bpe_tokenizer:
                    self.debug_reconstruction(bpe_tokenizer, debug_texts, f"BPE_{vocab_size}")
                    
                    bpe_fragmentation = self.calculate_fragmentation_rate(bpe_tokenizer, test_texts)
                    bpe_compression = self.calculate_compression_ratio(bpe_tokenizer, test_texts)
                    bpe_reconstruction = self.calculate_reconstruction_efficiency(bpe_tokenizer, test_texts[:10])
                    
                    self.results.append({
                        '–ú–æ–¥–µ–ª—å': 'BPE',
                        '–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è': vocab_size,
                        '–ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞': min_freq,
                        '–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)': round(bpe_fragmentation, 2),
                        '–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è': round(bpe_compression, 3),
                        '–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (%)': round(bpe_reconstruction, 2)
                    })
                else:
                    print("BPE: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å")
                
                # WordPiece
                print("–û–±—É—á–µ–Ω–∏–µ WordPiece...")
                wp_tokenizer = self.train_wordpiece(vocab_size, min_freq)
                if wp_tokenizer:
                    self.debug_reconstruction(wp_tokenizer, debug_texts, f"WordPiece_{vocab_size}")
                    
                    wp_fragmentation = self.calculate_fragmentation_rate(wp_tokenizer, test_texts)
                    wp_compression = self.calculate_compression_ratio(wp_tokenizer, test_texts)
                    wp_reconstruction = self.calculate_reconstruction_efficiency(wp_tokenizer, test_texts[:10])
                    
                    self.results.append({
                        '–ú–æ–¥–µ–ª—å': 'WordPiece',
                        '–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è': vocab_size,
                        '–ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞': min_freq,
                        '–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)': round(wp_fragmentation, 2),
                        '–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è': round(wp_compression, 3),
                        '–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (%)': round(wp_reconstruction, 2)
                    })
                else:
                    print("WordPiece: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å")
                
                # Unigram
                print("–û–±—É—á–µ–Ω–∏–µ Unigram...")
                unigram_tokenizer = self.train_unigram(vocab_size, min_freq)
                if unigram_tokenizer:
                    self.debug_reconstruction(unigram_tokenizer, debug_texts, f"Unigram_{vocab_size}")
                    
                    unigram_fragmentation = self.calculate_fragmentation_rate(unigram_tokenizer, test_texts)
                    unigram_compression = self.calculate_compression_ratio(unigram_tokenizer, test_texts)
                    unigram_reconstruction = self.calculate_reconstruction_efficiency(unigram_tokenizer, test_texts[:10])
                    
                    self.results.append({
                        '–ú–æ–¥–µ–ª—å': 'Unigram',
                        '–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è': vocab_size,
                        '–ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞': min_freq,
                        '–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)': round(unigram_fragmentation, 2),
                        '–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è': round(unigram_compression, 3),
                        '–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (%)': round(unigram_reconstruction, 2)
                    })
                else:
                    print("Unigram: –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å")
        
        print(f"\n–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω. –ü–æ–ª—É—á–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(self.results)}")
    
    def save_results(self, filename: str = 'subword_models_metrics.csv'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return None
            
        df = pd.DataFrame(self.results)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        return df

    def visualize_results(self):
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
            return
        
        df = pd.DataFrame(self.results)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–≥—É—Ä—É —Å 4 subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–¥—Å–ª–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏', fontsize=16)
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        colors = {'BPE': 'blue', 'WordPiece': 'red', 'Unigram': 'green'}
        markers = {'BPE': 'o', 'WordPiece': 's', 'Unigram': '^'}
        
        # –ì—Ä–∞—Ñ–∏–∫ 1: –§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        for model in df['–ú–æ–¥–µ–ª—å'].unique():
            model_data = df[df['–ú–æ–¥–µ–ª—å'] == model]
            axes[0,0].plot(model_data['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è'], model_data['–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)'], 
                    marker=markers[model], label=model, linewidth=2, color=colors[model])
            # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            for idx, row in model_data.iterrows():
                axes[0,0].annotate(f"{row['–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)']}%", 
                            (row['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è'], row['–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)']),
                            textcoords="offset points", xytext=(0,5), ha='center', fontsize=8)
        
        axes[0,0].set_title('–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å–ª–æ–≤ (–º–µ–Ω—å—à–µ ‚Üí –ª—É—á—à–µ)')
        axes[0,0].set_xlabel('–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è')
        axes[0,0].set_ylabel('–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)')
        axes[0,0].legend()
        axes[0,0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 2: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è
        for model in df['–ú–æ–¥–µ–ª—å'].unique():
            model_data = df[df['–ú–æ–¥–µ–ª—å'] == model]
            axes[0,1].plot(model_data['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è'], model_data['–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è'], 
                    marker=markers[model], label=model, linewidth=2, color=colors[model])
            for idx, row in model_data.iterrows():
                axes[0,1].annotate(f"{row['–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è']}x", 
                            (row['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è'], row['–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è']),
                            textcoords="offset points", xytext=(0,5), ha='center', fontsize=8)
        
        axes[0,1].set_title('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è (–±–æ–ª—å—à–µ ‚Üí –ª—É—á—à–µ)')
        axes[0,1].set_xlabel('–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è')
        axes[0,1].set_ylabel('–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è')
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 3: –†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (—Å—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏)
        models = df['–ú–æ–¥–µ–ª—å'].unique()
        vocab_sizes = sorted(df['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è'].unique())
        
        bar_width = 0.25
        x_pos = np.arange(len(vocab_sizes))
        
        for i, model in enumerate(models):
            model_data = df[df['–ú–æ–¥–µ–ª—å'] == model]
            values = []
            for size in vocab_sizes:
                size_data = model_data[model_data['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è'] == size]
                if not size_data.empty:
                    values.append(size_data['–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (%)'].values[0])
                else:
                    values.append(0)
            
            axes[1,0].bar(x_pos + i * bar_width, values, bar_width, label=model, color=colors[model])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ–≤–µ—Ä—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
            for j, v in enumerate(values):
                axes[1,0].text(x_pos[j] + i * bar_width, v + 1, f"{v}%", 
                        ha='center', va='bottom', fontsize=9)
        
        axes[1,0].set_title('–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–±–æ–ª—å—à–µ ‚Üí –ª—É—á—à–µ)')
        axes[1,0].set_xlabel('–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è')
        axes[1,0].set_ylabel('–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (%)')
        axes[1,0].set_xticks(x_pos + bar_width)
        axes[1,0].set_xticklabels(vocab_sizes)
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        
        # –ì—Ä–∞—Ñ–∏–∫ 4: –°–≤–æ–¥–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π score)
        axes[1,1].axis('off')  # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        plt.tight_layout()
        plt.savefig('subword_models_analysis_improved.png', dpi=300, bbox_inches='tight')
        plt.show()

    def calculate_additional_metrics(self, tokenizer, sample_texts: List[str]):
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        metrics = {}
        
        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–æ–∫–µ–Ω–∞
        total_chars = 0
        total_tokens = 0
        
        for text in sample_texts:
            encoding = tokenizer.encode(text)
            tokens = encoding.tokens
            total_tokens += len(tokens)
            total_chars += sum(len(token) for token in tokens)
        
        metrics['avg_token_length'] = total_chars / total_tokens if total_tokens > 0 else 0
        
        # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        special_tokens = 0
        for text in sample_texts:
            encoding = tokenizer.encode(text)
            tokens = encoding.tokens
            special_tokens += sum(1 for token in tokens if token in ['[UNK]', '<NUM>', '<URL>', '<EMAIL>'])
        
        metrics['special_tokens_rate'] = (special_tokens / total_tokens) * 100 if total_tokens > 0 else 0
        
        return metrics

def load_texts_from_jsonl(file_path: str, max_texts: int = 50) -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ JSONL —Ñ–∞–π–ª–∞"""
    texts = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= max_texts:
                    break
                if line.strip():
                    article = json.loads(line)
                    if 'text' in article and article['text'].strip():
                        texts.append(article['text'])
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ {file_path}")
        return texts
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return []

def analyze_results(df):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    if df.empty:
        print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        return
    
    print("\n" + "="*60)
    print("–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*60)
    
    # –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ
    metrics = ['–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)', '–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è', '–†–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è (%)']
    
    for metric in metrics:
        if metric == '–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è (%)':
            best_idx = df[metric].idxmin()
            best_value = df.loc[best_idx, metric]
            print(f"\nüèÜ –õ—É—á—à–∞—è {metric}: {best_value}%")
        else:
            best_idx = df[metric].idxmax()
            best_value = df.loc[best_idx, metric]
            unit = 'x' if metric == '–ö–æ—ç—Ñ. —Å–∂–∞—Ç–∏—è' else '%'
            print(f"\nüèÜ –õ—É—á—à–∞—è {metric}: {best_value}{unit}")
        
        best_row = df.iloc[best_idx]
        print(f"   –ú–æ–¥–µ–ª—å: {best_row['–ú–æ–¥–µ–ª—å']}")
        print(f"   –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {best_row['–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è']}")
        print(f"   –ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞: {best_row['–ú–∏–Ω. —á–∞—Å—Ç–æ—Ç–∞']}")

# –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    texts = load_texts_from_jsonl('indicator_ru_corpus_advanced_cleaned.jsonl', max_texts=50)
    
    if not texts:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–µ–∫—Å—Ç—ã. –°–æ–∑–¥–∞–µ–º –¥–µ–º–æ-–¥–∞–Ω–Ω—ã–µ...")
        texts = [
            "–±–∏–æ–ª–æ–≥–∏—è –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ <NUM> –∏—é–ª—è <NUM> <NUM>:<NUM> <NUM> –º–∏–Ω. a a —Ä–∞—á–∫–∏-–±–æ–∫–æ–ø–ª–∞–≤—ã –Ω–æ—Å—è—Ç —Å–∞–º–æ–∫ –ª–∞–ø–∫–∞—Ö",
            "–º–µ–¥–∏—Ü–∏–Ω–∞ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ <NUM> –∏—é–ª—è <NUM> <NUM>:<NUM> <NUM> –º–∏–Ω. a a —ç–∫—Å–ø–µ—Ä—Ç—ã –Ω–∞–∑–≤–∞–ª–∏ –Ω–∞–∏–±–æ–ª–µ–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω—ã–µ",
            "–≥—É–º–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ –Ω–∞—É–∫–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ <NUM> –∏—é–ª—è <NUM> <NUM>:<NUM> <NUM> –º–∏–Ω. a a —Å—Ç–∞—Ä–æ–π —Ä—É—Å—Å–µ –Ω–∞—à–ª–∏ –ø–µ—Ä–≤—É—é"
        ] * 10
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞")
    
    # –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    experiment = SubwordModelExperiment(texts)
    experiment.run_experiment()
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results_df = experiment.save_results()
    
    if results_df is not None and not results_df.empty:
        print("\n" + "="*60)
        print("–°–í–û–î–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print("="*60)
        print(results_df.to_string(index=False))
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        experiment.visualize_results()
        
        # –ê–Ω–∞–ª–∏–∑
        analyze_results(results_df)
    else:
        print("–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")