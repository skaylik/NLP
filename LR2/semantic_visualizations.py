# semantic_visualizations.py
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import pandas as pd
from typing import List, Tuple, Dict, Any, Optional
import re
from collections import defaultdict

class SemanticVisualizations:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –∏ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self, models):
        self.models = models
    
    def _get_word_vector(self, word: str, model) -> tuple:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ —Å–ª–æ–≤–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OOV –¥–ª—è FastText
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (–≤–µ–∫—Ç–æ—Ä, —Å—Ç–∞—Ç—É—Å) –≥–¥–µ —Å—Ç–∞—Ç—É—Å: 'found', 'oov_fasttext', 'not_found'
        """
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if word in model.wv:
            return (model.wv[word].copy(), 'found')
        
        # –î–ª—è FastText –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –≤–µ–∫—Ç–æ—Ä —á–µ—Ä–µ–∑ OOV
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ FastText –º–æ–¥–µ–ª—å (–∏–º–µ–µ—Ç –º–µ—Ç–æ–¥ get_vector –¥–ª—è OOV)
        is_fasttext = False
        try:
            from gensim.models import FastText as FastTextModel
            is_fasttext = isinstance(model, FastTextModel)
        except:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–æ–¥–∞ get_vector –∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            is_fasttext = (hasattr(model, 'wv') and 
                          hasattr(model.wv, 'get_vector') and 
                          'fasttext' in str(type(model)).lower())
        
        if is_fasttext:
            try:
                # FastText –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å OOV —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ subword information
                vector = model.wv.get_vector(word, norm=True)
                return (vector, 'oov_fasttext')
            except KeyError:
                pass
        
        # –ü—Ä–æ–±—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è (–ø–µ—Ä–≤–∞—è –±—É–∫–≤–∞ –∑–∞–≥–ª–∞–≤–Ω–∞—è/—Å—Ç—Ä–æ—á–Ω–∞—è)
        variants = [word.lower(), word.capitalize(), word.title()]
        for variant in variants:
            if variant != word and variant in model.wv:
                return (model.wv[variant].copy(), 'found')
        
        # –î–ª—è FastText –ø—Ä–æ–±—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —á–µ—Ä–µ–∑ OOV
        if is_fasttext:
            for variant in variants:
                if variant != word:
                    try:
                        vector = model.wv.get_vector(variant, norm=True)
                        return (vector, 'oov_fasttext')
                    except KeyError:
                        continue
        
        return (None, 'not_found')
    
    def parse_vector_expression(self, expression: str, model) -> Dict[str, Any]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ –≤—ã—Ä–∞–∂–µ–Ω–∏—è —Ç–∏–ø–∞ "–∫–æ—Ä–æ–ª—å - –º—É–∂—á–∏–Ω–∞ + –∂–µ–Ω—â–∏–Ω–∞"
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OOV —Å–ª–æ–≤–∞ –¥–ª—è FastText –º–æ–¥–µ–ª–µ–π
        """
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–µ
            expression = expression.strip()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏
            is_fasttext = False
            try:
                from gensim.models import FastText as FastTextModel
                is_fasttext = isinstance(model, FastTextModel)
            except:
                is_fasttext = (hasattr(model, 'wv') and 
                              hasattr(model.wv, 'get_vector') and 
                              'fasttext' in str(type(model)).lower())
            
            model_type = 'FastText' if is_fasttext else 'Word2Vec'
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏: —Å–ª–æ–≤–∞ –∏ –æ–ø–µ—Ä–∞—Ü–∏–∏
            parts = re.split(r'([+\-])', expression)
            parts = [p.strip() for p in parts if p.strip()]
            
            positive_words = []
            negative_words = []
            operation_steps = []
            oov_words = []  # –°–ª–æ–≤–∞, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ OOV
            missing_words = []  # –°–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏
            
            current_sign = '+'
            
            for part in parts:
                if part in ['+', '-']:
                    current_sign = part
                else:
                    word = part.strip()
                    vector, status = self._get_word_vector(word, model)
                    
                    if status == 'not_found':
                        missing_words.append(word)
                    else:
                        if current_sign == '+':
                            positive_words.append(word)
                        else:
                            negative_words.append(word)
                        
                        if status == 'oov_fasttext':
                            oov_words.append(word)
                        
                        operation_steps.append({
                            'word': word,
                            'operation': current_sign,
                            'vector': vector.copy(),
                            'status': status
                        })
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
            if missing_words:
                error_msg = f'–°–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ –º–æ–¥–µ–ª–∏: {", ".join(missing_words)}'
                if is_fasttext:
                    error_msg += f'\n\nüí° –î–ª—è FastText –º–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "{missing_words[0].lower()}" –∏–ª–∏ "{missing_words[0].capitalize()}")'
                else:
                    error_msg += f'\n\nüí° Word2Vec –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç OOV —Å–ª–æ–≤–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ FastText –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏.'
                return {'error': error_msg}
            
            if not positive_words and not negative_words:
                return {'error': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –º–æ–¥–µ–ª–∏'}
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
            intermediate_vectors = []
            
            # –®–∞–≥ 1: –ù–∞—á–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä (–ø–µ—Ä–≤–æ–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ)
            if positive_words:
                current_vector = model.wv[positive_words[0]].copy()
                intermediate_vectors.append({
                    'step': 1,
                    'description': f'–ù–∞—á–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä: {positive_words[0]}',
                    'vector': current_vector.copy(),
                    'norm': np.linalg.norm(current_vector)
                })
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ
                for word in positive_words[1:]:
                    current_vector += model.wv[word]
                    intermediate_vectors.append({
                        'step': len(intermediate_vectors) + 1,
                        'description': f'–î–æ–±–∞–≤–ª–µ–Ω–∏–µ: +{word}',
                        'vector': current_vector.copy(),
                        'norm': np.linalg.norm(current_vector)
                    })
            
            # –í—ã—á–∏—Ç–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ
            for word in negative_words:
                current_vector -= model.wv[word]
                intermediate_vectors.append({
                    'step': len(intermediate_vectors) + 1,
                    'description': f'–í—ã—á–∏—Ç–∞–Ω–∏–µ: -{word}',
                    'vector': current_vector.copy(),
                    'norm': np.linalg.norm(current_vector)
                })
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_vector = current_vector
            
            result = {
                'expression': expression,
                'positive_words': positive_words,
                'negative_words': negative_words,
                'intermediate_vectors': intermediate_vectors,
                'final_vector': final_vector,
                'final_norm': np.linalg.norm(final_vector),
                'operation_steps': operation_steps,
                'model_type': model_type
            }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± OOV —Å–ª–æ–≤–∞—Ö
            if oov_words:
                result['oov_words'] = oov_words
                result['info'] = f'‚ÑπÔ∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–ª–æ–≤–∞ ({", ".join(oov_words)}) –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —á–µ—Ä–µ–∑ OOV (FastText)'
            
            return result
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {str(e)}'}
    
    def visualize_vector_arithmetic(self, expression_result: Dict[str, Any], model, top_n: int = 10) -> Dict[str, Any]:
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        if 'error' in expression_result:
            return expression_result
        
        # –ü–æ–ª—É—á–∞–µ–º –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–≥–æ —à–∞–≥–∞
        neighbors_for_steps = []
        
        for step_data in expression_result['intermediate_vectors']:
            try:
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–ª—é—á –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∞
                neighbors = model.wv.similar_by_vector(
                    step_data['vector'], 
                    topn=top_n
                )
                neighbors_for_steps.append({
                    'step': step_data['step'],
                    'description': step_data['description'],
                    'neighbors': neighbors,
                    'vector_norm': step_data['norm']
                })
            except:
                neighbors_for_steps.append({
                    'step': step_data['step'],
                    'description': step_data['description'],
                    'neighbors': [],
                    'vector_norm': step_data['norm']
                })
        
        # –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        try:
            final_neighbors = model.wv.similar_by_vector(
                expression_result['final_vector'],
                topn=top_n
            )
        except:
            final_neighbors = []
        
        return {
            **expression_result,
            'step_neighbors': neighbors_for_steps,
            'final_neighbors': final_neighbors
        }
    
    def calculate_cosine_distance(self, word1: str, word2: str, model) -> Optional[float]:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É –¥–≤—É–º—è —Å–ª–æ–≤–∞–º–∏"""
        try:
            if word1 not in model.wv or word2 not in model.wv:
                return None
            
            vec1 = model.wv[word1]
            vec2 = model.wv[word2]
            
            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (1 - –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ)
            similarity = cosine_similarity([vec1], [vec2])[0][0]
            distance = 1 - similarity
            
            return {
                'word1': word1,
                'word2': word2,
                'cosine_similarity': float(similarity),
                'cosine_distance': float(distance),
                'euclidean_distance': float(np.linalg.norm(vec1 - vec2))
            }
        except Exception as e:
            return None
    
    def build_semantic_graph(self, words: List[str], model, threshold: float = 0.3) -> Dict[str, Any]:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π"""
        try:
            G = nx.Graph()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –≤ –º–æ–¥–µ–ª–∏
            valid_words = [w for w in words if w in model.wv]
            
            if len(valid_words) < 2:
                return {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞'}
            
            # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
            for word in valid_words:
                G.add_node(word)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ–ø–∞—Ä–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ä—ë–±—Ä–∞
            edges_data = []
            for i, word1 in enumerate(valid_words):
                for word2 in valid_words[i+1:]:
                    similarity = cosine_similarity(
                        [model.wv[word1]], 
                        [model.wv[word2]]
                    )[0][0]
                    
                    if similarity >= threshold:
                        G.add_edge(word1, word2, weight=similarity)
                        edges_data.append({
                            'source': word1,
                            'target': word2,
                            'similarity': float(similarity)
                        })
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≥—Ä–∞—Ñ–∞
            metrics = {
                'nodes': len(G.nodes()),
                'edges': len(G.edges()),
                'density': nx.density(G),
                'average_clustering': nx.average_clustering(G) if len(G.nodes()) > 1 else 0,
                'connected_components': nx.number_connected_components(G)
            }
            
            # –ü–æ–∑–∏—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º eigenvectors)
            if len(G.nodes()) > 0:
                try:
                    pos = nx.spring_layout(G, k=1, iterations=50)
                except:
                    pos = nx.circular_layout(G)
            else:
                pos = {}
            
            return {
                'graph': G,
                'positions': pos,
                'edges_data': edges_data,
                'metrics': metrics,
                'words': valid_words
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∞: {str(e)}'}
    
    def visualize_semantic_axis_interactive(self, axis_data: Dict[str, Any], test_words: List[str], model) -> Dict[str, Any]:
        """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ü–∏–π —Å–ª–æ–≤ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –æ—Å—å"""
        try:
            positive_words = axis_data.get('positive', [])
            negative_words = axis_data.get('negative', [])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—Å–∏
            positive_vectors = [model.wv[w] for w in positive_words if w in model.wv]
            negative_vectors = [model.wv[w] for w in negative_words if w in model.wv]
            
            if not positive_vectors or not negative_vectors:
                return {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å–∏'}
            
            axis_direction = np.mean(positive_vectors, axis=0) - np.mean(negative_vectors, axis=0)
            axis_direction = axis_direction / np.linalg.norm(axis_direction)
            
            # –ü—Ä–æ–µ—Ü–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Å–ª–æ–≤–∞
            projections = []
            for word in test_words:
                if word in model.wv:
                    vector = model.wv[word]
                    projection = np.dot(vector, axis_direction)
                    projections.append({
                        'word': word,
                        'projection': float(projection),
                        'vector': vector
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ–µ–∫—Ü–∏–∏
            projections.sort(key=lambda x: x['projection'])
            
            return {
                'axis_direction': axis_direction,
                'projections': projections,
                'positive_words': positive_words,
                'negative_words': negative_words
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å–∏: {str(e)}'}
    
    def generate_comprehensive_report(self, model_name: str, semantic_ops, test_words: List[str] = None) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        try:
            if model_name not in self.models:
                return {'error': '–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}
            
            model = self.models[model_name]
            
            if test_words is None:
                test_words = ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–¥–∞–Ω–Ω—ã–µ', '–≥–æ—Ä–æ–¥', '—Ö–æ—Ä–æ—à–∏–π']
            
            report = {
                'model_name': model_name,
                'vocabulary_size': len(model.wv.key_to_index),
                'vector_size': model.vector_size if hasattr(model, 'vector_size') else 0
            }
            
            # 1. –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            distance_analysis = semantic_ops.analyze_distance_distribution(model_name)
            report['distance_analysis'] = distance_analysis
            
            # 2. –û—Ü–µ–Ω–∫–∞ –∞–Ω–∞–ª–æ–≥–∏–π
            analogy_analysis = semantic_ops.categorical_analogy_evaluation(model_name)
            report['analogy_analysis'] = analogy_analysis
            
            # 3. –ê–Ω–∞–ª–∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
            neighbors_analysis = semantic_ops.comprehensive_neighbors_analysis(model_name)
            report['neighbors_analysis'] = neighbors_analysis
            
            # 4. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª–æ–≤
            graph_data = self.build_semantic_graph(test_words, model, threshold=0.3)
            report['semantic_graph'] = graph_data
            
            # 5. –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö —Å–ª–æ–≤
            similarity_matrix = np.eye(len(test_words))
            word_to_index = {word: idx for idx, word in enumerate(test_words)}
            
            for i, word1 in enumerate(test_words):
                for j, word2 in enumerate(test_words):
                    if word1 in model.wv and word2 in model.wv:
                        sim = cosine_similarity(
                            [model.wv[word1]], 
                            [model.wv[word2]]
                        )[0][0]
                        similarity_matrix[i][j] = sim
            
            report['similarity_matrix'] = {
                'matrix': similarity_matrix.tolist(),
                'words': test_words
            }
            
            return report
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}'}
    
    def project_to_2d_3d(self, words: List[str], model, method: str = 'tsne', dim: int = 2) -> Dict[str, Any]:
        """–ü—Ä–æ–µ–∫—Ü–∏—è —Å–ª–æ–≤ –≤ 2D/3D –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ"""
        try:
            valid_words = [w for w in words if w in model.wv]
            
            if len(valid_words) < 2:
                return {'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏'}
            
            vectors = np.array([model.wv[w] for w in valid_words])
            
            if method == 'tsne':
                reducer = TSNE(n_components=dim, random_state=42, perplexity=min(30, len(valid_words)-1))
            elif method == 'pca':
                reducer = PCA(n_components=dim)
            else:
                return {'error': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –º–µ—Ç–æ–¥ –ø—Ä–æ–µ–∫—Ü–∏–∏'}
            
            projected = reducer.fit_transform(vectors)
            
            return {
                'words': valid_words,
                'projections': projected.tolist(),
                'method': method,
                'dimensions': dim,
                'explained_variance': getattr(reducer, 'explained_variance_ratio_', None)
            }
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –ø—Ä–æ–µ–∫—Ü–∏–∏: {str(e)}'}


