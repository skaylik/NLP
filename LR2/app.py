# app.py
import streamlit as st
import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
import tempfile
import shutil
import matplotlib.pyplot as plt
import matplotlib
from collections import Counter
import time
import tracemalloc
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401
import altair as alt
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import plotly.express as px

matplotlib.use('Agg')

CPU_TOTAL = os.cpu_count() or 2
DEFAULT_WORKERS = min(max(1, CPU_TOTAL - 1), CPU_TOTAL)

# –ò–º–ø–æ—Ä—Ç –≤–∞—à–∏—Ö –º–æ–¥—É–ª–µ–π
from preparation_corpus import process_corpus, analyze_corpus, TextProcessor
from implementation_vectorization_methods import ClassicalVectorizers
from dimensionality_reduction_topic_modeling import DimensionalityReduction
from comparative_analysis_vectorization import VectorizationComparator
from training_models import DistributedRepresentations
from vector_arithmetic_semantic_operations import SemanticOperations, get_russian_test_sets

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º ‚Ññ 2",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è session state
if 'corpus_processed' not in st.session_state:
    st.session_state.corpus_processed = False
if 'processed_file' not in st.session_state:
    st.session_state.processed_file = None
if 'vectorization_done' not in st.session_state:
    st.session_state.vectorization_done = False
if 'vectorizers' not in st.session_state:
    st.session_state.vectorizers = None
if 'dim_reduction_done' not in st.session_state:
    st.session_state.dim_reduction_done = False
if 'models_trained' not in st.session_state:
    st.session_state.models_trained = False
if 'distributed_models' not in st.session_state:
    st.session_state.distributed_models = None
if 'texts' not in st.session_state:
    st.session_state.texts = None
if 'categories' not in st.session_state:
    st.session_state.categories = None

# –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.title("–õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º ‚Ññ 2")
st.subheader("–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –∫–æ—Ä–ø—É—Å–æ–≤")
st.markdown("---")

# ============================================================================
# –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨: –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –ò –°–¢–ê–¢–£–°
# ============================================================================
with st.sidebar:
    st.header("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ JSONL —Ñ–∞–π–ª",
        type=['jsonl', 'json'],
        help="–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL"
    )

if uploaded_file is not None:
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.jsonl') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        input_file_path = tmp_file.name

    st.session_state.input_file = input_file_path
    st.session_state.input_filename = uploaded_file.name

    # –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞
    with st.spinner("–ê–Ω–∞–ª–∏–∑..."):
        analysis = analyze_corpus(input_file_path)


# ============================================================================
# –û–°–ù–û–í–ù–û–ô –ö–û–ù–¢–ï–ù–¢: –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–´–ï –≠–¢–ê–ü–´
# ============================================================================

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
if 'input_file' not in st.session_state:
    st.info("üëÜ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ JSONL —Ñ–∞–π–ª –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")
    st.stop()

# ============================================================================
# –≠–¢–ê–ü 1: –ü–û–î–ì–û–¢–û–í–ö–ê –ö–û–†–ü–£–°–ê
# ============================================================================
st.header("üîß 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞")
st.markdown("""
**–¢–µ–æ—Ä–∏—è:** –Ω–∞ —ç—Ç–æ–º —à–∞–≥–µ –º—ã –ø—Ä–∏–≤–æ–¥–∏–º —Å—ã—Ä—ã–µ —Ç–µ–∫—Å—Ç—ã –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É. –¶–µ–ª—å ‚Äî –æ—á–∏—Å—Ç–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ—Ç —à—É–º–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Å–ª–æ–≤–∞ –∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å, —á—Ç–æ –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, –ø—Ä–µ–∂–¥–µ —á–µ–º –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ —á–∏—Å–ª–æ–≤–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é.
""")

st.markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

# –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ - 100000 —Å–ª–æ–≤
target_words = 100000

def render_preprocessing_results(summary):
    if not summary:
        return

    total_words = summary.get('total_words', 0)
    if total_words <= 0:
        return

    target = summary.get('target_words', target_words)
    processed_count = summary.get('processed_count', 0)
    stats = summary.get('stats', {})
    total_docs_in_file = summary.get('total_docs_in_file', 0)
    skipped_count = summary.get('skipped_count', 0)
    validation_failed = summary.get('validation_failed', 0)
    processing_stats = summary.get('processing_stats') or {}
    all_categories = summary.get('all_categories', [])
    total_categories_in_file = summary.get('total_categories_in_file', len(all_categories))

    achievement_ratio = total_words / target if target else 0

    if achievement_ratio >= 1.0:
        st.success(
            f"‚úÖ **–û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!** –°–æ–±—Ä–∞–Ω–æ **{total_words:,} —Å–ª–æ–≤** - —Ü–µ–ª–µ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –≤ {target:,} —Å–ª–æ–≤ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç!"
        )
    else:
        st.warning(
            f"‚ö†Ô∏è **–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.** –°–æ–±—Ä–∞–Ω–æ **{total_words:,} —Å–ª–æ–≤** –∏–∑ —Ü–µ–ª–µ–≤—ã—Ö {target:,} ({achievement_ratio*100:.1f}%)"
        )

    st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", f"{processed_count:,}")
    with col2:
        st.metric("–í—Å–µ–≥–æ —Å–ª–æ–≤", f"{total_words:,}")
    with col3:
        if total_docs_in_file > 0:
            efficiency = (processed_count / total_docs_in_file) * 100
            st.metric("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏", f"{efficiency:.1f}%")
    with col4:
        cache_efficiency = processing_stats.get('cache_efficiency', 0)
        if cache_efficiency:
            st.metric("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫—ç—à–∞", f"{cache_efficiency:.1%}")

    st.markdown("#### üéØ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–±–æ—Ä–∞ —Å–ª–æ–≤")
    progress_percent = min(achievement_ratio * 100, 100)
    st.progress(progress_percent / 100 if progress_percent else 0.0)
    st.caption(f"–°–æ–±—Ä–∞–Ω–æ {total_words:,} –∏–∑ {target:,} —Å–ª–æ–≤ ({progress_percent:.1f}%)")

    with st.expander("üîç –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏"):
        col1_exp, col2_exp = st.columns(2)

        with col1_exp:
            st.write("**üìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:**")
            if total_docs_in_file:
                st.write(f"- –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {total_docs_in_file:,}")
            st.write(f"- –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed_count:,}")
            st.write(f"- –ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {skipped_count}")
            st.write(f"- –ù–µ –ø—Ä–æ—à–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏—é: {validation_failed}")

            if processing_stats:
                st.write(f"- –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–µ–∑–∞–Ω–æ: {processing_stats.get('documents_truncated', 0)}")
                st.write(f"- –ù–∏–∑–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: {processing_stats.get('low_diversity_documents', 0)}")
                st.write(f"- –û—à–∏–±–æ–∫ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏: {processing_stats.get('lemmatization_errors', 0)}")

        with col2_exp:
            st.write("**‚öôÔ∏è –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∫–æ—Ä–ø—É—Å–∞:**")
            if processed_count > 0:
                words_per_doc = total_words / processed_count
                st.write(f"- –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {words_per_doc:.1f} —Å–ª–æ–≤")
            st.write(f"- –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(stats)}")
            st.write("**‚úÖ –ü—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –º–µ—Ç–æ–¥—ã:**")
            st.write("- –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è (pymorphy3)")
            st.write("- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–æ–ø-—Å–ª–æ–≤")
            st.write("- –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è (NLTK)")
            st.write("- –û—á–∏—Å—Ç–∫–∞ –æ—Ç —à—É–º–∞")

    if stats:
        st.subheader("üìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º")

        if total_categories_in_file:
            st.info(
                f"üìã –í—Å–µ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ: {total_categories_in_file}. "
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(stats)}"
            )

        stats_df = pd.DataFrame([
            {'–ö–∞—Ç–µ–≥–æ—Ä–∏—è': cat, '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤': count}
            for cat, count in stats.items()
        ])
        stats_df = stats_df.sort_values('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤', ascending=False)

        col1_stats, col2_stats = st.columns([2, 1])

        with col1_stats:
            st.write("**üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ª–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º**")
            st.bar_chart(stats_df.set_index('–ö–∞—Ç–µ–≥–æ—Ä–∏—è'))

        with col2_stats:
            st.write("**üìã –î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º**")
            st.dataframe(stats_df, use_container_width=True)

            top_categories = stats_df.head(3)
            st.write("**üèÜ –¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:**")
            for _, row in top_categories.iterrows():
                st.write(f"- {row['–ö–∞—Ç–µ–≥–æ—Ä–∏—è']}: {row['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤']:,} —Å–ª–æ–≤")

    processed_file = st.session_state.get('processed_file')
    if processed_file:
        try:
            with open(processed_file, 'r', encoding='utf-8') as f:
                processed_data = f.read()

            st.download_button(
                label="üì• –°–∫–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π –∫–æ—Ä–ø—É—Å",
                data=processed_data,
                file_name=processed_file,
                mime="application/jsonl",
                help="–°–∫–∞—á–∞–π—Ç–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö"
            )
        except Exception as exc:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è: {exc}")

# –ò–ù–§–û–†–ú–ê–¶–ò–û–ù–ù–´–ô –ë–õ–û–ö - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π
st.info("""
**üîç –ü—Ä–æ—Ü–µ—Å—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –≤–∫–ª—é—á–∞–µ—Ç:**

- **üßπ –û—á–∏—Å—Ç–∫–∞ –æ—Ç —à—É–º–∞** - –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤, –∑–∞–º–µ–Ω–∞ URL/email/—á–∏—Å–µ–ª –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
- **üî§ –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è** - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É, —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π ("—Ç.–µ." ‚Üí "—Ç–æ –µ—Å—Ç—å")
- **‚úÇÔ∏è –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è** - –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Å–ª–æ–≤–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
- **üö´ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è** - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (<3 —Å–∏–º–≤–æ–ª–æ–≤)
- **üìñ –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è** - –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º pymorphy3
- **üìä –í–∞–ª–∏–¥–∞—Ü–∏—è** - –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞ (50 —Å–∏–º–≤–æ–ª–æ–≤, 5 —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏)

**üéØ –¶–µ–ª—å –æ–±—Ä–∞–±–æ—Ç–∫–∏:** –ü–æ–ª—É—á–∏—Ç—å –Ω–µ –º–µ–Ω–µ–µ **100,000 —Å–ª–æ–≤** –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
""")

enable_logging = st.checkbox(
    "–í–∫–ª—é—á–∏—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ",
    value=False,
    help="–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Ñ–∞–π–ª text_processing.log"
)

if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ—Ä–ø—É—Å–∞", type="primary", key="process_btn"):
    output_file = f"processed_{st.session_state.input_filename}"
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    def progress_callback(progress):
        progress_bar.progress(min(progress, 1.0))
    
    def status_callback(status):
        status_text.text(status)
    
    with st.spinner("–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ—Ä–ø—É—Å–∞..."):
        result = process_corpus(
            st.session_state.input_file,
            output_file,
            target_words=target_words,
            progress_callback=progress_callback,
            status_callback=status_callback,
            enable_logging=enable_logging
        )
    
    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏
    total_words = result.get('total_words', 0)
    stats = result.get('category_stats', {})
    processed_count = result.get('processed_count', 0)
    total_lines = result.get('total_lines', 0)
    skipped_count = result.get('skipped_count', 0)
    validation_failed = result.get('validation_failed', 0)
    processing_stats = result.get('processing_stats', {})
    
    if total_words > 0:
        st.session_state.corpus_processed = True
        st.session_state.processed_file = output_file

        analysis = analyze_corpus(st.session_state.input_file)
        total_docs_in_file = analysis.get('total_documents', 0) if analysis else 0
        all_categories = analysis.get('categories_found', []) if analysis else []

        st.session_state.processing_summary = {
            "total_words": total_words,
            "processed_count": processed_count,
            "stats": stats,
            "total_lines": total_lines,
            "skipped_count": skipped_count,
            "validation_failed": validation_failed,
            "processing_stats": processing_stats,
            "target_words": target_words,
            "total_docs_in_file": total_docs_in_file,
            "all_categories": all_categories,
            "total_categories_in_file": len(all_categories)
        }
    else:
        st.error("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å - –Ω–µ –±—ã–ª–æ —Å–æ–±—Ä–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞")
        st.info("""
        **üí° –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏ —Ä–µ—à–µ–Ω–∏—è:**
        
        - **–í—Å–µ –∑–∞–ø–∏—Å–∏ –Ω–µ –ø—Ä–æ—à–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏—é** - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–µ–π 'title' –∏ 'text' –≤ –∏—Å—Ö–æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ
        - **–¢–µ–∫—Å—Ç—ã —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ** - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è: 50 —Å–∏–º–≤–æ–ª–æ–≤ –∏ 5 —Å–ª–æ–≤ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        - **–ü—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π** - —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ UTF-8
        - **–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSONL** - –∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤–∞–ª–∏–¥–Ω—ã–º JSON –æ–±—ä–µ–∫—Ç–æ–º
        
        **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
        - –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª text_processing.log –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫
        - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        - –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Å—Ç—Ä–æ–≥–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞—Å—Ç—Ä–æ–π–∫–∏)
        """)
        st.session_state.corpus_processed = False
        st.session_state.pop('processed_file', None)
        st.session_state.pop('processing_summary', None)

if st.session_state.get('processing_summary'):
    render_preprocessing_results(st.session_state.processing_summary)

st.markdown("---")

# ============================================================================
# –≠–¢–ê–ü 2: –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø
# ============================================================================
st.header("üî¢ 2. –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
st.markdown("""
**–¢–µ–æ—Ä–∏—è:** –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∏—Ç —Ç–µ–∫—Å—Ç –≤ —á–∏—Å–ª–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É ¬´–¥–æ–∫—É–º–µ–Ω—Ç √ó –ø—Ä–∏–∑–Ω–∞–∫¬ª. –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏ (Bag of Words, TF‚ÄëIDF –∏ –¥—Ä.), —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ª–æ–≤–∞—Ö –∏ –∏—Ö –≤–∞–∂–Ω–æ—Å—Ç–∏, –ø–æ–¥–≥–æ—Ç–æ–≤–∏–≤ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
""")

def render_vectorization_results(summary):
    if not summary:
        return

    results = summary.get('results') or {}
    if not results:
        return

    text_count = summary.get('text_count', 0)
    methods_selected = summary.get('methods', [])
    max_features_val = summary.get('max_features')
    ngram_max_val = summary.get('ngram_max')

    if text_count:
        st.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {text_count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

    st.success("‚úÖ –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

    caption_parts = []
    if methods_selected:
        caption_parts.append("–º–µ—Ç–æ–¥—ã: " + ", ".join(methods_selected))
    param_parts = []
    if max_features_val is not None:
        param_parts.append(f"–º–∞–∫—Å–∏–º—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ = {max_features_val}")
    if ngram_max_val is not None:
        param_parts.append(f"–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è n-–≥—Ä–∞–º–º–∞ = {ngram_max_val}")
    if param_parts:
        caption_parts.append("–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: " + ", ".join(param_parts))
    if caption_parts:
        st.caption("; ".join(caption_parts))

    st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")

    results_df = pd.DataFrame(results).T
    st.markdown("#### üìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç–æ–¥–æ–≤")
    st.dataframe(results_df, use_container_width=True)

    st.subheader("üìà –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏")

    sparsity_data = {}
    density_data = {}

    for method, stats in results.items():
        try:
            sparsity_str = stats['–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)'].replace('%', '').replace(',', '')
            density_str = stats['–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)'].replace('%', '').replace(',', '')
            sparsity_data[method] = float(sparsity_str)
            density_data[method] = float(density_str)
        except Exception as exc:
            st.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–µ—Ç–æ–¥–∞ {method}: {exc}")

    if sparsity_data and density_data:
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("##### üìä –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü")
            sparsity_df = pd.DataFrame({
                '–ú–µ—Ç–æ–¥': list(sparsity_data.keys()),
                '–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)': list(sparsity_data.values())
            }).sort_values('–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)', ascending=False)
            st.bar_chart(sparsity_df.set_index('–ú–µ—Ç–æ–¥'))

            with st.expander("üîç –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏"):
                min_sparsity_method = min(sparsity_data, key=sparsity_data.get)
                max_sparsity_method = max(sparsity_data, key=sparsity_data.get)
                st.write(f"**–ù–∞–∏–º–µ–Ω—å—à–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å:** {min_sparsity_method} ({sparsity_data[min_sparsity_method]:.2f}%)")
                st.write(f"**–ù–∞–∏–±–æ–ª—å—à–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å:** {max_sparsity_method} ({sparsity_data[max_sparsity_method]:.2f}%)")
                st.write("""
                **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å:**
                - –í—ã—Å–æ–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (>95%): –º–∞—Ç—Ä–∏—Ü–∞ –æ—á–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–∞, –º–Ω–æ–≥–æ –Ω—É–ª–µ–π
                - –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (80-95%): —É–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å  
                - –ù–∏–∑–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (<80%): –º–∞—Ç—Ä–∏—Ü–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–ª–æ—Ç–Ω–∞—è
                """)

        with col2:
            st.markdown("##### üéØ –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü")
            density_df = pd.DataFrame({
                '–ú–µ—Ç–æ–¥': list(density_data.keys()),
                '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)': list(density_data.values())
            }).sort_values('–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)', ascending=False)
            st.bar_chart(density_df.set_index('–ú–µ—Ç–æ–¥'))

            with st.expander("üîç –ê–Ω–∞–ª–∏–∑ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏"):
                min_density_method = min(density_data, key=density_data.get)
                max_density_method = max(density_data, key=density_data.get)
                st.write(f"**–ù–∞–∏–º–µ–Ω—å—à–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å:** {min_density_method} ({density_data[min_density_method]:.2f}%)")
                st.write(f"**–ù–∞–∏–±–æ–ª—å—à–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å:** {max_density_method} ({density_data[max_density_method]:.2f}%)")
                st.write("""
                **–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç—å:**
                - –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (>20%): –º–∞—Ç—Ä–∏—Ü–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–Ω–æ–≥–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                - –°—Ä–µ–¥–Ω—è—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (5-20%): —É–º–µ—Ä–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                - –ù–∏–∑–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å (<5%): –º–∞—Ç—Ä–∏—Ü–∞ –æ—á–µ–Ω—å —Ä–∞–∑—Ä–µ–∂–µ–Ω–∞
                """)

        st.markdown("---")
        st.subheader("üìä –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

        comparison_df = pd.DataFrame([
            {
                '–ú–µ—Ç–æ–¥': method,
                '–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)': sparsity_data[method],
                '–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)': density_data[method],
                '–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å': 100 - sparsity_data[method]
            }
            for method in sparsity_data.keys()
        ])

        col3, col4 = st.columns(2)

        with col3:
            st.markdown("##### üèÜ –õ—É—á—à–∏–µ –º–µ—Ç–æ–¥—ã")
            best_sparsity = comparison_df.loc[comparison_df['–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)'].idxmin()]
            best_density = comparison_df.loc[comparison_df['–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)'].idxmax()]
            best_efficiency = comparison_df.loc[comparison_df['–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'].idxmax()]

            st.metric("–õ—É—á—à–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å", f"{best_sparsity['–ú–µ—Ç–æ–¥']}", f"{best_sparsity['–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)']:.1f}%")
            st.metric("–õ—É—á—à–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å", f"{best_density['–ú–µ—Ç–æ–¥']}", f"{best_density['–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)']:.1f}%")
            st.metric("–û–±—â–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å", f"{best_efficiency['–ú–µ—Ç–æ–¥']}", f"{best_efficiency['–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å']:.1f}%")

        with col4:
            st.markdown("##### üìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")
            if best_sparsity['–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)'] < 90:
                st.success("**‚úÖ –•–æ—Ä–æ—à–∞—è —Å–∏—Ç—É–∞—Ü–∏—è:** –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤ –∏–º–µ—é—Ç –ø—Ä–∏–µ–º–ª–µ–º—É—é —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å")
            else:
                st.warning("**‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ:** –í—ã—Å–æ–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç —Å–Ω–∏–∑–∏—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π")

            if len(methods_selected) >= 2:
                st.info(f"**üí° –°–æ–≤–µ—Ç:** –î–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º **{best_efficiency['–ú–µ—Ç–æ–¥']}**")

            st.write("""
            **–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞:**
            - –ù–∏–∑–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å = –ª—É—á—à–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            - –í—ã—Å–æ–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å = –±–æ–ª—å—à–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è
            - –ë–∞–ª–∞–Ω—Å = –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            """)

        st.markdown("---")
        st.subheader("üîç –î–µ—Ç–∞–ª–∏ –ø–æ –º–µ—Ç–æ–¥–∞–º")

        for method, stats in results.items():
            with st.expander(f"{method}", expanded=False):
                col_a, col_b = st.columns(2)

                with col_a:
                    st.write("**–û—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**")
                    st.write(f"- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {stats['–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å']}")
                    st.write(f"- –í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {stats['–í—Å–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤']}")
                    st.write(f"- –ù–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {stats['–ù–µ–Ω—É–ª–µ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã']}")

                with col_b:
                    st.write("**–ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏:**")
                    st.write(f"- –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {stats['–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)']}")
                    st.write(f"- –ü–ª–æ—Ç–Ω–æ—Å—Ç—å: {stats['–ü–ª–æ—Ç–Ω–æ—Å—Ç—å (%)']}")

                    sparsity_val = sparsity_data.get(method, 0)
                    if sparsity_val < 80:
                        st.success("‚úÖ –û—Ç–ª–∏—á–Ω–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å")
                    elif sparsity_val < 95:
                        st.info("‚ÑπÔ∏è –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å")
                    else:
                        st.warning("‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å")

    else:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")

if not st.session_state.corpus_processed:
    st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∫–æ—Ä–ø—É—Å –Ω–∞ —ç—Ç–∞–ø–µ 1")
else:
    st.markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    
    col1, col2 = st.columns(2)
    
    with col1:
        max_features = st.number_input(
            "–ú–∞–∫—Å–∏–º—É–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤",
            min_value=1000,
            max_value=50000,
            value=10000,
            step=1000,
            key="max_features_2"
        )
    
    with col2:
        ngram_max = st.selectbox(
            "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è n-–≥—Ä–∞–º–º–∞",
            options=[1, 2, 3],
            index=1,
            key="ngram_max_2"
        )
    
    st.markdown("### –ú–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
    
    methods = st.multiselect(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–µ—Ç–æ–¥—ã",
        options=[
            "One-Hot Encoding",
            "Bag of Words", 
            "TF-IDF",
            "–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ n-–≥—Ä–∞–º–º—ã"
        ],
        default=["Bag of Words", "TF-IDF"],
        key="methods_2"
    )
    
    if st.button("üîÑ –ü—Ä–∏–º–µ–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é", type="primary", key="vectorize_btn"):
        vectorizers = ClassicalVectorizers()
        
        with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞..."):
            texts, categories = vectorizers.load_corpus(
                st.session_state.processed_file,
                text_field='text',
                category_field='category'
            )
        
        text_count = len(texts)
        
        st.session_state.texts = texts
        st.session_state.categories = categories
        st.session_state.vectorizers = vectorizers
        
        results = {}
        matrices = {}
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        total_methods = len(methods) or 1
        
        for idx, method in enumerate(methods):
            status_text.text(f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞: {method}...")
            
            if method == "One-Hot Encoding":
                X = vectorizers.one_hot_encoding(
                    texts,
                    ngram_range=(1, ngram_max),
                    max_features=max_features
                )
                if X is not None:
                    results[method] = vectorizers.analyze_sparsity(X, method)
                    matrices[method] = X
            
            elif method == "Bag of Words":
                X = vectorizers.bag_of_words(
                    texts,
                    ngram_range=(1, ngram_max),
                    max_features=max_features,
                    binary=False
                )
                if X is not None:
                    results[method] = vectorizers.analyze_sparsity(X, method)
                    matrices[method] = X
            
            elif method == "TF-IDF":
                X = vectorizers.tfidf(
                    texts,
                    ngram_range=(1, ngram_max),
                    max_features=max_features
                )
                if X is not None:
                    results[method] = vectorizers.analyze_sparsity(X, method)
                    matrices[method] = X
                    st.session_state.last_tfidf_matrix = X
            
            elif method == "–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ n-–≥—Ä–∞–º–º—ã":
                X = vectorizers.combined_ngrams(
                    texts,
                    max_ngram=ngram_max,
                    max_features=max_features
                )
                if X is not None:
                    results[method] = vectorizers.analyze_sparsity(X, method)
                    matrices[method] = X
            
            if total_methods:
                progress_bar.progress((idx + 1) / total_methods)
        
        status_text.text("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        
        if results:
            st.session_state.vectorization_done = True
            st.session_state.vectorization_matrices = matrices
            st.session_state.vectorization_summary = {
                "results": results,
                "text_count": text_count,
                "methods": methods,
                "max_features": max_features,
                "ngram_max": ngram_max
            }
        else:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
            st.session_state.vectorization_done = False
            st.session_state.pop('vectorization_summary', None)

    if st.session_state.get('vectorization_summary'):
        render_vectorization_results(st.session_state.vectorization_summary)

st.markdown("---")

# ============================================================================
# –≠–¢–ê–ü 3: –°–ù–ò–ñ–ï–ù–ò–ï –†–ê–ó–ú–ï–†–ù–û–°–¢–ò
# ============================================================================
st.header("üìâ 3. –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ")
st.markdown("""
**–¢–µ–æ—Ä–∏—è:** –ø–æ—Å–ª–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—á–µ–Ω—å –≤–µ–ª–∏–∫–æ. SVD –∏ t‚ÄëSNE –ø–æ–∑–≤–æ–ª—è—é—Ç —Å–∂–∞—Ç—å –µ–≥–æ, –≤—ã–¥–µ–ª–∏—Ç—å –≥–ª–∞–≤–Ω—ã–µ —Ç–µ–º–∞—Ç–∏–∫–∏ –∏ —É–≤–∏–¥–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∫–æ—Ä–ø—É—Å–∞, —Å–æ—Ö—Ä–∞–Ω–∏–≤ –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø—Ä–∏ –º–µ–Ω—å—à–µ–º —á–∏—Å–ª–µ –∏–∑–º–µ—Ä–µ–Ω–∏–π.
""")

def render_dim_reduction_results(summary):
    if not summary:
        return

    st.success("‚úÖ –°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    svd_shape = summary.get('svd_shape')
    if svd_shape:
        st.write(f"**SVD –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ:** {svd_shape[0]} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ √ó {svd_shape[1]} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")

    component_stats = summary.get('component_stats') or {}
    if component_stats:
        col1, col2, col3 = st.columns(3)
        col1.metric("–ò—Å—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å", "√ó".join(map(str, component_stats.get('original_dimensions', []))))
        col2.metric("–°–∂–∞—Ç–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å", "√ó".join(map(str, component_stats.get('reduced_dimensions', []))))
        col3.metric("–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è", f"{component_stats.get('compression_ratio', 0):.2f}√ó")

    variance_info = summary.get('variance_info') or {}
    if variance_info:
        st.subheader("üìà –ê–Ω–∞–ª–∏–∑ –∫—É–º—É–ª—è—Ç–∏–≤–Ω–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–∏")
        col1, col2 = st.columns(2)
        col1.metric(
            "–û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç",
            variance_info.get('optimal_components', 0)
        )
        col2.metric(
            "–î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞—è –¥–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏",
            f"{variance_info.get('achieved_variance', 0)*100:.1f}%"
        )

        variance_df = pd.DataFrame({
            '–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞': variance_info.get('components_range', []),
            '–ö—É–º—É–ª—è—Ç–∏–≤–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è': variance_info.get('cumulative_variance', [])
        })
        if not variance_df.empty:
            st.line_chart(variance_df.set_index('–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞'))
            st.caption(
                f"–ü–æ—Ä–æ–≥ –ø–æ –¥–∏—Å–ø–µ—Ä—Å–∏–∏: {variance_info.get('variance_threshold', 0)*100:.0f}%"
            )

    component_keywords = summary.get('component_keywords') or []
    if component_keywords:
        st.subheader("üîé –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç")
        for component in component_keywords:
            idx = component.get('component')
            variance = component.get('explained_variance', 0)
            keywords = component.get('keywords', [])
            label = f"–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ {idx + 1} ‚Ä¢ –≤–∫–ª–∞–¥ {variance*100:.1f}%"
            with st.expander(label, expanded=False):
                if keywords:
                    keywords_df = pd.DataFrame(keywords, columns=['–°–ª–æ–≤–æ', '–í–µ—Å'])
                    st.table(keywords_df)
                else:
                    st.info("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã")

    tsne_points = summary.get('tsne_points')
    if tsne_points:
        st.subheader("üó∫Ô∏è –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è t-SNE")
        df_vis = pd.DataFrame(tsne_points, columns=['x', 'y'])
        labels = summary.get('labels') or []
        if labels and len(labels) >= len(df_vis):
            df_vis['category'] = labels[:len(df_vis)]
            st.scatter_chart(df_vis, x='x', y='y', color='category', width='stretch')
        else:
            st.scatter_chart(df_vis, x='x', y='y', width='stretch')

if not st.session_state.vectorization_done:
    st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ —ç—Ç–∞–ø–µ 2")
else:
    st.markdown("### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏")
    
    col1, col2 = st.columns(2)
    
    with col1:
        n_components = st.slider(
            "–ß–∏—Å–ª–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç SVD",
            min_value=10,
            max_value=200,
            value=50,
            step=5,
            key="n_components_3"
        )
    
    with col2:
        variance_threshold = st.slider(
            "–¶–µ–ª–µ–≤–∞—è –¥–æ–ª—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏",
            min_value=0.70,
            max_value=0.99,
            value=0.90,
            step=0.01,
            key="variance_threshold_3"
        )
    
    tsne_perplexity = st.slider(
        "Perplexity –¥–ª—è t-SNE",
        min_value=5,
        max_value=50,
        value=30,
        step=5,
        key="tsne_perplexity_3"
    )
    
    if st.button("üîÑ –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Å–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏", type="primary", key="dim_red_btn"):
        dim_reduction = DimensionalityReduction()
        
        if st.session_state.vectorizers and st.session_state.texts:
            with st.spinner("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö..."):
                X = st.session_state.vectorizers.tfidf(
                    st.session_state.texts,
                    ngram_range=(1, 2),
                    max_features=10000
                )
                dim_reduction.load_vectors(X)
            
            with st.spinner("–ü—Ä–∏–º–µ–Ω—è–µ–º SVD..."):
                svd_matrix = dim_reduction.apply_svd(n_components=n_components)
            
            component_stats = dim_reduction.get_component_statistics(svd_matrix)
            component_keywords = dim_reduction.interpret_svd_components(
                n_top_words=10,
                n_components=min(5, n_components)
            )
            
            max_components = min(max(n_components, 50), min(svd_matrix.shape[0], svd_matrix.shape[1]) - 1)
            variance_info = dim_reduction.find_optimal_components(
                max_components=max_components,
                variance_threshold=variance_threshold
            )
            
            with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ–º t-SNE –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏..."):
                labels = st.session_state.categories or []
                tsne_points = dim_reduction.visualize_components(
                    svd_matrix,
                    labels=np.array(labels) if labels else None,
                    method='tsne',
                    perplexity=tsne_perplexity
                )
            
            st.session_state.dim_reduction_done = True
            st.session_state.dim_reduction_summary = {
                "svd_shape": tuple(svd_matrix.shape),
                "component_stats": {
                    "original_dimensions": component_stats.get('original_dimensions') if component_stats else None,
                    "reduced_dimensions": component_stats.get('reduced_dimensions') if component_stats else None,
                    "compression_ratio": component_stats.get('compression_ratio') if component_stats else None,
                },
                "component_keywords": [
                    {
                        "component": item.get('component', idx),
                        "explained_variance": float(item.get('explained_variance', 0)),
                        "keywords": [(word, float(weight)) for word, weight in item.get('keywords', [])]
                    }
                    for idx, item in enumerate(component_keywords or [])
                ],
                "variance_info": {
                    "optimal_components": variance_info.get('optimal_components'),
                    "achieved_variance": variance_info.get('achieved_variance'),
                    "variance_threshold": variance_info.get('variance_threshold'),
                    "components_range": list(variance_info.get('components_range', [])),
                    "cumulative_variance": list(variance_info.get('cumulative_variance', []))
                } if variance_info else {},
                "tsne_points": tsne_points.tolist() if tsne_points is not None else None,
                "labels": labels[:len(tsne_points)] if tsne_points is not None and labels else []
            }
        else:
            st.error("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏")
    
    if st.session_state.get('dim_reduction_summary'):
        render_dim_reduction_results(st.session_state.dim_reduction_summary)

st.markdown("---")

# ============================================================================
# –≠–¢–ê–ü 4: –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó
# ============================================================================
st.header("üìä 4. –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–¥–æ–≤ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏")
st.markdown("""
**–¢–µ–æ—Ä–∏—è:** —Ä–∞–∑–Ω—ã–µ —Å–ø–æ—Å–æ–±—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –°—Ä–∞–≤–Ω–∏–≤–∞—è –∏—Ö –ø–æ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏, –º—ã –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –º–æ–¥–µ–ª—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö —à–∞–≥–æ–≤.
""")

def render_comparative_analysis_results(summary):
    if not summary:
        return

    results_comp = summary.get('results') or {}
    if not results_comp:
        return

    comp_df = pd.DataFrame(results_comp).T

    if comp_df.empty:
        st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        return

    st.success("‚úÖ –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    st.dataframe(comp_df, width='stretch')

    efficiency_cols = {}
    if 'Processing Time (s)' in comp_df.columns:
        efficiency_cols['time'] = pd.to_numeric(comp_df['Processing Time (s)'], errors='coerce')
    if 'Peak Memory (MB)' in comp_df.columns:
        efficiency_cols['memory'] = pd.to_numeric(comp_df['Peak Memory (MB)'], errors='coerce')

    if efficiency_cols:
        st.markdown("##### ‚öôÔ∏è –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å")
        col_time, col_mem = st.columns(2)

        if 'time' in efficiency_cols and not efficiency_cols['time'].isna().all():
            fastest_method = efficiency_cols['time'].idxmin()
            fastest_value = efficiency_cols['time'].min()
            col_time.metric(
                "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                f"{fastest_value:.2f} —Å",
                fastest_method
            )
        else:
            col_time.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        if 'memory' in efficiency_cols and not efficiency_cols['memory'].isna().all():
            best_memory_method = efficiency_cols['memory'].idxmin()
            best_memory_value = efficiency_cols['memory'].min()
            col_mem.metric(
                "–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏",
                f"{best_memory_value:.2f} –ú–ë",
                best_memory_method
            )
        else:
            col_mem.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –ø–∞–º—è—Ç–∏")

    col1, col2 = st.columns(2)

    with col1:
        if 'Semantic Coherence' in comp_df.columns:
            st.write("**–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å**")
            coherence_data = comp_df[['Semantic Coherence']].copy()
            st.bar_chart(coherence_data)

    with col2:
        if 'Sparsity (%)' in comp_df.columns:
            st.write("**–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å (%)**")
            sparsity_data = comp_df[['Sparsity (%)']].copy()
            st.bar_chart(sparsity_data)

    st.markdown("---")
    st.subheader("üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

    col3, col4 = st.columns(2)

    with col3:
        if 'Dimensions' in comp_df.columns:
            st.write("**–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤**")
            dim_data = comp_df[['Dimensions']].copy()
            st.bar_chart(dim_data)

    with col4:
        if 'Semantic Coherence' in comp_df.columns:
            best_method = comp_df['Semantic Coherence'].idxmax()
            best_score = comp_df['Semantic Coherence'].max()
            st.metric("–õ—É—á—à–∏–π –º–µ—Ç–æ–¥ –ø–æ —Å–µ–º–∞–Ω—Ç–∏–∫–µ", best_method, f"{best_score:.4f}")

    st.markdown("---")
    st.subheader("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

    if 'Semantic Coherence' in comp_df.columns and 'Sparsity (%)' in comp_df.columns:
        best_semantic = comp_df['Semantic Coherence'].idxmax()
        best_sparsity = comp_df['Sparsity (%)'].idxmin()

        col5, col6 = st.columns(2)

        with col5:
            st.info(f"**–õ—É—á—à–∞—è —Å–µ–º–∞–Ω—Ç–∏–∫–∞:** {best_semantic}")
            st.write("–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –ø–æ–∏—Å–∫–∞")

        with col6:
            st.success(f"**–õ—É—á—à–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å:** {best_sparsity}")
            st.write("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –ø–æ –ø–∞–º—è—Ç–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º")

    st.markdown("""
    **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Bag of Words:**
    - –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    - –ö–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
    - –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

    **–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TF-IDF:**
    - –ü–æ–∏—Å–∫–æ–≤—ã–µ —Å–∏—Å—Ç–µ–º—ã
    - –ó–∞–¥–∞—á–∏ —Å –≤–∞–∂–Ω–æ—Å—Ç—å—é —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤
    - –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ –ª—É—á—à–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
    """)
 
if not st.session_state.vectorization_done:
    st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ —ç—Ç–∞–ø–µ 2")
else:
    st.markdown("### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤")

    if st.button("üîÑ –ü—Ä–æ–≤–µ—Å—Ç–∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑", type="primary", key="compare_btn"):
        comparator = VectorizationComparator(st.session_state.vectorizers)
        comparator.texts = st.session_state.get('texts', [])
        comparator.categories = st.session_state.get('categories', [])
 
        with st.spinner("–ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞..."):
            results_comp = {}
 
            stored_matrices = st.session_state.get('vectorization_matrices', {}) or {}
            summary_methods = st.session_state.get('vectorization_summary', {}).get('methods', [])
            methods_in_order = [m for m in summary_methods if m in stored_matrices]
            for method_name in stored_matrices:
                if method_name not in methods_in_order:
                    methods_in_order.append(method_name)
 
            if not methods_in_order:
                st.warning("–ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –Ω–∞ —ç—Ç–∞–ø–µ 2.")
            else:
                for method_name in methods_in_order:
                    matrix = stored_matrices.get(method_name)
                    if matrix is None:
                        continue
 
                    tracemalloc.start()
                    start_time = time.perf_counter()
 
                    evaluation = comparator.evaluate_method(matrix, comparator.categories, method_name)
 
                    elapsed = time.perf_counter() - start_time
                    current, peak = tracemalloc.get_traced_memory()
                    tracemalloc.stop()
 
                    if evaluation is None:
                        continue
 
                    evaluation['Processing Time (s)'] = round(elapsed, 2)
                    evaluation['Peak Memory (MB)'] = round(peak / (1024 * 1024), 2)
 
                    results_comp[method_name] = evaluation
 
        if results_comp:
            st.session_state.comparative_summary = {
                "results": results_comp
            }
        else:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            st.session_state.pop('comparative_summary', None)

    if st.session_state.get('comparative_summary'):
        render_comparative_analysis_results(st.session_state.comparative_summary)
 
# ============================================================================
# –≠–¢–ê–ü 5: –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –†–ê–°–ü–†–ï–î–ï–õ–Å–ù–ù–´–• –ü–†–ï–î–°–¢–ê–í–õ–ï–ù–ò–ô
# ============================================================================
st.header("ü§ñ 5. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π")
st.markdown("""
**–¢–µ–æ—Ä–∏—è:** —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Word2Vec, FastText, Doc2Vec) —É—á–∞—Ç—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Å–ª–æ–≤–∞ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –≤–∏–¥–µ –ø–ª–æ—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, –æ—Ç—Ä–∞–∂–∞—é—â–∏—Ö —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏. –≠—Ç–æ—Ç —ç—Ç–∞–ø —Å—Ç—Ä–æ–∏—Ç ¬´—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ¬ª –∫–æ—Ä–ø—É—Å–∞.
""")

def render_training_results(summary):
    if not summary:
        return

    models_created = summary.get('models_created', 0)
    text_count = summary.get('text_count', 0)
    params = summary.get('params', {})
    evaluation = summary.get('evaluation', [])

    st.success(f"üéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –£—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–æ {models_created} –º–æ–¥–µ–ª–µ–π")
    if text_count:
        st.caption(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {text_count}")

    if params:
        with st.expander("üîß –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è", expanded=False):
            st.write(f"–ú–æ–¥–µ–ª–∏: {', '.join(params.get('model_types', []))}")
            st.write(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤: {params.get('vector_size')}")
            st.write(f"–û–∫–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {params.get('window')}")
            st.write(f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞: {params.get('min_count')}")
            st.write(f"–≠–ø–æ—Ö–∏: {params.get('epochs')}")
            st.write(f"–ú–∞–∫—Å–∏–º—É–º —ç–ø–æ—Ö –ø–æ—Å–ª–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: {params.get('max_epochs')}")
            st.write(f"–ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: {params.get('negative')}")
            st.write(f"–ü–æ—Ç–æ–∫–∏: {params.get('workers')}")
            st.write(f"–ü–æ–¥—Å—á—ë—Ç loss: {'–¥–∞' if params.get('compute_loss') else '–Ω–µ—Ç'}")

    if evaluation:
        st.subheader("üß™ –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞")
        for model_eval in evaluation:
            st.write(f"**{model_eval.get('model')}**:")

            for pair_info in model_eval.get('pairs', []):
                word1 = pair_info.get('word1')
                word2 = pair_info.get('word2')
                if 'missing_words' in pair_info:
                    missing = ", ".join(pair_info['missing_words'])
                    st.write(f"  {word1} - {word2}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {missing}")
                else:
                    similarity = pair_info.get('similarity', 0)
                    status = pair_info.get('status', '')
                    st.write(f"  {word1} - {word2}: {similarity:.3f} ({status})")

            neighbors = model_eval.get('neighbors', [])
            if neighbors:
                st.write("  –ë–ª–∏–∂–∞–π—à–∏–µ –∫ '–∫–æ–º–ø—å—é—Ç–µ—Ä':")
                for neighbor in neighbors:
                    st.write(f"    - {neighbor['word']}: {neighbor['similarity']:.3f}")
            elif model_eval.get('neighbors_error'):
                st.write(f"  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ —Å–æ—Å–µ–¥–µ–π: {model_eval['neighbors_error']}")

            st.write("---")

    st.info("‚úÖ –ì–æ—Ç–æ–≤–æ! –ü–µ—Ä–µ—Ö–æ–¥–∏—Ç–µ –∫ —ç—Ç–∞–ø—É 6 –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")
 
if not st.session_state.corpus_processed:
    st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∫–æ—Ä–ø—É—Å –Ω–∞ —ç—Ç–∞–ø–µ 1")
else:
    # –ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ö–û–†–ü–£–°–ï
    st.info("üìä **–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤–∞—à–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞:**")
    if st.session_state.get('processing_summary'):
        proc_summary = st.session_state.processing_summary
        st.write(f"- **–î–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {proc_summary.get('processed_count', '‚Äî')}")
        st.write(f"- **–í—Å–µ–≥–æ —Å–ª–æ–≤:** {proc_summary.get('total_words', '‚Äî'):,}")
        total_docs_input = proc_summary.get('total_docs_in_file')
        if total_docs_input:
            st.write(f"- **–ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {total_docs_input}")
        stats = proc_summary.get('stats') or {}
        st.write(f"- **–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π:** {len(stats)}")
        all_categories = proc_summary.get('all_categories') or []
        if all_categories:
            st.write(f"- **–ö–∞—Ç–µ–≥–æ—Ä–∏–π –≤—Å–µ–≥–æ:** {len(all_categories)}")
    else:
        st.write("- –ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∫–æ—Ä–ø—É—Å, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.")
    
    # –†–ê–°–®–ò–†–ï–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò
    st.markdown("### üéõÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è")
    
    cpu_total = CPU_TOTAL
    default_workers = DEFAULT_WORKERS

    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üß† –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –º–æ–¥–µ–ª–µ–π")
        model_types = st.multiselect(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è:",
            options=[
                "word2vec_skipgram", 
                "word2vec_cbow",
                "fasttext_skipgram", 
                "fasttext_cbow",
                "doc2vec"
            ],
            default=["word2vec_skipgram", "fasttext_skipgram"],
            help="–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –≤—ã–±—Ä–∞—Ç—å –æ–±–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"
        )
        
        vector_size = st.slider(
            "–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤:",
            min_value=50,
            max_value=300,
            value=100,
            step=25,
            help="–ë–æ–ª—å—à–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å = –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö. –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ 50-150 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ."
        )
        
        window = st.slider(
            "–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:",
            min_value=2,
            max_value=20,
            value=8,
            help="–ë–æ–ª—å—à–µ–µ –æ–∫–Ω–æ = –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏. –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ 5-10 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ."
        )
    
    with col2:
        st.subheader("‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è")
        
        min_count = st.slider(
            "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–ª–æ–≤–∞:",
            min_value=1,
            max_value=10,
            value=2,
            help="–°–ª–æ–≤–∞, –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ä–µ–∂–µ, –±—É–¥—É—Ç –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω—ã. –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ 2-5 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ."
        )
        
        epochs = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö:",
            min_value=10,
            max_value=200,
            value=100,
            help="–ë–æ–ª—å—à–µ —ç–ø–æ—Ö = –ª—É—á—à–µ –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –¥–æ–ª—å—à–µ –≤—Ä–µ–º—è. –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ 80-120 –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ."
        )
        
        negative = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:",
            min_value=0,
            max_value=20,
            value=10,
            help="0 = Hierarchical Softmax (–ª—É—á—à–µ –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏—Ö –∫–æ—Ä–ø—É—Å–æ–≤), >0 = Negative Sampling (–æ–±—ã—á–Ω–æ –±—ã—Å—Ç—Ä–µ–µ –∏ –ª—É—á—à–µ)"
        )

        workers_count = st.slider(
            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤:",
            min_value=1,
            max_value=int(cpu_total),
            value=int(default_workers),
            help="–ß–µ–º –±–æ–ª—å—à–µ –ø–æ—Ç–æ–∫–æ–≤, —Ç–µ–º –±—ã—Å—Ç—Ä–µ–µ –æ–±—É—á–µ–Ω–∏–µ (–¥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä)"
        )

    compute_loss_enabled = st.checkbox(
        "–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è",
        value=False,
        help="–í–∫–ª—é—á–∏—Ç–µ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å loss ‚Äî —ç—Ç–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 30-40%"
    )

    max_epochs_cap = st.slider(
        "–ú–∞–∫—Å–∏–º—É–º —ç–ø–æ—Ö –ø–æ—Å–ª–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏",
        min_value=int(max(20, epochs)),
        max_value=300,
        value=int(max(150, epochs)),
        step=10,
        help="–û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —ç–ø–æ—Ö –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞"
    )
    
    # –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ò –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–Ø
    st.markdown("### üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º")
    
    rec_col1, rec_col2, rec_col3 = st.columns(3)
    
    with rec_col1:
        if vector_size > 150:
            st.warning("‚ö†Ô∏è –ë–æ–ª—å—à–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –º–æ–∂–µ—Ç –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é")
        else:
            st.success("‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞")
            
        if window > 12:
            st.warning("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ –æ–∫–Ω–æ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞")
        else:
            st.success("‚úÖ –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –æ–ø—Ç–∏–º–∞–ª–µ–Ω")
    
    with rec_col2:
        if min_count == 1:
            st.warning("‚ö†Ô∏è –ú–Ω–æ–≥–æ —à—É–º–Ω—ã—Ö —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ")
        else:
            st.success("‚úÖ –•–æ—Ä–æ—à–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞")
            
        if epochs < 50:
            st.warning("‚ö†Ô∏è –ú–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–ø–æ—Ö")
        else:
            st.success("‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–ø–æ—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        if max_epochs_cap > 200:
            st.warning("‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π –ª–∏–º–∏—Ç —ç–ø–æ—Ö –º–æ–∂–µ—Ç —Å–∏–ª—å–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è")
        else:
            st.success("‚úÖ –õ–∏–º–∏—Ç —ç–ø–æ—Ö –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è")
    
    with rec_col3:
        if negative == 0:
            st.info("‚ÑπÔ∏è Hierarchical Softmax - —Å—Ç–∞–±–∏–ª—å–Ω–æ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
        else:
            st.info("‚ÑπÔ∏è Negative Sampling - –±—ã—Å—Ç—Ä–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ")
        if len(model_types) == 0:
            st.error("‚ùå –í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å")
        else:
            st.success(f"‚úÖ –í—ã–±—Ä–∞–Ω–æ {len(model_types)} –º–æ–¥–µ–ª–µ–π")
        if compute_loss_enabled:
            st.warning("‚ö†Ô∏è –ü–æ–¥—Å—á—ë—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∑–∞–º–µ–¥–ª–∏—Ç –æ–±—É—á–µ–Ω–∏–µ")
        if workers_count < default_workers:
            st.info(f"‚ÑπÔ∏è –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ (—Å–µ–π—á–∞—Å {workers_count} –∏–∑ {cpu_total})")

    # –ö–ù–û–ü–ö–ê –û–ë–£–ß–ï–ù–ò–Ø
    st.markdown("---")
    
    if st.button("üöÄ –ù–∞—á–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏", type="primary", key="train_btn"):
        if not model_types:
            st.error("‚ùå –í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        else:
            distributed = DistributedRepresentations()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
            with st.spinner("–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ä–ø—É—Å–∞..."):
                texts = []
                categories = []
                
                with open(st.session_state.processed_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        data = json.loads(line)
                        text = data.get('text', '')
                        category = data.get('category', '')
                        
                        if text:
                            words = text.split()
                            texts.append(words)
                            categories.append(category)
            
            text_count = len(texts)
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text("üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
            progress_bar.progress(20)
            
            try:
                models_created = distributed.train_with_parameters(
                    texts=texts,
                    categories=categories if 'doc2vec' in model_types else None,
                    model_types=model_types,
                    vector_size=vector_size,
                    window=window,
                    min_count=min_count,
                    epochs=epochs,
                    negative=negative,
                    workers=workers_count,
                    compute_loss=compute_loss_enabled,
                    max_epochs=max_epochs_cap
                )
                
                progress_bar.progress(100)
                status_text.text("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
                
                if models_created > 0:
                    st.session_state.models_trained = True
                    st.session_state.distributed_models = distributed
                    
                    all_models = distributed.get_available_models()
                    test_pairs = [
                        ('–∫–æ–º–ø—å—é—Ç–µ—Ä', '–Ω–æ—É—Ç–±—É–∫'),
                        ('–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'), 
                        ('–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º'),
                        ('–≥–æ—Ä–æ–¥', '—Ä–µ–∫–∞')
                    ]
                    
                    evaluation_results = []
                    for model_name, model in all_models.items():
                        pair_results = []
                        for word1, word2 in test_pairs:
                            if word1 in model.wv and word2 in model.wv:
                                similarity = float(model.wv.similarity(word1, word2))
                                status_label = "‚úÖ –•–û–†–û–®–û" if similarity > 0.6 else "‚ö†Ô∏è –°–õ–ê–ë–û" if similarity > 0.3 else "‚ùå –ü–õ–û–•–û"
                                pair_results.append({
                                    "word1": word1,
                                    "word2": word2,
                                    "similarity": similarity,
                                    "status": status_label
                                })
                            else:
                                missing_words = [word for word in (word1, word2) if word not in model.wv]
                                pair_results.append({
                                    "word1": word1,
                                    "word2": word2,
                                    "missing_words": missing_words
                                })
                        
                        neighbors = []
                        neighbors_error = None
                        if '–∫–æ–º–ø—å—é—Ç–µ—Ä' in model.wv:
                            try:
                                neighbors_raw = model.wv.most_similar('–∫–æ–º–ø—å—é—Ç–µ—Ä', topn=3)
                                neighbors = [
                                    {"word": neighbor_word, "similarity": float(sim)}
                                    for neighbor_word, sim in neighbors_raw
                                ]
                            except Exception as err:
                                neighbors_error = str(err)
                        else:
                            neighbors_error = "—Å–ª–æ–≤–æ '–∫–æ–º–ø—å—é—Ç–µ—Ä' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ"
                        
                        evaluation_results.append({
                            "model": model_name,
                            "pairs": pair_results,
                            "neighbors": neighbors,
                            "neighbors_error": neighbors_error
                        })
                    
                    st.session_state.training_summary = {
                        "models_created": models_created,
                        "text_count": text_count,
                        "params": {
                            "model_types": model_types,
                            "vector_size": vector_size,
                            "window": window,
                            "min_count": min_count,
                            "epochs": epochs,
                        "negative": negative,
                        "workers": workers_count,
                        "compute_loss": compute_loss_enabled,
                        "max_epochs": max_epochs_cap
                        },
                        "evaluation": evaluation_results
                    }
                else:
                    st.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å")
                    st.session_state.pop('training_summary', None)
            
            except Exception as e:
                st.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {str(e)}")
                st.info("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –∏–ª–∏ —É–≤–µ–ª–∏—á–∏—Ç—å min_count)")
                st.session_state.pop('training_summary', None)

    if st.session_state.get('training_summary'):
        render_training_results(st.session_state.training_summary)

st.markdown("---")

# =========================================================================
# –≠–¢–ê–ü 6: –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –° –í–ï–ö–¢–û–†–ù–´–ú–ò –ü–†–û–°–¢–†–ê–ù–°–¢–í–ê–ú–ò
# =========================================================================
st.header("üßÆ 6. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞–º–∏")
st.markdown("""
**–¢–µ–æ—Ä–∏—è:** —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤. –ú—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è, –∞–Ω–∞–ª–æ–≥–∏–∏ –∏ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π, —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º–æ–¥–µ–ª–∏ –æ—Ç—Ä–∞–∂–∞—é—Ç —Å–º—ã—Å–ª–æ–≤—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
""")

if not st.session_state.get('models_trained') or not st.session_state.get('distributed_models'):
    st.warning("‚ö†Ô∏è –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –º–æ–¥–µ–ª—å –Ω–∞ —ç—Ç–∞–ø–µ 5, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã.")
else:
    available_models = {}
    try:
        available_models = st.session_state.distributed_models.get_available_models() or {}
    except Exception as exc:
        st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π: {exc}")

    if not available_models:
        st.info("‚ÑπÔ∏è –û–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ 5.")
    else:
        semantic_ops = SemanticOperations(available_models)
        model_names = list(available_models.keys())

        if not st.session_state.get('semantic_styles_applied'):
            st.markdown(
                """
                <style>
                    .semantic-card {
                        background: linear-gradient(135deg, rgba(247,249,252,0.97), rgba(255,255,255,0.9));
                        border: 1px solid rgba(229,234,242,0.9);
                        border-radius: 18px;
                        padding: 1.1rem 1.25rem;
                        margin-bottom: 1.2rem;
                        box-shadow: 0 6px 16px rgba(15, 42, 98, 0.04);
                    }
                    .semantic-card h4 {
                        font-size: 1.05rem;
                        margin-bottom: 0.75rem;
                        font-weight: 600;
                    }
                    .semantic-chip {
                        display: inline-block;
                        padding: 0.25rem 0.7rem;
                        border-radius: 999px;
                        font-size: 0.8rem;
                        font-weight: 600;
                        margin-right: 0.4rem;
                        margin-bottom: 0.35rem;
                        color: #2e3a59;
                        background: rgba(67,97,238,0.08);
                        border: 1px solid rgba(67,97,238,0.18);
                    }
                    .semantic-metric-label {
                        font-size: 0.75rem;
                        letter-spacing: 0.05em;
                        text-transform: uppercase;
                        color: #6b7a99;
                    }
                    .semantic-metric-value {
                        font-size: 1.45rem;
                        font-weight: 600;
                        color: #2e3a59;
                    }
                    .semantic-hint {
                        font-size: 0.9rem;
                        color: #51607a;
                        margin-bottom: 0;
                    }
                    .semantic-badge {
                        display: inline-flex;
                        align-items: center;
                        font-size: 0.8rem;
                        padding: 0.3rem 0.6rem;
                        border-radius: 999px;
                        background: rgba(20, 184, 166, 0.12);
                        color: #0f766e;
                        border: 1px solid rgba(20, 184, 166, 0.25);
                        margin-right: 0.4rem;
                        margin-bottom: 0.35rem;
                    }
                    .semantic-badge span {
                        font-weight: 600;
                        margin-left: 0.35rem;
                    }
                </style>
                """,
                unsafe_allow_html=True
            )
            st.session_state.semantic_styles_applied = True

        if not model_names:
            st.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–∞—Ä—É–∂–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ 5.")
            st.stop()

        if st.session_state.get('semantic_model_select') not in model_names:
            st.session_state.semantic_model_select = model_names[0]

        selected_model = st.session_state.get('semantic_model_select')

        tab_similarity, tab_analogies, tab_axes, tab_neighbors, tab_report = st.tabs([
            "6.1 –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∏ —Å—Ö–æ–¥—Å—Ç–≤–æ",
            "6.2 –í–µ–∫—Ç–æ—Ä–Ω—ã–µ –∞–Ω–∞–ª–æ–≥–∏–∏",
            "6.3 –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–∏",
            "6.4 –ë–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏",
            "6.5 –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç"
        ])

        with tab_similarity:
            st.subheader("6.1. –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ")

            selected_model = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å",
                options=model_names,
                index=0,
                key="semantic_model_select"
            )

            if selected_model:
                if st.session_state.get('semantic_active_model') != selected_model:
                    st.session_state.semantic_active_model = selected_model
                    st.session_state.pop('semantic_pair_report', None)
                    st.session_state.pop('semantic_matrix_result', None)
                    st.session_state.pop('semantic_group_report', None)
                    st.session_state.pop('semantic_manual_analogy_result', None)
                    st.session_state.pop('semantic_category_analogy_result', None)

                if 'semantic_pair_report' not in st.session_state:
                    st.session_state.semantic_pair_report = None
                if 'semantic_matrix_result' not in st.session_state:
                    st.session_state.semantic_matrix_result = None
                if 'semantic_group_report' not in st.session_state:
                    st.session_state.semantic_group_report = None

                model_stats = semantic_ops.get_model_statistics(selected_model)
                if model_stats:
                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üîç –ü—Ä–æ—Ñ–∏–ª—å –º–æ–¥–µ–ª–∏")
                        stat_col1, stat_col2, stat_col3 = st.columns(3)
                        stat_col1.markdown(
                            f"""
                            <div class="semantic-metric-label">–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è</div>
                            <div class="semantic-metric-value">{model_stats.get('vocabulary_size', 0):,}</div>
                            """,
                            unsafe_allow_html=True
                        )
                        stat_col2.markdown(
                            f"""
                            <div class="semantic-metric-label">–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤</div>
                            <div class="semantic-metric-value">{model_stats.get('vector_size', '‚Äî')}</div>
                            """,
                            unsafe_allow_html=True
                        )
                        stat_col3.markdown(
                            f"""
                            <div class="semantic-metric-label">–†–∞–∑–º–µ—Ä –æ–∫–Ω–∞</div>
                            <div class="semantic-metric-value">{model_stats.get('window_size', '‚Äî')}</div>
                            """,
                            unsafe_allow_html=True
                        )
                        st.markdown("</div>", unsafe_allow_html=True)

                st.markdown("### üîç –ê–Ω–∞–ª–∏–∑ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø–∞—Ä —Å–ª–æ–≤")

                default_pairs = [
                    ("–∫–æ–º–ø—å—é—Ç–µ—Ä", "–Ω–æ—É—Ç–±—É–∫"),
                    ("–¥–∞–Ω–Ω—ã–µ", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"),
                    ("–≥–æ—Ä–æ–¥", "—Å—Ç–æ–ª–∏—Ü–∞"),
                    ("–∂–µ–Ω—â–∏–Ω–∞", "–¥–µ–≤—É—à–∫–∞"),
                    ("—Ä–∞–±–æ—Ç–∞", "—Ç—Ä—É–¥")
                ]
                default_pairs_text = "\n".join([", ".join(pair) for pair in default_pairs])

                with st.container():
                    st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                    st.markdown("#### ‚úèÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä —Å–ª–æ–≤")
                    pairs_input = st.text_area(
                        "–£–∫–∞–∂–∏—Ç–µ –ø–∞—Ä—ã —Å–ª–æ–≤ (–ø–æ –æ–¥–Ω–æ–π –ø–∞—Ä–µ –≤ —Å—Ç—Ä–æ–∫–µ, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å ‚Äî –∑–∞–ø—è—Ç–∞—è)",
                        value=default_pairs_text,
                        help="–ü—Ä–∏–º–µ—Ä —Å—Ç—Ä–æ–∫–∏: –∫–æ–º–ø—å—é—Ç–µ—Ä, –Ω–æ—É—Ç–±—É–∫"
                    )
                    st.markdown(
                        '<p class="semantic-hint">–î–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–∏ –ø–∞—Ä—ã –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.</p>',
                        unsafe_allow_html=True
                    )
                    st.markdown("</div>", unsafe_allow_html=True)

                parsed_pairs = []
                for line in pairs_input.splitlines():
                    parts = [part.strip() for part in line.split(",") if part.strip()]
                    if len(parts) >= 2:
                        parsed_pairs.append((parts[0], parts[1]))

                with st.container():
                    st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                    compute_pairs_btn = st.button(
                        "üîÅ –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–∞—Ä",
                        key="compute_pairs_similarity"
                    )

                    if compute_pairs_btn:
                        if parsed_pairs:
                            with st.spinner("–í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–∞—Ä —Å–ª–æ–≤..."):
                                similarity_report = semantic_ops.cosine_similarity_analysis(selected_model, parsed_pairs)

                            st.session_state.semantic_pair_report = {
                                "pairwise_results": similarity_report.get('pairwise_analysis', []),
                                "distribution_stats": similarity_report.get('distribution_analysis', {}),
                                "input_pairs": list(parsed_pairs)
                            }
                        else:
                            st.warning("–î–æ–±–∞–≤—å—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é –ø–∞—Ä—É —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
                            st.session_state.semantic_pair_report = None

                    pair_report = st.session_state.get('semantic_pair_report')
                    if pair_report:
                        if pair_report.get('input_pairs') != parsed_pairs:
                            st.info("–°–ø–∏—Å–æ–∫ –ø–∞—Ä –∏–∑–º–µ–Ω—ë–Ω. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É, —á—Ç–æ–±—ã –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.")

                        pairwise_results = pair_report.get('pairwise_results', [])
                        if pairwise_results:
                            st.markdown("##### üìã –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –ø–∞—Ä–∞–º —Å–ª–æ–≤")
                            pairwise_df = pd.DataFrame(pairwise_results)
                            st.dataframe(pairwise_df, use_container_width=True)

                        distribution_stats = pair_report.get('distribution_stats') or {}
                        if distribution_stats:
                            st.markdown("##### üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è")
                            col_ds1, col_ds2, col_ds3, col_ds4 = st.columns(4)
                            col_ds1.metric("–°—Ä–µ–¥–Ω–µ–µ", f"{distribution_stats.get('mean_similarity', 0):.3f}")
                            col_ds2.metric("–°—Ç. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ", f"{distribution_stats.get('std_similarity', 0):.3f}")
                            col_ds3.metric("–ú–∏–Ω.", f"{distribution_stats.get('min_similarity', 0):.3f}")
                            col_ds4.metric("–ú–∞–∫—Å.", f"{distribution_stats.get('max_similarity', 0):.3f}")
                    else:
                        st.info("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –≤—ã—à–µ, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å—á—ë—Ç —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–∞—Ä.")
                    st.markdown("</div>", unsafe_allow_html=True)

                st.session_state.setdefault('semantic_distance_cache', {})

                st.markdown("### üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –º–∞—Ç—Ä–∏—Ü–∞")
                refresh_distance = st.button(
                    "üîÑ –ü–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ",
                    key=f"refresh_distance_{selected_model}"
                )

                cached_distance = st.session_state.semantic_distance_cache.get(selected_model)
                if refresh_distance or not cached_distance:
                    with st.spinner("–°—Ç—Ä–æ–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π..."):
                        distance_report = semantic_ops.analyze_distance_distribution(selected_model)
                    st.session_state.semantic_distance_cache[selected_model] = distance_report
                else:
                    distance_report = cached_distance

                if distance_report:
                    dist_info = distance_report.get('distance_distribution', {})
                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üéØ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π")
                        col_md1, col_md2, col_md3 = st.columns(3)
                        col_md1.metric("–°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ", f"{distance_report.get('mean_distance', 0):.3f}")
                        col_md2.metric("–°—Ç. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ", f"{distance_report.get('std_distance', 0):.3f}")
                        col_md3.metric("–î–∏–∞–ø–∞–∑–æ–Ω", f"{distance_report.get('min_distance', 0):.3f} ‚Äì {distance_report.get('max_distance', 0):.3f}")

                    if dist_info:
                        hist = dist_info.get('histogram', [])
                        bin_centers = dist_info.get('bin_centers', [])
                        if hist and bin_centers:
                            fig_hist, ax_hist = plt.subplots()
                            bin_width = bin_centers[1] - bin_centers[0] if len(bin_centers) > 1 else 0.05
                            ax_hist.bar(bin_centers, hist, width=bin_width, color="#4C72B0", alpha=0.8)
                            ax_hist.set_xlabel("–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ")
                            ax_hist.set_ylabel("–ß–∞—Å—Ç–æ—Ç–∞")
                            ax_hist.set_title("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω—ã—Ö —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π (—Å–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ —Å–ª–æ–≤)")
                            st.pyplot(fig_hist)
                        st.markdown("</div>", unsafe_allow_html=True)

                    demo_word_pool = [
                        "–∫–æ–º–ø—å—é—Ç–µ—Ä", "–ø—Ä–æ–≥—Ä–∞–º–º–∞", "–¥–∞–Ω–Ω—ã–µ", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "–∞–ª–≥–æ—Ä–∏—Ç–º",
                        "—Å–∏—Å—Ç–µ–º–∞", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è", "—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞", "—Å–µ—Ç—å", "–±–∞–∑–∞"
                    ]
                    available_demo_words = [
                        word for word in demo_word_pool
                        if word in available_models[selected_model].wv
                    ]
                    selected_demo_words = available_demo_words[:10]

                    if len(selected_demo_words) >= 2:
                        demo_vectors = np.array([available_models[selected_model].wv[word] for word in selected_demo_words])
                        demo_norms = np.linalg.norm(demo_vectors, axis=1, keepdims=True)
                        demo_norms[demo_norms == 0] = 1.0
                        demo_vectors = demo_vectors / demo_norms
                        demo_similarity_matrix = np.dot(demo_vectors, demo_vectors.T)

                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown("#### üü© –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ (10 —Å–ª–æ–≤)")
                            st.markdown(
                                "".join([f'<span class="semantic-chip">{word}</span>' for word in selected_demo_words]),
                                unsafe_allow_html=True
                            )
                            demo_df = pd.DataFrame(demo_similarity_matrix, index=selected_demo_words, columns=selected_demo_words)
                            st.dataframe(demo_df, use_container_width=True)

                            fig_heat, ax_heat = plt.subplots(figsize=(5, 4))
                            cax = ax_heat.imshow(demo_df.values, cmap="viridis", vmin=-1, vmax=1)
                            ax_heat.set_xticks(range(len(selected_demo_words)))
                            ax_heat.set_xticklabels(selected_demo_words, rotation=90)
                            ax_heat.set_yticks(range(len(selected_demo_words)))
                            ax_heat.set_yticklabels(selected_demo_words)
                            ax_heat.set_title("–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ (10 —Å–ª–æ–≤)")
                            fig_heat.colorbar(cax, fraction=0.046, pad=0.04)
                            st.pyplot(fig_heat)
                            st.markdown("</div>", unsafe_allow_html=True)

                st.markdown("### üß© –ú–∞—Ç—Ä–∏—Ü–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤")

                with st.container():
                    st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                    st.markdown("#### üéØ –í–∞—à –Ω–∞–±–æ—Ä —Å–ª–æ–≤")
                    default_test_words = "–∫–æ–º–ø—å—é—Ç–µ—Ä, –Ω–æ—É—Ç–±—É–∫, –¥–∞–Ω–Ω—ã–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è, —Å–∏—Å—Ç–µ–º–∞"
                    user_words_input = st.text_area(
                        "–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–º–∏–Ω–∏–º—É–º –¥–≤–∞ —Å–ª–æ–≤–∞) –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã",
                        value=default_test_words,
                        key="semantic_user_words"
                    )
                    st.markdown(
                        '<p class="semantic-hint">–°–æ–≤–µ—Ç: –∫–æ–º–±–∏–Ω–∏—Ä—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –≥—Ä—É–ø–ø—ã, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏.</p>',
                        unsafe_allow_html=True
                    )

                matrix_result = st.session_state.get('semantic_matrix_result')
                if matrix_result and matrix_result.get('source_input') != user_words_input:
                    st.info("–°–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –±—ã–ª –∏–∑–º–µ–Ω—ë–Ω. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã.")

                build_matrix_btn = st.button(
                    "–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤–∞",
                    key="build_user_similarity"
                )

                if build_matrix_btn:
                    test_words = [w.strip() for w in user_words_input.split(",") if w.strip()]

                    if len(test_words) < 2:
                        st.warning("–£–∫–∞–∂–∏—Ç–µ –º–∏–Ω–∏–º—É–º –¥–≤–∞ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã.")
                        st.session_state.semantic_matrix_result = None
                    else:
                        available_words = [word for word in test_words if word in available_models[selected_model].wv]
                        missing_words = [word for word in test_words if word and word not in available_models[selected_model].wv]

                        if missing_words:
                            st.warning(f"–°–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏: {', '.join(missing_words)}")

                        if len(available_words) >= 2:
                            vectors = np.array([available_models[selected_model].wv[word] for word in available_words])
                            norms = np.linalg.norm(vectors, axis=1, keepdims=True)
                            norms[norms == 0] = 1.0
                            normalized_vectors = vectors / norms
                            similarity_matrix_test = np.dot(normalized_vectors, normalized_vectors.T)

                            st.session_state.semantic_matrix_result = {
                                "available_words": available_words,
                                "matrix": similarity_matrix_test.tolist(),
                                "missing_words": missing_words,
                                "source_input": user_words_input
                            }
                        else:
                            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã.")
                            st.session_state.semantic_matrix_result = None

                matrix_result = st.session_state.get('semantic_matrix_result')
                if matrix_result:
                    available_words = matrix_result.get('available_words', [])
                    missing_words = matrix_result.get('missing_words', [])

                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üßæ –ú–∞—Ç—Ä–∏—Ü–∞ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–º —Å–ª–æ–≤–∞–º")

                        if missing_words:
                            st.warning(f"–°–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏: {', '.join(missing_words)}")

                        if len(available_words) >= 2:
                            similarity_df = pd.DataFrame(
                                matrix_result.get('matrix', []),
                                index=available_words,
                                columns=available_words
                            )

                            st.dataframe(similarity_df, use_container_width=True)

                            fig_test, ax_test = plt.subplots(figsize=(6, 5))
                            cax_test = ax_test.imshow(similarity_df.values, cmap="magma", vmin=-1, vmax=1)
                            ax_test.set_xticks(range(len(available_words)))
                            ax_test.set_xticklabels(available_words, rotation=90)
                            ax_test.set_yticks(range(len(available_words)))
                            ax_test.set_yticklabels(available_words)
                            ax_test.set_title("–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤")
                            fig_test.colorbar(cax_test, fraction=0.046, pad=0.04)
                            st.pyplot(fig_test)
                        else:
                            st.info("–î–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –º–∏–Ω–∏–º—É–º –¥–≤–∞ —Å–ª–æ–≤–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏.")
                        st.markdown("</div>", unsafe_allow_html=True)
                else:
                    st.info("–ú–∞—Ç—Ä–∏—Ü–∞ –µ—â—ë –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å—á—ë—Ç.")

                st.markdown("### ‚öñÔ∏è –û—Ü–µ–Ω–∫–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –¥–ª—è —Å–∏–Ω–æ–Ω–∏–º–æ–≤, –∞–Ω—Ç–æ–Ω–∏–º–æ–≤ –∏ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ä")

                compute_groups_btn = st.button(
                    "–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä—É–ø–ø–∞–º",
                    key="compute_semantic_groups"
                )

                if compute_groups_btn:
                    test_sets = get_russian_test_sets()

                    semantic_groups = {
                        "–°–∏–Ω–æ–Ω–∏–º—ã": [
                            ("–∫–æ–º–ø—å—é—Ç–µ—Ä", "–Ω–æ—É—Ç–±—É–∫"),
                            ("–ø—Ä–æ–≥—Ä–∞–º–º–∞", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"),
                            ("–¥–∞–Ω–Ω—ã–µ", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"),
                            ("–∂–µ–Ω—â–∏–Ω–∞", "–¥–µ–≤—É—à–∫–∞"),
                            ("—Ä–∞–±–æ—Ç–∞", "—Ç—Ä—É–¥")
                        ],
                        "–ê–Ω—Ç–æ–Ω–∏–º—ã": [
                            ("—Ö–æ—Ä–æ—à–∏–π", "–ø–ª–æ—Ö–æ–π"),
                            ("–¥–µ–Ω—å", "–Ω–æ—á—å"),
                            ("–≤—ã—Å–æ–∫–∏–π", "–Ω–∏–∑–∫–∏–π"),
                            ("–≥–æ—Ä—è—á–∏–π", "—Ö–æ–ª–æ–¥–Ω—ã–π"),
                            ("–º–∏—Ä", "–≤–æ–π–Ω–∞")
                        ],
                        "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä—ã": test_sets.get('semantic_relationships', [])
                    }

                    aggregate_rows = []
                    detailed_results = {}

                    for group_name, group_pairs in semantic_groups.items():
                        valid_pairs = [(pair[0], pair[1]) for pair in group_pairs if len(pair) >= 2]
                        if not valid_pairs:
                            detailed_results[group_name] = []
                            continue

                        group_report = semantic_ops.cosine_similarity_analysis(selected_model, valid_pairs)
                        pair_results = group_report.get('pairwise_analysis', [])
                        detailed_results[group_name] = pair_results

                        similarities = [row['cosine_similarity'] for row in pair_results if row.get('cosine_similarity') is not None]
                        coverage = sum(1 for row in pair_results if row.get('cosine_similarity') is not None)

                        aggregate_rows.append({
                            "–ì—Ä—É–ø–ø–∞": group_name,
                            "–°—Ä–µ–¥–Ω–µ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ": float(np.mean(similarities)) if similarities else None,
                            "–°—Ç. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ": float(np.std(similarities)) if similarities else None,
                            "–ü–æ–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ä": f"{coverage}/{len(valid_pairs)}"
                        })

                    st.session_state.semantic_group_report = {
                        "aggregate": aggregate_rows,
                        "details": detailed_results
                    }

                group_report = st.session_state.get('semantic_group_report')
                if group_report:
                    aggregate_rows = group_report.get('aggregate') or []
                    if aggregate_rows:
                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown("#### üìö –ò—Ç–æ–≥–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º")
                            aggregate_df = pd.DataFrame(aggregate_rows)
                            st.dataframe(aggregate_df, use_container_width=True)
                            st.markdown("</div>", unsafe_allow_html=True)
                    else:
                        st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π.")

                    details = group_report.get('details') or {}
                    if details:
                        with st.expander("–î–µ—Ç–∞–ª–∏ –ø–æ –≥—Ä—É–ø–ø–∞–º —Å–ª–æ–≤", expanded=False):
                            tabs_labels = list(details.keys())
                            if tabs_labels:
                                detail_tabs = st.tabs(tabs_labels)
                                for tab_widget, group_name in zip(detail_tabs, tabs_labels):
                                    with tab_widget:
                                        group_items = details.get(group_name, [])
                                        if group_items:
                                            st.markdown(
                                                "".join([f'<span class="semantic-chip">{pair["word_pair"]}</span>' for pair in group_items if pair.get("word_pair")]),
                                                unsafe_allow_html=True
                                            )
                                            st.dataframe(pd.DataFrame(group_items), use_container_width=True)
                                        else:
                                            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
                    else:
                        st.info("–ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç.")
                else:
                    st.info("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Ä–∞—Å—á—ë—Ç –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –≥—Ä—É–ø–ø–∞–º.")

        with tab_analogies:
            st.subheader("6.2. –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ –∏ word analogies")

            selected_model_name = st.session_state.get('semantic_model_select')

            if not selected_model_name:
                st.warning("–°–Ω–∞—á–∞–ª–∞ –≤—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.1 –∏ —É–∫–∞–∂–∏—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞.")
            else:
                model_ref = available_models.get(selected_model_name)
                if model_ref is None:
                    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ 5.")
                else:
                    st.session_state.setdefault('manual_word_a', '–º—É–∂—á–∏–Ω–∞')
                    st.session_state.setdefault('manual_word_b', '–∂–µ–Ω—â–∏–Ω–∞')
                    st.session_state.setdefault('manual_word_c', '–∫–æ—Ä–æ–ª—å')
                    st.session_state.setdefault('manual_topn', 5)
                    st.session_state.setdefault('manual_preset_choice', "‚Äî")
                    st.session_state.setdefault('manual_pending_update', None)
                    st.session_state.setdefault('manual_reset_choice', False)
                    st.session_state.setdefault('semantic_manual_analogy_result', None)
                    st.session_state.setdefault('semantic_category_analogy_result', None)

                    if st.session_state.pop('manual_reset_choice', False):
                        st.session_state.manual_preset_choice = "‚Äî"

                    pending_update = st.session_state.pop('manual_pending_update', None)
                    if pending_update is not None:
                        a_val, b_val, c_val = pending_update
                        st.session_state.manual_word_a = a_val
                        st.session_state.manual_word_b = b_val
                        st.session_state.manual_word_c = c_val

                    preset_analogies = {
                        "üëë –ú—É–∂—á–∏–Ω–∞ ‚àí –ñ–µ–Ω—â–∏–Ω–∞ + –ö–æ—Ä–æ–ª—å": ("–º—É–∂—á–∏–Ω–∞", "–∂–µ–Ω—â–∏–Ω–∞", "–∫–æ—Ä–æ–ª—å"),
                        "üåç –ú–æ—Å–∫–≤–∞ ‚àí –†–æ—Å—Å–∏—è + –§—Ä–∞–Ω—Ü–∏—è": ("–ú–æ—Å–∫–≤–∞", "–†–æ—Å—Å–∏—è", "–§—Ä–∞–Ω—Ü–∏—è"),
                        "‚öñÔ∏è –•–æ—Ä–æ—à–∏–π ‚àí –õ—É—á—à–µ + –ü–ª–æ—Ö–æ–π": ("—Ö–æ—Ä–æ—à–∏–π", "–ª—É—á—à–µ", "–ø–ª–æ—Ö–æ–π"),
                        "‚úçÔ∏è –î–µ–ª–∞—Ç—å ‚àí –°–¥–µ–ª–∞–ª + –ü–∏—Å–∞—Ç—å": ("–¥–µ–ª–∞—Ç—å", "—Å–¥–µ–ª–∞–ª", "–ø–∏—Å–∞—Ç—å"),
                        "üèôÔ∏è –õ–æ–Ω–¥–æ–Ω ‚àí –ê–Ω–≥–ª–∏—è + –ì–µ—Ä–º–∞–Ω–∏—è": ("–õ–æ–Ω–¥–æ–Ω", "–ê–Ω–≥–ª–∏—è", "–ì–µ—Ä–º–∞–Ω–∏—è")
                    }

                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üßÆ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞ (A ‚àí B + C)")
                        st.markdown(
                            '<p class="semantic-hint">–í–≤–µ–¥–∏—Ç–µ —Ç—Ä–∏ —Å–ª–æ–≤–∞ –∏ –ø–æ–ª—É—á–∏—Ç–µ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–º–∏ –∫ –≤—ã—Ä–∞–∂–µ–Ω–∏—é A ‚àí B + C.</p>',
                            unsafe_allow_html=True
                        )

                        preset_options_list = ["‚Äî"] + list(preset_analogies.keys())
                        current_preset = st.session_state.get('manual_preset_choice', "‚Äî")
                        if current_preset not in preset_options_list:
                            current_preset = "‚Äî"
                            st.session_state.manual_preset_choice = "‚Äî"

                        preset_choice = st.selectbox(
                            "–ë—ã—Å—Ç—Ä—ã–π –ø—Ä–∏–º–µ—Ä",
                            options=preset_options_list,
                            index=preset_options_list.index(current_preset),
                            key="manual_preset_choice"
                        )

                        preset_cols = st.columns([1, 1, 2])
                        with preset_cols[0]:
                            apply_preset = st.button("–ü–æ–¥—Å—Ç–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä", key="apply_manual_preset")
                        with preset_cols[1]:
                            clear_inputs = st.button("–û—á–∏—Å—Ç–∏—Ç—å –≤–≤–æ–¥", key="clear_manual_inputs")

                        if apply_preset:
                            choice = st.session_state.get('manual_preset_choice', "‚Äî")
                            if choice != "‚Äî":
                                st.session_state.manual_pending_update = preset_analogies.get(choice, ("", "", ""))
                                st.session_state.semantic_manual_analogy_result = None

                        if clear_inputs:
                            st.session_state.manual_pending_update = ("", "", "")
                            st.session_state.manual_reset_choice = True
                            st.session_state.semantic_manual_analogy_result = None

                        col_a, col_b, col_c = st.columns(3)
                        word_a = col_a.text_input(
                            "–°–ª–æ–≤–æ A (–∏—Å—Ö–æ–¥–Ω–æ–µ)",
                            value=st.session_state.get('manual_word_a', '–º—É–∂—á–∏–Ω–∞'),
                            key="manual_word_a"
                        )
                        word_b = col_b.text_input(
                            "–°–ª–æ–≤–æ B (–≤—ã—á–µ—Å—Ç—å)",
                            value=st.session_state.get('manual_word_b', '–∂–µ–Ω—â–∏–Ω–∞'),
                            key="manual_word_b"
                        )
                        word_c = col_c.text_input(
                            "–°–ª–æ–≤–æ C (–¥–æ–±–∞–≤–∏—Ç—å)",
                            value=st.session_state.get('manual_word_c', '–∫–æ—Ä–æ–ª—å'),
                            key="manual_word_c"
                        )

                        topn = st.slider(
                            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (Top-N)",
                            min_value=1,
                            max_value=10,
                            value=int(st.session_state.get('manual_topn', 5)),
                            key="manual_topn"
                        )

                        run_manual_btn = st.button("üîé –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—é", key="run_manual_analogy")
                        st.markdown("</div>", unsafe_allow_html=True)

                    if run_manual_btn:
                        words = [word_a.strip(), word_b.strip(), word_c.strip()]
                        if any(not w for w in words):
                            st.warning("–ó–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ —Ç—Ä–∏ —Å–ª–æ–≤–∞, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É.")
                            st.session_state.semantic_manual_analogy_result = None
                        else:
                            missing_words = [w for w in words if w not in model_ref.wv]
                            if missing_words:
                                st.warning(f"–°–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏: {', '.join(missing_words)}")
                                st.session_state.semantic_manual_analogy_result = {
                                    "error": f"–°–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ: {', '.join(missing_words)}",
                                    "words": tuple(words)
                                }
                            else:
                                try:
                                    results = model_ref.wv.most_similar(
                                        positive=[word_a, word_c],
                                        negative=[word_b],
                                        topn=int(topn)
                                    )
                                    st.session_state.semantic_manual_analogy_result = {
                                        "words": tuple(words),
                                        "topn": int(topn),
                                        "results": [(candidate, float(score)) for candidate, score in results]
                                    }
                                except Exception as err:
                                    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—é: {err}")
                                    st.session_state.semantic_manual_analogy_result = {
                                        "error": f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {err}",
                                        "words": tuple(words)
                                    }

                    manual_result = st.session_state.get('semantic_manual_analogy_result')
                    current_words_tuple = (word_a.strip(), word_b.strip(), word_c.strip())
                    if manual_result and not manual_result.get('error') and manual_result.get('words'):
                        if tuple(w.strip() for w in manual_result.get('words')) != current_words_tuple:
                            st.info("–í—ã –∏–∑–º–µ–Ω–∏–ª–∏ –≤—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞. –ù–∞–∂–º–∏—Ç–µ ¬´–í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–æ–≥–∏—é¬ª, —á—Ç–æ–±—ã –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.")

                    if manual_result:
                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown("#### üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–æ–≥–∏–∏")

                            error_text = manual_result.get('error')
                            if error_text:
                                st.warning(error_text)
                            else:
                                words_tuple = manual_result.get('words', ("", "", ""))
                                expression = f"{words_tuple[0]} ‚àí {words_tuple[1]} + {words_tuple[2]}"
                                st.markdown(f"**–í—ã—Ä–∞–∂–µ–Ω–∏–µ:** `{expression}`")

                                results_list = manual_result.get('results', [])
                                if results_list:
                                    top_word, top_score = results_list[0]
                                    badge_html = (
                                        f'<span class="semantic-badge">–û—Ç–≤–µ—Ç<span>{top_word}</span></span>'
                                        f'<span class="semantic-badge">–°—Ö–æ–¥—Å—Ç–≤–æ<span>{top_score:.3f}</span></span>'
                                    )
                                    st.markdown(badge_html, unsafe_allow_html=True)

                                    manual_df = pd.DataFrame(results_list, columns=["–°–ª–æ–≤–æ", "–°—Ö–æ–¥—Å—Ç–≤–æ"])
                                    st.dataframe(manual_df, use_container_width=True)
                                else:
                                    st.info("–ú–æ–¥–µ–ª—å –Ω–µ –≤–µ—Ä–Ω—É–ª–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è.")
                            st.markdown("</div>", unsafe_allow_html=True)
                    else:
                        st.info("–í–≤–µ–¥–∏—Ç–µ —Å–ª–æ–≤–∞ –∏ –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã—Ä–∞–∂–µ–Ω–∏—è A ‚àí B + C.")

                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üìö –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∞–Ω–∞–ª–æ–≥–∏–π")
                        st.markdown(
                            '<p class="semantic-hint">–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–∏ –≥—Ä—É–ø–ø—ã: —Å—Ç–æ–ª–∏—Ü—ã —Å—Ç—Ä–∞–Ω, —Å—Ç–µ–ø–µ–Ω–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ –≥–ª–∞–≥–æ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.</p>',
                            unsafe_allow_html=True
                        )
                        run_categories_btn = st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ—Ü–µ–Ω–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π", key="run_category_analogies")
                        st.markdown("</div>", unsafe_allow_html=True)

                    if run_categories_btn:
                        with st.spinner("–í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–æ–≥–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º..."):
                            category_eval = semantic_ops.categorical_analogy_evaluation(selected_model_name)
                        st.session_state.semantic_category_analogy_result = category_eval

                    category_result = st.session_state.get('semantic_category_analogy_result')
                    if category_result:
                        label_map = {
                            "semantic_capitals": "–°—Ç–æ–ª–∏—Ü—ã —Å—Ç—Ä–∞–Ω",
                            "semantic_gender": "–†–æ–¥–æ–≤—ã–µ –ø–∞—Ä—ã",
                            "syntactic_comparative": "–°—Ç–µ–ø–µ–Ω–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è",
                            "morphological_verbs": "–ì–ª–∞–≥–æ–ª—ã –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏"
                        }

                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown("#### üìà –°–≤–æ–¥–∫–∞ –ø–æ –∞–Ω–∞–ª–æ–≥–∏—è–º")
                            overall_accuracy = category_result.get('overall_accuracy', 0.0)
                            total_tests = category_result.get('total_tests', 0)
                            total_correct = category_result.get('total_correct', 0)

                            col_acc1, col_acc2, col_acc3 = st.columns(3)
                            col_acc1.metric("–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å", f"{overall_accuracy * 100:.1f}%")
                            col_acc2.metric("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤", total_tests)
                            col_acc3.metric("–í–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤", total_correct)

                            summary_rows = []
                            for key, value in category_result.items():
                                if key in ("overall_accuracy", "total_tests", "total_correct"):
                                    continue
                                total = value.get('total', 0)
                                if total == 0:
                                    accuracy_text = "‚Äî"
                                else:
                                    accuracy_text = f"{value.get('accuracy', 0.0) * 100:.1f}%"
                                summary_rows.append({
                                    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": label_map.get(key, key),
                                    "–¢–µ—Å—Ç–æ–≤": total,
                                    "–í–µ—Ä–Ω–æ": value.get('correct', 0),
                                    "–¢–æ—á–Ω–æ—Å—Ç—å": accuracy_text
                                })

                            if summary_rows:
                                summary_df = pd.DataFrame(summary_rows)
                                st.dataframe(summary_df, use_container_width=True)
                            else:
                                st.info("–î–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Ç–µ—Å—Ç—ã –∞–Ω–∞–ª–æ–≥–∏–π.")
                            st.markdown("</div>", unsafe_allow_html=True)

                        with st.expander("–î–µ—Ç–∞–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º", expanded=False):
                            for key, value in category_result.items():
                                if key in ("overall_accuracy", "total_tests", "total_correct"):
                                    continue
                                friendly_name = label_map.get(key, key)
                                details = value.get('details', [])
                                total = value.get('total', 0)
                                correct = value.get('correct', 0)
                                accuracy_pct = value.get('accuracy', 0.0) * 100 if total else 0.0

                                st.markdown(
                                    f"**{friendly_name}** ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å {accuracy_pct:.1f}% ({correct}/{total})"
                                )
                                if details:
                                    detail_df = pd.DataFrame([
                                        {
                                            "–ê–Ω–∞–ª–æ–≥–∏—è": item.get('analogy'),
                                            "–û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç": item.get('expected'),
                                            "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ": item.get('predicted'),
                                            "–¢–æ–ø-1": "‚úÖ" if item.get('is_correct') else "‚ùå",
                                            "–¢–æ–ø-5": ", ".join(item.get('top_5', []))
                                        }
                                        for item in details
                                    ])
                                    st.dataframe(detail_df, use_container_width=True)
                                else:
                                    st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö: —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏.")
                    else:
                        st.info("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –∞–Ω–∞–ª–æ–≥–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º.")

        with tab_axes:
            st.subheader("6.3. –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π")

            selected_model_name = st.session_state.get('semantic_model_select')
            if not selected_model_name:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.1, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –æ—Å–µ–π.")
            else:
                st.session_state.setdefault('semantic_axes_cache', {})
                st.session_state.setdefault('semantic_axes_custom_words', "–º—É–∂—á–∏–Ω–∞, –∂–µ–Ω—â–∏–Ω–∞, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç, —É—á–∏—Ç–µ–ª—å, —Ö–æ—Ä–æ—à–∏–π, –ø–ª–æ—Ö–æ–π, —É—Å–ø–µ—Ö, –ø—Ä–æ–≤–∞–ª")
                st.session_state.setdefault('semantic_axes_topn', 8)

                with st.container():
                    st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                    st.markdown("#### üß≠ –û–ø–∏—Å–∞–Ω–∏–µ")
                    st.markdown(
                        """
                        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –æ—Å–∏ (–≥–µ–Ω–¥–µ—Ä–Ω—É—é, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—É—é, –æ—Ü–µ–Ω–æ—á–Ω—É—é –∏ –≤—Ä–µ–º–µ–Ω–Ω—É—é), 
                        –∏–∑–º–µ—Ä—è–µ–º —Å–º–µ—â–µ–Ω–∏–µ –∏ —Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –æ–∫–∞–∑—ã–≤–∞—é—Ç—Å—è –Ω–∞ –ø–æ–ª—é—Å–∞—Ö. –í—ã —Ç–∞–∫–∂–µ –º–æ–∂–µ—Ç–µ 
                        –ø—Ä–æ–µ—Ü–∏—Ä–æ–≤–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –Ω–∞–±–æ—Ä —Å–ª–æ–≤ –Ω–∞ –∫–∞–∂–¥—É—é –æ—Å—å.
                        """
                    )
                    st.markdown("</div>", unsafe_allow_html=True)

                axes_topn = st.slider(
                    "–°–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ –∫–∞–∂–¥–æ–º –ø–æ–ª—é—Å–µ",
                    min_value=3,
                    max_value=15,
                    value=int(st.session_state.get('semantic_axes_topn', 8)),
                    key="semantic_axes_topn"
                )

                custom_words_input = st.text_area(
                    "–°–ª–æ–≤–∞ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
                    key="semantic_axes_custom_words"
                )

                compute_axes_btn = st.button("üîç –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –æ—Å–µ–π", key="compute_semantic_axes")

                if compute_axes_btn:
                    with st.spinner("–í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –æ—Å–∏..."):
                        axes_result = semantic_ops.comprehensive_axes_analysis(selected_model_name)
                    st.session_state.semantic_axes_cache[selected_model_name] = axes_result

                axes_result = st.session_state.semantic_axes_cache.get(selected_model_name)

                if not axes_result:
                    st.info("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –≤—ã—à–µ, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Å–µ–π –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.")
                else:
                    axis_labels = {
                        "gender_axis": "–ì–µ–Ω–¥–µ—Ä–Ω–∞—è –æ—Å—å",
                        "profession_axis": "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –æ—Å—å",
                        "evaluation_axis": "–û—Ü–µ–Ω–æ—á–Ω–∞—è –æ—Å—å",
                        "temporal_axis": "–í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å"
                    }

                    summary_rows = []
                    for axis_key, axis_data in axes_result.items():
                        summary_rows.append({
                            "–û—Å—å": axis_labels.get(axis_key, axis_key),
                            "–°–∏–ª–∞ –æ—Å–∏": float(axis_data.get('axis_strength', 0.0)),
                            "–°–º–µ—â–µ–Ω–∏–µ": float(axis_data.get('bias_metric', 0.0))
                        })

                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üìà –°–≤–æ–¥–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
                        if summary_rows:
                            summary_df = pd.DataFrame(summary_rows)
                            summary_df["–°–∏–ª–∞ –æ—Å–∏"] = summary_df["–°–∏–ª–∞ –æ—Å–∏"].map(lambda x: f"{x:.3f}")
                            summary_df["–°–º–µ—â–µ–Ω–∏–µ"] = summary_df["–°–º–µ—â–µ–Ω–∏–µ"].map(lambda x: f"{x:.3f}")
                            st.dataframe(summary_df, use_container_width=True)
                        else:
                            st.info("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.")
                        st.markdown("</div>", unsafe_allow_html=True)

                    custom_words = [w.strip() for w in custom_words_input.split(',') if w.strip()]

                    combined_custom_df = []
                    missing_overall = set()

                    for axis_key, axis_data in axes_result.items():
                        axis_name = axis_labels.get(axis_key, axis_key)
                        positive_df = pd.DataFrame(axis_data.get('positive_end') or [], columns=["–°–ª–æ–≤–æ", "–ü—Ä–æ–µ–∫—Ü–∏—è"])
                        negative_df = pd.DataFrame(axis_data.get('negative_end') or [], columns=["–°–ª–æ–≤–æ", "–ü—Ä–æ–µ–∫—Ü–∏—è"])
                        full_df = pd.DataFrame(axis_data.get('all_projections') or [], columns=["–°–ª–æ–≤–æ", "–ü—Ä–æ–µ–∫—Ü–∏—è"])

                        display_df = full_df if not full_df.empty else pd.concat([positive_df, negative_df], ignore_index=True)

                        if not display_df.empty:
                            positive_display = display_df.sort_values("–ü—Ä–æ–µ–∫—Ü–∏—è", ascending=False).head(int(axes_topn))
                            negative_display = display_df.sort_values("–ü—Ä–æ–µ–∫—Ü–∏—è", ascending=True).head(int(axes_topn))
                        else:
                            positive_display = positive_df.sort_values("–ü—Ä–æ–µ–∫—Ü–∏—è", ascending=False).head(int(axes_topn))
                            negative_display = negative_df.sort_values("–ü—Ä–æ–µ–∫—Ü–∏—è", ascending=True).head(int(axes_topn))

                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown(f"#### üß≠ {axis_name}")

                            metric_cols = st.columns(2)
                            metric_cols[0].metric("–°–∏–ª–∞ –æ—Å–∏", f"{axis_data.get('axis_strength', 0.0):.3f}")
                            metric_cols[1].metric("–°–º–µ—â–µ–Ω–∏–µ", f"{axis_data.get('bias_metric', 0.0):.3f}")

                            axis_combined = pd.concat([
                                positive_display.assign(–ü–æ–ª—é—Å="–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π"),
                                negative_display.assign(–ü–æ–ª—é—Å="–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π")
                            ], ignore_index=True)

                            if not axis_combined.empty:
                                axis_combined["–ü—Ä–æ–µ–∫—Ü–∏—è"] = axis_combined["–ü—Ä–æ–µ–∫—Ü–∏—è"].astype(float)
                                axis_combined.sort_values("–ü—Ä–æ–µ–∫—Ü–∏—è", ascending=True, inplace=True)
                                max_abs_axis = float(max(abs(axis_combined["–ü—Ä–æ–µ–∫—Ü–∏—è"].min()), abs(axis_combined["–ü—Ä–æ–µ–∫—Ü–∏—è"].max()), 1e-6))
                                chart_height_axis = max(160, 28 * len(axis_combined))

                                axis_chart = alt.Chart(axis_combined).mark_bar().encode(
                                    y=alt.Y('–°–ª–æ–≤–æ:N', sort=None, title=''),
                                    x=alt.X(
                                        '–ü—Ä–æ–µ–∫—Ü–∏—è:Q',
                                        title='–ü—Ä–æ–µ–∫—Ü–∏—è –≤–¥–æ–ª—å –æ—Å–∏',
                                        scale=alt.Scale(domain=[-max_abs_axis, max_abs_axis], zero=True, nice=False)
                                    ),
                                    color=alt.Color('–ü–æ–ª—é—Å:N', scale=alt.Scale(range=['#ff6b6b', '#4dabf7'])),
                                    tooltip=[
                                        alt.Tooltip('–°–ª–æ–≤–æ:N', title='–°–ª–æ–≤–æ'),
                                        alt.Tooltip('–ü–æ–ª—é—Å:N', title='–ü–æ–ª—é—Å'),
                                        alt.Tooltip('–ü—Ä–æ–µ–∫—Ü–∏—è:Q', format='.3f', title='–ü—Ä–æ–µ–∫—Ü–∏—è')
                                    ]
                                ).properties(height=chart_height_axis, width='container')

                                axis_zero_line = alt.Chart(pd.DataFrame({'x': [0]})).mark_rule(color='#6b7a99', strokeDash=[4, 4]).encode(x='x:Q')
                                st.altair_chart(axis_chart + axis_zero_line, use_container_width=True)
                            else:
                                st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å–∏.")

                        if custom_words:
                            projections, missing_words = semantic_ops.project_words_on_axis(
                                selected_model_name,
                                axis_data.get('axis_direction'),
                                custom_words
                            )
                            if projections:
                                custom_df = pd.DataFrame(projections, columns=["–°–ª–æ–≤–æ", "–ü—Ä–æ–µ–∫—Ü–∏—è"])
                                custom_df["–ü—Ä–æ–µ–∫—Ü–∏—è"] = custom_df["–ü—Ä–æ–µ–∫—Ü–∏—è"].astype(float)
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                custom_df_sorted = custom_df.copy()
                                custom_df_sorted.sort_values("–ü—Ä–æ–µ–∫—Ü–∏—è", ascending=True, inplace=True)
                                custom_df_sorted["–ü–æ–ª—é—Å"] = np.where(custom_df_sorted["–ü—Ä–æ–µ–∫—Ü–∏—è"] >= 0, "–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π", "–û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π")
                                
                                # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ –ø–æ—Ä—è–¥–∫–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–º)
                                word_order = custom_df_sorted["–°–ª–æ–≤–æ"].tolist()
                                
                                max_abs_custom = float(max(abs(custom_df_sorted["–ü—Ä–æ–µ–∫—Ü–∏—è"].min()), abs(custom_df_sorted["–ü—Ä–æ–µ–∫—Ü–∏—è"].max()), 1e-6))
                                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤—ã—Å–æ—Ç—É –≥—Ä–∞—Ñ–∏–∫–∞, —á—Ç–æ–±—ã –≤—Å–µ —Å–ª–æ–≤–∞ –±—ã–ª–∏ –≤–∏–¥–Ω—ã
                                chart_height_custom = max(200, 35 * len(custom_df_sorted))
                                
                                st.markdown(f"**–í–∞—à–∏ —Å–ª–æ–≤–∞** (–æ—Ç–æ–±—Ä–∞–∂–µ–Ω–æ {len(custom_df_sorted)} –∏–∑ {len(custom_words)} —Å–ª–æ–≤)")

                                custom_chart = alt.Chart(custom_df_sorted).mark_bar().encode(
                                    y=alt.Y('–°–ª–æ–≤–æ:N', sort=word_order, title=''),
                                    x=alt.X(
                                        '–ü—Ä–æ–µ–∫—Ü–∏—è:Q',
                                        title='–ü—Ä–æ–µ–∫—Ü–∏—è –≤–¥–æ–ª—å –æ—Å–∏',
                                        scale=alt.Scale(domain=[-max_abs_custom, max_abs_custom], zero=True, nice=False)
                                    ),
                                    color=alt.Color('–ü–æ–ª—é—Å:N', scale=alt.Scale(range=['#ff6b6b', '#4dabf7'])),
                                    tooltip=[
                                        alt.Tooltip('–°–ª–æ–≤–æ:N', title='–°–ª–æ–≤–æ'),
                                        alt.Tooltip('–ü—Ä–æ–µ–∫—Ü–∏—è:Q', format='.3f', title='–ü—Ä–æ–µ–∫—Ü–∏—è')
                                    ]
                                ).properties(
                                    height=min(chart_height_custom, 800),  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤—ã—Å–æ—Ç—É –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                    width='container'
                                )

                                custom_zero_line = alt.Chart(pd.DataFrame({'x': [0]})).mark_rule(color='#6b7a99', strokeDash=[4, 4]).encode(x='x:Q')
                                st.altair_chart(custom_chart + custom_zero_line, use_container_width=True)
                                
                                # –¢–∞–∫–∂–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –≤—Å–µ—Ö —Å–ª–æ–≤
                                with st.expander(f"üìã –¢–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö —Å–ª–æ–≤ –¥–ª—è –æ—Å–∏ {axis_name}", expanded=False):
                                    display_df = custom_df_sorted[["–°–ª–æ–≤–æ", "–ü—Ä–æ–µ–∫—Ü–∏—è", "–ü–æ–ª—é—Å"]].copy()
                                    st.dataframe(display_df, use_container_width=True)

                                custom_df_sorted["–û—Å—å"] = axis_name
                                combined_custom_df.append(custom_df_sorted)
                            elif missing_words and len(missing_words) == len(custom_words):
                                st.info(f"–í—Å–µ –≤–≤–µ–¥–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Å–∏ {axis_name}.")
                            if missing_words:
                                missing_overall.update(missing_words)

                        st.markdown("</div>", unsafe_allow_html=True)

                    if combined_custom_df:
                        merged_df = pd.concat(combined_custom_df, ignore_index=True)
                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown("#### üó∫Ô∏è –ü—Ä–æ–µ–∫—Ü–∏–∏ –≤–∞—à–∏—Ö —Å–ª–æ–≤")
                            st.dataframe(merged_df, use_container_width=True)
                            st.markdown("</div>", unsafe_allow_html=True)

                    if missing_overall:
                        st.warning(
                            "–°–ª–µ–¥—É—é—â–∏–µ —Å–ª–æ–≤–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏: " + ", ".join(sorted(missing_overall))
                        )

        with tab_neighbors:
            st.subheader("6.4. –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π")

            selected_model_name = st.session_state.get('semantic_model_select')
            if not selected_model_name:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.1, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–æ—Å–µ–¥–µ–π.")
            else:
                st.session_state.setdefault('semantic_neighbors_cache', {})
                st.session_state.setdefault('semantic_neighbors_words', "–∫–æ–º–ø—å—é—Ç–µ—Ä, –ø—Ä–æ–≥—Ä–∞–º–º–∞, –¥–∞–Ω–Ω—ã–µ, –≥–æ—Ä–æ–¥, —Ö–æ—Ä–æ—à–∏–π, —Ä–∞–±–æ—Ç–∞, –≤—Ä–µ–º—è, –∂–µ–Ω—â–∏–Ω–∞, –º—É–∂—á–∏–Ω–∞, —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è, —Å–∏—Å—Ç–µ–º–∞")
                st.session_state.setdefault('semantic_neighbors_topk', 10)

                with st.container():
                    st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                    st.markdown("#### üîç –û–ø–∏—Å–∞–Ω–∏–µ")
                    st.markdown(
                        """
                        –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-10 –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤, –æ—Ü–µ–Ω–∏–≤–∞–µ–º –∏—Ö —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é 
                        —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –∏ —Ñ–∏–∫—Å–∏—Ä—É–µ–º —Å–ª—É—á–∞–∏ —Å–º–µ—à–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏–∫–∏ –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞. 
                        """
                    )
                    st.markdown("</div>", unsafe_allow_html=True)

                topk_value = st.slider(
                    "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π (Top-N)",
                    min_value=5,
                    max_value=20,
                    value=int(st.session_state.get('semantic_neighbors_topk', 10)),
                    key="semantic_neighbors_topk"
                )

                neighbors_words_input = st.text_area(
                    "–°–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)",
                    key="semantic_neighbors_words"
                )

                analyze_neighbors_btn = st.button("üîé –í—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–æ—Å–µ–¥–µ–π", key="compute_neighbors_analysis")

                cache_key = (selected_model_name, topk_value, tuple(sorted([w.strip() for w in neighbors_words_input.split(',') if w.strip()])))

                if analyze_neighbors_btn or cache_key not in st.session_state.semantic_neighbors_cache:
                    test_words = [w.strip() for w in neighbors_words_input.split(',') if w.strip()]
                    if not test_words:
                        st.warning("–£–∫–∞–∂–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ —Å–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Å–µ–¥–µ–π.")
                        neighbors_result = None
                    else:
                        with st.spinner("–°–æ–±–∏—Ä–∞–µ–º –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π..."):
                            neighbors_result = semantic_ops.nearest_neighbors_analysis(
                                selected_model_name,
                                test_words,
                                top_k=int(topk_value)
                            )
                        st.session_state.semantic_neighbors_cache[cache_key] = neighbors_result
                else:
                    neighbors_result = st.session_state.semantic_neighbors_cache.get(cache_key)

                if not neighbors_result:
                    st.info("–ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –≤—ã—à–µ, —á—Ç–æ–±—ã –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π.")
                else:
                    overall_analysis = neighbors_result.get('overall_analysis', {})

                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("#### üìà –°–≤–æ–¥–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏")
                        col_n1, col_n2, col_n3 = st.columns(3)
                        col_n1.metric("–°—Ä–µ–¥–Ω—è—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", f"{overall_analysis.get('mean_semantic_coherence', 0):.3f}")
                        col_n2.metric("–°—Ç. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ", f"{overall_analysis.get('semantic_coherence_std', 0):.3f}")
                        col_n3.metric("–°–ª–æ–≤ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ", overall_analysis.get('total_words_analyzed', 0))

                        neighbor_category_analysis = overall_analysis.get('neighbor_category_analysis', {})
                        if neighbor_category_analysis:
                            cat_rows = []
                            for category, values in neighbor_category_analysis.items():
                                cat_rows.append({
                                    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": category,
                                    "–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ": f"{values.get('mean_count', 0):.2f}",
                                    "–í—Å–µ–≥–æ": values.get('total_occurrences', 0)
                                })
                            cat_df = pd.DataFrame(cat_rows)
                            st.dataframe(cat_df, use_container_width=True)
                        else:
                            st.info("–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ—Å–µ–¥–µ–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω—ã.")
                        st.markdown("</div>", unsafe_allow_html=True)

                    def classify_neighbor(target_word: str, neighbor_word: str, similarity: float) -> str:
                        if (neighbor_word in target_word or target_word in neighbor_word or
                                len(set(neighbor_word) & set(target_word)) > 3):
                            return "–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ"
                        if semantic_ops._check_syntactic_relation(target_word, neighbor_word):
                            return "–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ"
                        if similarity > 0.6:
                            return "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ"
                        if semantic_ops._check_thematic_relation(target_word, neighbor_word):
                            return "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ"
                        return "–ü—Ä–æ—á–∏–µ"

                    color_scale_neighbor = alt.Scale(domain=["–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ", "–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ", "–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ", "–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ", "–ü—Ä–æ—á–∏–µ"],
                                                     range=['#4dabf7', '#ffa94d', '#845ef7', '#51cf66', '#adb5bd'])

                    word_results = neighbors_result.get('word_analysis', {})
                    input_order = [w.strip() for w in neighbors_words_input.split(',') if w.strip()]

                    for word in input_order:
                        word_data = word_results.get(word)
                        if not word_data:
                            continue

                        with st.container():
                            st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                            st.markdown(f"#### üî† {word}")

                            status = word_data.get('status', 'success')
                            if status != 'success':
                                st.warning(f"{word_data.get('status', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}")
                            else:
                                neighbors = word_data.get('neighbors', []) or []
                                semantic_coherence = word_data.get('semantic_coherence', 0)
                                avg_similarity = word_data.get('average_similarity', 0)
                                neighbor_types_counts = word_data.get('neighbor_types', {})

                                metric_cols = st.columns(2)
                                metric_cols[0].metric("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å", f"{semantic_coherence:.3f}")
                                metric_cols[1].metric("–°—Ä–µ–¥–Ω—è—è –ø–æ—Ö–æ–∂–µ—Å—Ç—å", f"{avg_similarity:.3f}")

                                neighbor_rows = []
                                for neighbor_word, similarity in neighbors:
                                    category = classify_neighbor(word, neighbor_word, similarity)
                                    neighbor_rows.append({
                                        "–°–æ—Å–µ–¥": neighbor_word,
                                        "–°—Ö–æ–¥—Å—Ç–≤–æ": float(similarity),
                                        "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": category
                                    })
                                neighbors_df = pd.DataFrame(neighbor_rows)

                                if not neighbors_df.empty:
                                    neighbors_df.sort_values("–°—Ö–æ–¥—Å—Ç–≤–æ", ascending=True, inplace=True)
                                    neighbor_order = neighbors_df["–°–æ—Å–µ–¥"].tolist()
                                    chart_height_neighbors = max(280, 34 * len(neighbors_df))

                                    neighbors_chart = alt.Chart(neighbors_df).mark_bar().encode(
                                        y=alt.Y('–°–æ—Å–µ–¥:N', sort=neighbor_order, title=''),
                                        x=alt.X('–°—Ö–æ–¥—Å—Ç–≤–æ:Q', title='–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ', scale=alt.Scale(domain=[0, 1])),
                                        color=alt.Color('–ö–∞—Ç–µ–≥–æ—Ä–∏—è:N', scale=color_scale_neighbor),
                                        tooltip=[
                                            alt.Tooltip('–°–æ—Å–µ–¥:N', title='–°–æ—Å–µ–¥'),
                                            alt.Tooltip('–ö–∞—Ç–µ–≥–æ—Ä–∏—è:N', title='–ö–∞—Ç–µ–≥–æ—Ä–∏—è'),
                                            alt.Tooltip('–°—Ö–æ–¥—Å—Ç–≤–æ:Q', format='.3f', title='–°—Ö–æ–¥—Å—Ç–≤–æ')
                                        ]
                                    ).properties(height=chart_height_neighbors, width='container')

                                    st.altair_chart(neighbors_chart, use_container_width=True)
                                else:
                                    st.info("–°–æ—Å–µ–¥–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")

                                if neighbor_types_counts:
                                    type_rows = [
                                        {"–ö–∞—Ç–µ–≥–æ—Ä–∏—è": label.capitalize(), "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ": count}
                                        for label, count in neighbor_types_counts.items()
                                    ]
                                    type_df = pd.DataFrame(type_rows)
                                    st.dataframe(type_df, use_container_width=True)

                                    semantic_mix = neighbor_types_counts.get('semantic_synonyms', 0)
                                    syntactic_mix = neighbor_types_counts.get('syntactic_related', 0)
                                    morph_mix = neighbor_types_counts.get('morphological_variants', 0)
                                    thematic_mix = neighbor_types_counts.get('thematic_related', 0)
                                    total_neighbors = max(len(neighbors), 1)

                                    notes = []
                                    if semantic_mix < total_neighbors * 0.4 and (syntactic_mix or morph_mix):
                                        notes.append("–ï—Å—Ç—å —Å–º–µ—à–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏—Ö —Å–æ—Å–µ–¥–µ–π")
                                    if thematic_mix > 0 and semantic_mix < total_neighbors * 0.5:
                                        notes.append("–ü—Ä–µ–æ–±–ª–∞–¥–∞—é—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏")

                                    if notes:
                                        st.warning("; ".join(notes))
                                else:
                                    st.info("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å–æ—Å–µ–¥–µ–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ —É–¥–∞–ª–æ—Å—å.")

                            st.markdown("</div>", unsafe_allow_html=True)

        with tab_report:
            st.subheader("6.5. –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç")

            selected_model_name = st.session_state.get('semantic_model_select')
            if not selected_model_name:
                st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.1, —á—Ç–æ–±—ã —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç—á—ë—Ç.")
            else:
                model_ref = available_models.get(selected_model_name)
                if model_ref is None:
                    st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ —ç—Ç–∞–ø–µ 5.")
                else:
                    st.markdown(
                        """
                        –û—Ç—á—ë—Ç –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤: 
                        –≤–µ–∫—Ç–æ—Ä–Ω—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É, –∞–Ω–∞–ª–æ–≥–∏, —Ç–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã –±–ª–∏–∑–æ—Å—Ç–µ–π –∏ 
                        –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–µ–∫—Ü–∏–π.
                        """
                    )

                    # ---------------- Vector arithmetic summary ----------------
                    manual_result = st.session_state.get('semantic_manual_analogy_result')
                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("### üî¢ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∞")

                        if manual_result and not manual_result.get('error'):
                            words_tuple = manual_result.get('words', ("", "", ""))
                            expression = f"{words_tuple[0]} ‚àí {words_tuple[1]} + {words_tuple[2]}"
                            st.markdown(f"**–í—ã—Ä–∞–∂–µ–Ω–∏–µ:** `{expression}`")

                            results_list = manual_result.get('results', [])
                            if results_list:
                                arithmetic_df = pd.DataFrame(results_list, columns=["–°–ª–æ–≤–æ", "–°—Ö–æ–¥—Å—Ç–≤–æ"])
                                arithmetic_df.sort_values("–°—Ö–æ–¥—Å—Ç–≤–æ", ascending=True, inplace=True)
                                chart_height_arith = max(180, 26 * len(arithmetic_df))
                                arith_chart = alt.Chart(arithmetic_df).mark_bar(color='#4dabf7').encode(
                                    y=alt.Y('–°–ª–æ–≤–æ:N', sort=None, title=''),
                                    x=alt.X('–°—Ö–æ–¥—Å—Ç–≤–æ:Q', title='–ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ', scale=alt.Scale(domain=[0, 1])),
                                    tooltip=[
                                        alt.Tooltip('–°–ª–æ–≤–æ:N', title='–°–ª–æ–≤–æ'),
                                        alt.Tooltip('–°—Ö–æ–¥—Å—Ç–≤–æ:Q', format='.3f', title='–°—Ö–æ–¥—Å—Ç–≤–æ')
                                    ]
                                ).properties(height=chart_height_arith, width='container')
                                st.altair_chart(arith_chart, use_container_width=True)
                            else:
                                st.info("–†–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                        elif manual_result and manual_result.get('error'):
                            st.warning(manual_result.get('error'))
                        else:
                            st.info("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫–∏ ‚Äî –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Ä–∞—Å—á—ë—Ç –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.2.")
                        st.markdown("</div>", unsafe_allow_html=True)

                    # ---------------- Analogy statistics ----------------
                    category_result = st.session_state.get('semantic_category_analogy_result')
                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("### üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–Ω–∞–ª–æ–≥–∏—è–º")

                        if category_result:
                            label_map = {
                                "semantic_capitals": "–°—Ç–æ–ª–∏—Ü—ã —Å—Ç—Ä–∞–Ω",
                                "semantic_gender": "–†–æ–¥–æ–≤—ã–µ –ø–∞—Ä—ã",
                                "syntactic_comparative": "–°—Ç–µ–ø–µ–Ω–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è",
                                "morphological_verbs": "–ì–ª–∞–≥–æ–ª—ã –ø—Ä–æ—à–µ–¥—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏"
                            }
                            summary_rows = []
                            for key, value in category_result.items():
                                if key in ("overall_accuracy", "total_tests", "total_correct"):
                                    continue
                                total = value.get('total', 0)
                                accuracy = value.get('accuracy', 0.0)
                                summary_rows.append({
                                    "–ö–∞—Ç–µ–≥–æ—Ä–∏—è": label_map.get(key, key),
                                    "–¢–æ—á–Ω–æ—Å—Ç—å": accuracy * 100,
                                    "–¢–µ—Å—Ç–æ–≤": total
                                })

                            if summary_rows:
                                analogy_df = pd.DataFrame(summary_rows)
                                analogy_chart = alt.Chart(analogy_df).mark_bar().encode(
                                    x=alt.X('–¢–æ—á–Ω–æ—Å—Ç—å:Q', title='–¢–æ—á–Ω–æ—Å—Ç—å (%)', scale=alt.Scale(domain=[0, 100])),
                                    y=alt.Y('–ö–∞—Ç–µ–≥–æ—Ä–∏—è:N', sort='-x', title=''),
                                    color=alt.Color('–ö–∞—Ç–µ–≥–æ—Ä–∏—è:N', legend=None),
                                    tooltip=[
                                        alt.Tooltip('–ö–∞—Ç–µ–≥–æ—Ä–∏—è:N', title='–ö–∞—Ç–µ–≥–æ—Ä–∏—è'),
                                        alt.Tooltip('–¢–æ—á–Ω–æ—Å—Ç—å:Q', format='.1f', title='–¢–æ—á–Ω–æ—Å—Ç—å (%)'),
                                        alt.Tooltip('–¢–µ—Å—Ç–æ–≤:Q', title='–ö–æ–ª-–≤–æ —Ç–µ—Å—Ç–æ–≤')
                                    ]
                                ).properties(height=200, width='container')
                                st.altair_chart(analogy_chart, use_container_width=True)
                            else:
                                st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∞–Ω–∞–ª–æ–≥–∏—è–º ‚Äî –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.2.")
                        else:
                            st.info("–ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ—Ü–µ–Ω–∫—É –∞–Ω–∞–ª–æ–≥–∏–π –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.2, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É.")
                        st.markdown("</div>", unsafe_allow_html=True)

                    # ---------------- Heatmap of semantic similarities ----------------
                    st.session_state.setdefault('semantic_distance_cache', {})
                    distance_report = st.session_state.semantic_distance_cache.get(selected_model_name)
                    if distance_report is None:
                        with st.spinner("–ì–æ—Ç–æ–≤–∏–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤..."):
                            distance_report = semantic_ops.analyze_distance_distribution(selected_model_name)
                        st.session_state.semantic_distance_cache[selected_model_name] = distance_report

                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("### üî• –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å—Ö–æ–¥—Å—Ç–≤")

                        heatmap_words_count = st.slider(
                            "–°–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å –≤ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç–µ",
                            min_value=5,
                            max_value=40,
                            value=20,
                            key="semantic_heatmap_wordcount"
                        )

                        if distance_report and distance_report.get('similarity_matrix'):
                            sample_words = distance_report.get('sample_words', [])
                            similarity_matrix = np.array(distance_report.get('similarity_matrix'))

                            if len(sample_words) >= heatmap_words_count:
                                selected_indices = np.arange(heatmap_words_count)
                                selected_words = [sample_words[i] for i in selected_indices]
                                matrix_subset = similarity_matrix[np.ix_(selected_indices, selected_indices)]

                                heatmap_df = pd.DataFrame(matrix_subset, index=selected_words, columns=selected_words)
                                heatmap_long = heatmap_df.reset_index().melt(id_vars='index', var_name='–°–ª–æ–≤–æ2', value_name='–°—Ö–æ–¥—Å—Ç–≤–æ')
                                heatmap_long.rename(columns={'index': '–°–ª–æ–≤–æ1'}, inplace=True)

                                heatmap_chart = alt.Chart(heatmap_long).mark_rect().encode(
                                    x=alt.X('–°–ª–æ–≤–æ2:N', title='', sort=selected_words),
                                    y=alt.Y('–°–ª–æ–≤–æ1:N', title='', sort=selected_words),
                                    color=alt.Color('–°—Ö–æ–¥—Å—Ç–≤–æ:Q', scale=alt.Scale(scheme='blues'), title='–°—Ö–æ–¥—Å—Ç–≤–æ'),
                                    tooltip=[
                                        alt.Tooltip('–°–ª–æ–≤–æ1:N', title='–°–ª–æ–≤–æ 1'),
                                        alt.Tooltip('–°–ª–æ–≤–æ2:N', title='–°–ª–æ–≤–æ 2'),
                                        alt.Tooltip('–°—Ö–æ–¥—Å—Ç–≤–æ:Q', format='.3f', title='–°—Ö–æ–¥—Å—Ç–≤–æ')
                                    ]
                                ).properties(width='container', height=400)
                                st.altair_chart(heatmap_chart, use_container_width=True)
                            else:
                                st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã ‚Äî –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Å—á–∏—Ç–∞—Ç—å –º–∞—Ç—Ä–∏—Ü—É –Ω–∞ –≤–∫–ª–∞–¥–∫–µ 6.1.")
                        else:
                            st.info("–ú–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–¥—Å—Ç–≤ –Ω–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å.")
                        st.markdown("</div>", unsafe_allow_html=True)

                    # ---------------- 2D/3D projections ----------------
                    with st.container():
                        st.markdown('<div class="semantic-card">', unsafe_allow_html=True)
                        st.markdown("### üó∫Ô∏è –ü—Ä–æ–µ–∫—Ü–∏—è –≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ")

                        projection_mode = st.radio(
                            "–†–µ–∂–∏–º –ø—Ä–æ–µ–∫—Ü–∏–∏",
                            options=["2D", "3D"],
                            horizontal=True,
                            key="semantic_projection_mode"
                        )
                        projection_sample = st.slider(
                            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏",
                            min_value=30,
                            max_value=200,
                            value=80,
                            key="semantic_projection_sample"
                        )
                        cluster_count = st.slider(
                            "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (0 = –±–µ–∑ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏)",
                            min_value=0,
                            max_value=10,
                            value=4,
                            key="semantic_projection_clusters"
                        )

                        all_words = list(model_ref.wv.key_to_index.keys())
                        if len(all_words) < 10:
                            st.warning("–í —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–ª–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ü–∏–∏.")
                        else:
                            rng = np.random.default_rng(42)
                            sample_size = min(projection_sample, len(all_words))
                            sampled_words = rng.choice(all_words, size=sample_size, replace=False)
                            vectors = model_ref.wv[sampled_words]

                            if projection_mode == "2D":
                                reducer = PCA(n_components=2)
                                coords = reducer.fit_transform(vectors)
                                coord_df = pd.DataFrame(coords, columns=['x', 'y'])
                            else:
                                reducer = PCA(n_components=3)
                                coords = reducer.fit_transform(vectors)
                                coord_df = pd.DataFrame(coords, columns=['x', 'y', 'z'])

                            coord_df['–°–ª–æ–≤–æ'] = sampled_words

                            if cluster_count and cluster_count > 1:
                                kmeans = KMeans(n_clusters=cluster_count, random_state=42, n_init=10)
                                clusters = kmeans.fit_predict(coords)
                                coord_df['–ö–ª–∞—Å—Ç–µ—Ä'] = clusters.astype(str)
                            else:
                                coord_df['–ö–ª–∞—Å—Ç–µ—Ä'] = '–í—Å–µ'

                            if projection_mode == "2D":
                                proj_chart = alt.Chart(coord_df).mark_circle(size=80).encode(
                                    x=alt.X('x:Q', title='–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 1'),
                                    y=alt.Y('y:Q', title='–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 2'),
                                    color=alt.Color('–ö–ª–∞—Å—Ç–µ—Ä:N', legend=alt.Legend(title='–ö–ª–∞—Å—Ç–µ—Ä—ã')),
                                    tooltip=[
                                        alt.Tooltip('–°–ª–æ–≤–æ:N', title='–°–ª–æ–≤–æ'),
                                        alt.Tooltip('–ö–ª–∞—Å—Ç–µ—Ä:N', title='–ö–ª–∞—Å—Ç–µ—Ä'),
                                        alt.Tooltip('x:Q', format='.3f', title='–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 1'),
                                        alt.Tooltip('y:Q', format='.3f', title='–ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∞ 2')
                                    ]
                                ).properties(height=500, width='container')
                                st.altair_chart(proj_chart, use_container_width=True)
                            else:
                                fig_3d = px.scatter_3d(
                                    coord_df,
                                    x='x', y='y', z='z',
                                    color='–ö–ª–∞—Å—Ç–µ—Ä',
                                    hover_name='–°–ª–æ–≤–æ',
                                    title="3D –ø—Ä–æ–µ–∫—Ü–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞"
                                )
                                fig_3d.update_traces(marker=dict(size=5))
                                st.plotly_chart(fig_3d, use_container_width=True)

                        st.markdown("</div>", unsafe_allow_html=True)

