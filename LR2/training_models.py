# training_models.py (–£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø –¢–û–õ–¨–ö–û –° –†–ê–°–®–ò–†–ï–ù–ù–´–ú–ò –ù–ê–°–¢–†–û–ô–ö–ê–ú–ò)
import time
import numpy as np
import pandas as pd
from gensim.models import Word2Vec, FastText, Doc2Vec
from gensim.models.doc2vec import TaggedDocument
import multiprocessing
import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
import psutil
import gc
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

class DistributedRepresentations:
    def __init__(self):
        self.word_models = {}
        self.doc_models = {}
        self.evaluation_results = {}
        self.training_history = []
        self.logger = self._setup_logger()
        
    def _setup_logger(self):
        logger = logging.getLogger('model_training')
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        return logger

    def train_with_parameters(self, texts: List[List[str]], categories: List[str] = None, 
                            model_types: List[str] = None, vector_size: int = 100,
                            window: int = 8, min_count: int = 2, epochs: int = 100,
                            sg: int = 1, workers: Optional[int] = None, hs: int = 0, 
                            negative: int = 10, sample: float = 1e-5,
                            compute_loss: bool = False, max_epochs: Optional[int] = 150):
        """
        –û–ë–£–ß–ï–ù–ò–ï –° –í–´–ë–û–†–û–ú –ü–ê–†–ê–ú–ï–¢–†–û–í –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
        """
        valid_texts, total_words, unique_words = self.validate_corpus(texts)

        if workers is None:
            workers = max(1, multiprocessing.cpu_count() - 1)
            self.logger.info(f"üßµ –ê–≤—Ç–æ–≤—ã–±–æ—Ä –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ—Ç–æ–∫–æ–≤: {workers}")
        elif workers < 1:
            self.logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ workers={workers}, –∏—Å–ø–æ–ª—å–∑—É–µ–º 1")
            workers = 1
        else:
            cpu_total = multiprocessing.cpu_count()
            if workers > cpu_total:
                self.logger.info(f"üßµ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ workers: {workers} ‚Üí {cpu_total} (–º–∞–∫—Å–∏–º—É–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —è–¥–µ—Ä)")
                workers = cpu_total

        if max_epochs is not None:
            max_epochs = max(max_epochs, epochs)
        
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –ê–î–ê–ü–¢–ê–¶–ò–Ø –ü–ê–†–ê–ú–ï–¢–†–û–í –î–õ–Ø –ú–ê–õ–ï–ù–¨–ö–û–ì–û –ö–û–†–ü–£–°–ê
        corpus_size = len(valid_texts)
        is_small_corpus = corpus_size < 1000 or total_words < 50000
        
        if is_small_corpus:
            self.logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω –º–∞–ª–µ–Ω—å–∫–∏–π –∫–æ—Ä–ø—É—Å ({corpus_size} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {total_words} —Å–ª–æ–≤)")
            self.logger.info("üîß –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞...")
            
            # –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞:
            # - min_count –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 1, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞
            if min_count > 1:
                original_min_count = min_count
                min_count = 1
                self.logger.info(f"  ‚Üí min_count: {original_min_count} ‚Üí {min_count} (—Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–¥–∫–∏–µ —Å–ª–æ–≤–∞)")
            
            # - window –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–Ω—å—à–µ (3-5 –≤–º–µ—Å—Ç–æ 8)
            # –î–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—â–µ –º–µ–Ω—å—à–µ–µ –æ–∫–Ω–æ
            if corpus_size < 600:
                target_window = 3
            else:
                target_window = 5
            
            if window > target_window:
                original_window = window
                window = target_window
                self.logger.info(f"  ‚Üí window: {original_window} ‚Üí {window} (–º–µ–Ω—å—à–µ –æ–∫–Ω–æ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞)")
            
            # - sample –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –¥–ª—è downsampling —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤
            # –î–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–º–µ—Ä–µ–Ω–Ω—ã–π sample
            if corpus_size < 600:
                target_sample = 1e-3  # 0.001 –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ (–º–µ–Ω—å—à–µ –¥–∞—É–Ω—Å–µ–º–ø–ª–∏–Ω–≥)
            else:
                target_sample = 1e-4  # 0.0001 –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
            
            if sample < target_sample or sample > 1e-2:
                original_sample = sample
                sample = target_sample
                self.logger.info(f"  ‚Üí sample: {original_sample} ‚Üí {sample} (–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π downsampling –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞)")
            
            # - epochs –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            # –î–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –Ω—É–∂–Ω–æ –µ—â–µ –±–æ–ª—å—à–µ —ç–ø–æ—Ö
            if corpus_size < 600:
                target_epochs = 300
            else:
                target_epochs = 200
            
            if epochs < target_epochs:
                adjusted_epochs = max(target_epochs, epochs * 2)
            else:
                adjusted_epochs = epochs

            if max_epochs is not None and adjusted_epochs > max_epochs:
                self.logger.info(f"  ‚Üí epochs: {adjusted_epochs} ‚Üí {max_epochs} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º–∞ —ç–ø–æ—Ö)")
                adjusted_epochs = max_epochs

            if epochs != adjusted_epochs:
                self.logger.info(f"  ‚Üí epochs: {epochs} ‚Üí {adjusted_epochs} (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏)")
            epochs = adjusted_epochs
            
            # - –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º negative sampling –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
            # –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–µ negative samples (5-10 –≤–º–µ—Å—Ç–æ 10-25)
            if corpus_size < 600 and hs == 0:
                if negative > 10:
                    original_negative = negative
                    negative = 10  # –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
                    self.logger.info(f"  ‚Üí negative: {original_negative} ‚Üí {negative} (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞)")
                elif negative < 5:
                    original_negative = negative
                    negative = 5  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                    self.logger.info(f"  ‚Üí negative: {original_negative} ‚Üí {negative} (—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –º–∏–Ω–∏–º—É–º)")
            
            # - –î–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å hierarchical softmax
            # –ù–æ negative sampling —Ç–æ–∂–µ —Ö–æ—Ä–æ—à–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, –ø–æ—ç—Ç–æ–º—É –æ—Å—Ç–∞–≤–ª—è–µ–º –≤—ã–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∫–æ—Ä–ø—É—Å –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∏–π (<300 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
            if corpus_size < 300 and hs == 0 and negative > 5:
                original_hs = hs
                original_negative = negative
                hs = 1
                negative = 0  # hs=1 –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º —Å negative sampling
                self.logger.info(f"  ‚Üí hs: {original_hs} ‚Üí {hs}, negative: {original_negative} ‚Üí {negative} (hierarchical softmax –¥–ª—è –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞)")
            
            # - –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞
            # –ù–µ –∏–∑–º–µ–Ω—è–µ–º vector_size –µ—Å–ª–∏ –æ–Ω –±—ã–ª —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω –∫–∞–∫ 50 (–ø–æ–ø—É–ª—è—Ä–Ω—ã–π –≤—ã–±–æ—Ä)
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è (<50)
            if corpus_size < 600:
                if vector_size < 50:
                    # –ï—Å–ª–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è (<50), —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ –º–∏–Ω–∏–º—É–º–∞ 50
                    original_vector_size = vector_size
                    vector_size = 50
                    self.logger.info(f"  ‚Üí vector_size: {original_vector_size} ‚Üí {vector_size} (—É–≤–µ–ª–∏—á–µ–Ω–∞ –¥–æ –º–∏–Ω–∏–º—É–º–∞)")
                # –î–ª—è vector_size >= 50 –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —è–≤–Ω–æ —É–∫–∞–∑–∞–ª)
            elif vector_size > 200:
                # –î–ª—è –±–æ–ª—å—à–µ–≥–æ –∫–æ—Ä–ø—É—Å–∞ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
                original_vector_size = vector_size
                vector_size = 200
                self.logger.info(f"  ‚Üí vector_size: {original_vector_size} ‚Üí {vector_size} (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å)")
        
        self.logger.info("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø –° –í–´–ë–†–ê–ù–ù–´–ú–ò –ü–ê–†–ê–ú–ï–¢–†–ê–ú–ò...")
        self.logger.info(f"üìä –ö–æ—Ä–ø—É—Å: {len(valid_texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {total_words} —Å–ª–æ–≤, {unique_words} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤")
        self.logger.info(
            f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: size={vector_size}, window={window}, min_count={min_count}, "
            f"epochs={epochs}, sample={sample:.2e}, hs={hs}, negative={negative}, "
            f"workers={workers}, compute_loss={compute_loss}"
        )
        
        if model_types is None:
            model_types = ['word2vec_skipgram', 'fasttext_skipgram']
        
        models_created = 0
        start_time = time.time()
        
        # WORD2VEC –ú–û–î–ï–õ–ò
        if 'word2vec_skipgram' in model_types or 'word2vec_cbow' in model_types:
            self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ Word2Vec –º–æ–¥–µ–ª–µ–π...")
            
            word2vec_configs = []
            if 'word2vec_skipgram' in model_types:
                word2vec_configs.append({'sg': 1, 'name': 'word2vec_skipgram'})
            if 'word2vec_cbow' in model_types:
                word2vec_configs.append({'sg': 0, 'name': 'word2vec_cbow'})
            
            for config in word2vec_configs:
                try:
                    model_name = f"{config['name']}_vs{vector_size}_w{window}_mc{min_count}_e{epochs}"
                    self.logger.info(f"üîß –û–±—É—á–µ–Ω–∏–µ {model_name}...")
                    
                    model_start = time.time()
                    model = Word2Vec(
                        sentences=valid_texts,
                        vector_size=vector_size,
                        window=window,
                        min_count=min_count,
                        sg=config['sg'],
                        workers=workers,
                        epochs=epochs,
                        hs=hs,
                        negative=negative,
                        sample=sample,
                        seed=42,
                        compute_loss=compute_loss
                    )
                    
                    training_time = time.time() - model_start
                    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    self.word_models[model_name] = model
                    models_created += 1
                    
                    self.training_history.append({
                        'model_name': model_name,
                        'training_time': training_time,
                        'memory_usage': memory_usage,
                        'vocab_size': len(model.wv.key_to_index),
                        'vector_size': vector_size,
                        'window': window,
                        'min_count': min_count,
                        'epochs': epochs,
                        'architecture': 'Skip-gram' if config['sg'] == 1 else 'CBOW',
                        'compute_loss': compute_loss,
                        'workers': workers
                    })
                    
                    vocab_size = len(model.wv.key_to_index)
                    self.logger.info(f"‚úÖ {model_name} –æ–±—É—á–µ–Ω –∑–∞ {training_time:.1f}—Å! –°–ª–æ–≤–∞—Ä—å: {vocab_size} —Å–ª–æ–≤")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏
                    important_words = ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–Ω–æ—É—Ç–±—É–∫', '–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', 
                                      '–∞–ª–≥–æ—Ä–∏—Ç–º', '–≥–æ—Ä–æ–¥', '—Ä–µ–∫–∞', '—Å–∏—Å—Ç–µ–º–∞', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è']
                    missing_in_model = [w for w in important_words if w not in model.wv]
                    if missing_in_model:
                        self.logger.warning(f"  ‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏: {', '.join(missing_in_model)}")
                    else:
                        self.logger.info(f"  ‚úÖ –í—Å–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ {model_name}: {e}")
        
        # FASTTEXT –ú–û–î–ï–õ–ò
        if 'fasttext_skipgram' in model_types or 'fasttext_cbow' in model_types:
            self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ FastText –º–æ–¥–µ–ª–µ–π...")
            
            fasttext_configs = []
            if 'fasttext_skipgram' in model_types:
                fasttext_configs.append({'sg': 1, 'name': 'fasttext_skipgram'})
            if 'fasttext_cbow' in model_types:
                fasttext_configs.append({'sg': 0, 'name': 'fasttext_cbow'})
            
            for config in fasttext_configs:
                try:
                    model_name = f"{config['name']}_vs{vector_size}_w{window}_mc{min_count}_e{epochs}"
                    self.logger.info(f"üîß –û–±—É—á–µ–Ω–∏–µ {model_name}...")
                    
                    model_start = time.time()
                    model = FastText(
                        sentences=valid_texts,
                        vector_size=vector_size,
                        window=window,
                        min_count=min_count,
                        sg=config['sg'],
                        workers=workers,
                        epochs=epochs,
                        hs=hs,
                        negative=negative,
                        sample=sample,  # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä sample –¥–ª—è FastText
                        seed=42
                    )
                    
                    training_time = time.time() - model_start
                    memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
                    
                    self.word_models[model_name] = model
                    models_created += 1
                    
                    self.training_history.append({
                        'model_name': model_name,
                        'training_time': training_time,
                        'memory_usage': memory_usage,
                        'vocab_size': len(model.wv.key_to_index),
                        'vector_size': vector_size,
                        'window': window,
                        'min_count': min_count,
                        'epochs': epochs,
                        'architecture': 'FastText Skip-gram' if config['sg'] == 1 else 'FastText CBOW',
                        'workers': workers,
                        'compute_loss': False
                    })
                    
                    vocab_size = len(model.wv.key_to_index)
                    self.logger.info(f"‚úÖ {model_name} –æ–±—É—á–µ–Ω –∑–∞ {training_time:.1f}—Å! –°–ª–æ–≤–∞—Ä—å: {vocab_size} —Å–ª–æ–≤")
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏
                    important_words = ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–Ω–æ—É—Ç–±—É–∫', '–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', 
                                      '–∞–ª–≥–æ—Ä–∏—Ç–º', '–≥–æ—Ä–æ–¥', '—Ä–µ–∫–∞', '—Å–∏—Å—Ç–µ–º–∞', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è']
                    missing_in_model = [w for w in important_words if w not in model.wv]
                    if missing_in_model:
                        self.logger.warning(f"  ‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏: {', '.join(missing_in_model)}")
                    else:
                        self.logger.info(f"  ‚úÖ –í—Å–µ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ")
                    
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ {model_name}: {e}")
        
        # DOC2VEC –ú–û–î–ï–õ–ò
        if 'doc2vec' in model_types and categories and len(categories) == len(valid_texts):
            try:
                self.logger.info("üéØ –û–±—É—á–µ–Ω–∏–µ Doc2Vec –º–æ–¥–µ–ª–µ–π...")
                tagged_documents = [
                    TaggedDocument(words=text, tags=[str(i), categories[i]]) 
                    for i, text in enumerate(valid_texts)
                ]
                
                doc2vec_configs = [
                    {'dm': 1, 'name': 'doc2vec_pv-dm'},
                    {'dm': 0, 'name': 'doc2vec_pv-dbow'}
                ]
                
                for config in doc2vec_configs:
                    try:
                        model_name = f"{config['name']}_vs{vector_size}_w{window}_mc{min_count}"
                        self.logger.info(f"üîß –û–±—É—á–µ–Ω–∏–µ {model_name}...")
                        
                        model_start = time.time()
                        model = Doc2Vec(
                            documents=tagged_documents,
                            vector_size=vector_size,
                            window=window,
                            min_count=min_count,
                            dm=config['dm'],
                            workers=workers,
                            epochs=epochs,
                            seed=42
                        )
                        
                        training_time = time.time() - model_start
                        memory_usage = psutil.Process().memory_info().rss / 1024 / 1024
                        
                        self.doc_models[model_name] = model
                        models_created += 1
                        
                        self.training_history.append({
                            'model_name': model_name,
                            'training_time': training_time,
                            'memory_usage': memory_usage,
                            'vector_size': vector_size,
                            'window': window,
                            'architecture': 'PV-DM' if config['dm'] == 1 else 'PV-DBOW',
                            'epochs': epochs,
                            'workers': workers
                        })
                        
                        self.logger.info(f"‚úÖ {model_name} –æ–±—É—á–µ–Ω –∑–∞ {training_time:.1f}—Å!")
                        
                    except Exception as e:
                        self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ {model_name}: {e}")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Doc2Vec: {e}")
        
        total_time = time.time() - start_time
        self.logger.info(f"üéâ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û! –°–æ–∑–¥–∞–Ω–æ {models_created} –º–æ–¥–µ–ª–µ–π –∑–∞ {total_time:.1f} —Å–µ–∫—É–Ω–¥")
        
        return models_created

    def evaluate_models_comprehensive(self, test_words: List[str] = None):
        """
        –ö–û–ú–ü–õ–ï–ö–°–ù–ê–Ø –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô
        """
        if test_words is None:
            test_words = [
                '–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–¥–∞–Ω–Ω—ã–µ', '—Å–∏—Å—Ç–µ–º–∞', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
                '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º', '—Å–µ—Ç—å', '–±–∞–∑–∞'
            ]
        
        evaluation_results = {}
        
        for model_name, model in self.word_models.items():
            try:
                metrics = {}
                
                # 1. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –∏ –ø–æ–∫—Ä—ã—Ç–∏–µ
                vocab_size = len(model.wv.key_to_index)
                coverage = self._calculate_vocabulary_coverage(model, test_words)
                
                metrics['vocabulary_size'] = vocab_size
                metrics['test_coverage'] = coverage['coverage_percentage']
                metrics['oov_rate'] = 100 - coverage['coverage_percentage']
                
                # 2. Word analogy accuracy
                analogy_accuracy = self._evaluate_analogies(model)
                metrics['analogy_accuracy'] = analogy_accuracy
                
                # 3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                similarity_score = self._evaluate_semantic_similarity(model)
                metrics['semantic_similarity_score'] = similarity_score
                
                # 4. –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –¥–ª—è FastText)
                if 'fasttext' in model_name:
                    morphology_score = self._evaluate_morphological_robustness(model)
                    metrics['morphology_score'] = morphology_score
                
                # 5. –í—Ä–µ–º—è –∏ –ø–∞–º—è—Ç—å
                training_info = next((item for item in self.training_history if item['model_name'] == model_name), {})
                metrics['training_time'] = training_info.get('training_time', 0)
                metrics['memory_usage'] = training_info.get('memory_usage', 0)
                
                evaluation_results[model_name] = metrics
                
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ {model_name}: {e}")
                continue
        
        self.evaluation_results = evaluation_results
        return evaluation_results

    def _evaluate_analogies(self, model) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–æ–≥–∏–π –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        analogy_tests = [
            (['–ú–æ—Å–∫–≤–∞', '–†–æ—Å—Å–∏—è'], ['–ü–∞—Ä–∏–∂'], '–§—Ä–∞–Ω—Ü–∏—è'),
            (['–∫–æ—Ä–æ–ª—å', '–∫–æ—Ä–æ–ª–µ–≤–∞'], ['–º—É–∂—á–∏–Ω–∞'], '–∂–µ–Ω—â–∏–Ω–∞'),
            (['—Å–æ–±–∞–∫–∞', '—â–µ–Ω–æ–∫'], ['–∫–æ—à–∫–∞'], '–∫–æ—Ç–µ–Ω–æ–∫'),
            (['—Ö–æ—Ä–æ—à–∏–π', '–ª—É—á—à–µ'], ['–ø–ª–æ—Ö–æ–π'], '—Ö—É–∂–µ'),
            (['–±–æ–ª—å—à–æ–π', '–±–æ–ª—å—à–µ'], ['–º–∞–ª–µ–Ω—å–∫–∏–π'], '–º–µ–Ω—å—à–µ'),
            (['–¥–µ–ª–∞—Ç—å', '—Å–¥–µ–ª–∞–ª'], ['–ø–∏—Å–∞—Ç—å'], '–Ω–∞–ø–∏—Å–∞–ª'),
        ]
        
        correct = 0
        total = 0
        
        for positive, negative, expected in analogy_tests:
            try:
                all_words = positive + negative + [expected]
                if all(word in model.wv for word in all_words):
                    results = model.wv.most_similar(positive=positive, negative=negative, topn=3)
                    top_words = [word for word, score in results]
                    if expected in top_words:
                        correct += 1
                    total += 1
            except:
                continue
        
        return correct / total if total > 0 else 0.0

    def _evaluate_morphological_robustness(self, model) -> float:
        """–û—Ü–µ–Ω–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ FastText"""
        test_words = ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–¥–∞–Ω–Ω—ã–µ']
        variations_score = 0
        
        for word in test_words:
            try:
                variations = [
                    word + '—ã',
                    word[:-1] if len(word) > 3 else word,
                    word + '–Ω—ã–π'
                ]
                
                if word in model.wv:
                    neighbors = [w for w, s in model.wv.most_similar(word, topn=10)]
                    found_variations = sum(1 for var in variations if var in neighbors)
                    variations_score += found_variations / len(variations)
                    
            except:
                continue
        
        return variations_score / len(test_words) if test_words else 0.0

    def _evaluate_semantic_similarity(self, model) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        test_pairs = [
            ('–∫–æ–º–ø—å—é—Ç–µ—Ä', '–Ω–æ—É—Ç–±—É–∫'),
            ('–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è'),
            ('–ø—Ä–æ–≥—Ä–∞–º–º–∞', '–∞–ª–≥–æ—Ä–∏—Ç–º'),
            ('–≥–æ—Ä–æ–¥', '—Ä–µ–∫–∞')
        ]
        
        similarities = []
        for word1, word2 in test_pairs:
            if word1 in model.wv and word2 in model.wv:
                try:
                    similarity = model.wv.similarity(word1, word2)
                    similarities.append(similarity)
                except:
                    continue
        
        return np.mean(similarities) if similarities else 0.0

    def _calculate_vocabulary_coverage(self, model, test_words: List[str]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏—è —Å–ª–æ–≤–∞—Ä—è"""
        found_words = [word for word in test_words if word in model.wv]
        
        return {
            'total_test_words': len(test_words),
            'found_words': len(found_words),
            'coverage_percentage': (len(found_words) / len(test_words)) * 100,
            'missing_words': [word for word in test_words if word not in model.wv]
        }

    def validate_corpus(self, texts: List[List[str]]) -> Tuple[List[List[str]], int, int]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ—Ä–ø—É—Å–∞"""
        if not texts or len(texts) == 0:
            raise ValueError("–ü—É—Å—Ç–æ–π –∫–æ—Ä–ø—É—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        valid_texts = [text for text in texts if text and len(text) > 0]
        if len(valid_texts) == 0:
            raise ValueError("–í—Å–µ —Ç–µ–∫—Å—Ç—ã –ø—É—Å—Ç—ã–µ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")
        
        total_words = sum(len(text) for text in valid_texts)
        all_words = [word for text in valid_texts for word in text]
        unique_words = len(set(all_words))
        
        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤
        from collections import Counter
        word_freq = Counter(all_words)
        
        # –í–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        important_words = ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–Ω–æ—É—Ç–±—É–∫', '–¥–∞–Ω–Ω—ã–µ', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '–ø—Ä–æ–≥—Ä–∞–º–º–∞', 
                          '–∞–ª–≥–æ—Ä–∏—Ç–º', '–≥–æ—Ä–æ–¥', '—Ä–µ–∫–∞', '—Å–∏—Å—Ç–µ–º–∞', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è']
        
        self.logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä–ø—É—Å–∞:")
        self.logger.info(f"- –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(valid_texts)}")
        self.logger.info(f"- –í—Å–µ–≥–æ —Å–ª–æ–≤: {total_words}")
        self.logger.info(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {unique_words}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤
        self.logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–∂–Ω—ã—Ö —Å–ª–æ–≤ –≤ –∫–æ—Ä–ø—É—Å–µ:")
        for word in important_words:
            freq = word_freq.get(word, 0)
            if freq > 0:
                self.logger.info(f"  ‚úÖ '{word}': {freq} —Ä–∞–∑")
            else:
                self.logger.warning(f"  ‚ùå '{word}': –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∫–æ—Ä–ø—É—Å–µ")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç–∞–º
        words_with_freq_1 = sum(1 for freq in word_freq.values() if freq == 1)
        words_with_freq_2 = sum(1 for freq in word_freq.values() if freq == 2)
        
        self.logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞—Å—Ç–æ—Ç:")
        self.logger.info(f"  - –°–ª–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π 1: {words_with_freq_1} ({words_with_freq_1/unique_words*100:.1f}%)")
        self.logger.info(f"  - –°–ª–æ–≤ —Å —á–∞—Å—Ç–æ—Ç–æ–π 2: {words_with_freq_2} ({words_with_freq_2/unique_words*100:.1f}%)")
        self.logger.info(f"  ‚ö†Ô∏è –° min_count=2 –±—É–¥–µ—Ç –ø–æ—Ç–µ—Ä—è–Ω–æ {words_with_freq_1} —Å–ª–æ–≤ ({words_with_freq_1/unique_words*100:.1f}%)")
        
        return valid_texts, total_words, unique_words

    def get_available_models(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        all_models = {}
        all_models.update(self.word_models)
        all_models.update(self.doc_models)
        return all_models

    def get_training_history_df(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è –≤ –≤–∏–¥–µ DataFrame"""
        return pd.DataFrame(self.training_history)

    def get_evaluation_results_df(self) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏ –≤ –≤–∏–¥–µ DataFrame"""
        return pd.DataFrame.from_dict(self.evaluation_results, orient='index')